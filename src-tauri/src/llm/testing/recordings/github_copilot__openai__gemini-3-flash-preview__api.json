{
  "version": 1,
  "provider_id": "github_copilot",
  "protocol": "openai",
  "model": "gemini-3-flash-preview",
  "endpoint_path": "chat/completions",
  "request": {
    "method": "POST",
    "url": "https://api.githubcopilot.com/chat/completions",
    "headers": {
      "authorization": "REDACTED",
      "editor-plugin-version": "copilot-chat/0.35.0",
      "user-agent": "GitHubCopilotChat/0.35.0",
      "content-type": "application/json",
      "editor-version": "vscode/1.105.1",
      "copilot-integration-id": "vscode-chat"
    },
    "body": {
      "max_tokens": 15000,
      "messages": [
        {
          "content": "\nYou are TalkCody, an expert Coding Planner and Lead Engineer. Your mandate is to orchestrate complex software tasks, manage sub-agents, and execute code changes with precision. You operate within a defined workspace and must strictly adhere to project constraints defined in AGENTS.md.\n\n# CORE IDENTITY & INTERACTION\n- **Orchestrator**: You clarify ambiguity immediately, then drive the task to completion.\n- **Directness**: Respond in the user's language. Omit conversational filler. Your output must be dense with utility (code, plans, or direct answers).\n- **Context Aware**: Your \"Source of Truth\" is the file system and AGENTS.md. Do not hallucinate APIs or dependencies.\n\n# TOOL USAGE STRATEGY\n\n## Concurrency & Batch Tool Calls\n\n**return as many tool calls as possible in a single response**.\n\n### Read Operations - ALWAYS Batch Together\n```\nI need to understand the authentication system. Making all read calls at once:\n\n[Tool Calls]\n- read-file: /src/auth/login.ts\n- read-file: /src/auth/register.ts\n- read-file: /src/auth/middleware.ts\n- read-file: /src/auth/types.ts\n- read-file: /src/lib/jwt.ts\n- read-file: /src/auth/session.ts\n- read-file: /src/auth/permissions.ts\n- glob: /src/auth/**/*.test.ts\n```\n\n### Write/Edit Operations - Batch Different Files\n```\nCreating 5 new components. Making all write calls at once:\n\n[Tool Calls]\n- write-file: /src/components/Button.tsx\n- write-file: /src/components/Input.tsx\n- write-file: /src/components/Card.tsx\n- write-file: /src/components/Modal.tsx\n- write-file: /src/components/Table.tsx\n```\n\n(Multiple edits to different files):\n```\n[Tool Calls]\n- edit-file: /src/app/page.tsx\n- edit-file: /src/app/layout.tsx\n- edit-file: /src/lib/utils.ts\n```\n\n## Decision Matrix\n\n### 1. Direct Execution\nUse standard tools (`readFile`, `editFile`, `codeSearch`) for straightforward tasks.\n- **Scope**: Single-file edits, simple bug fixes, or quick info gathering.\n- **Logic**: When the task is clearly bounded and you have all necessary context.\n\n### 2. Strategic Delegation (`callAgent`)\nActively consider delegation for enhanced efficiency and quality. When using `callAgent`, **always** consider whether multiple sub-agents can work in parallel to maximize efficiency.\n- **Complex Analysis**: Multi-domain analysis (architecture, testing, security, performance) benefits from specialized focus.\n- **Parallel Processing**: Independent modules or components can be handled simultaneously for faster completion.\n- **Expertise Areas**: Tasks requiring deep domain knowledge (AI integration, database design, security audits) benefit from focused sub-agents.\n- **Large Scope**: When comprehensive analysis across multiple files/systems is needed.\n\n## Delegation Protocol\n- **Gather First**: Use `codeSearch`, `readFile`, `glob` tool or explore agent to gather context *before* calling an agent to ensure you pass valid context.\n- **Explicit Payload**:\n  - `context`: Dump relevant file contents/search results here. Do not assume the agent knows previous chat history.\n  - `targets`: List specific files to avoid overwrite conflicts.\n- **Write Tasks**: When delegating multi-file or multi-module modifications, break them into clearly separated, independent tasks to enable parallel execution without file conflicts.\n- **Task Decomposition**: Each sub-agent should handle a complete, self-contained modification unit (e.g., one component, one service, one feature module) with exact file boundaries.\n- **Dependency Management**: If tasks have dependencies, clearly define execution sequence or use separate delegation rounds.\n\n## callAgent Parallelism Strategy\n\n**ALWAYS maximize parallelism** - spawn multiple subagents in a single response when tasks are independent.\n\n### Strategy 1: Same Agent Type, Different Modules\nUse multiple instances of the same agent to gather context from different areas:\n```\n[Tool Calls]\n- callAgent: explore → gather context from /src/auth\n- callAgent: explore → gather context from /src/api\n- callAgent: explore → gather context from /src/db\n```\n\n### Strategy 2: Different Agent Types, Different Concerns\nUse specialized agents in parallel for different aspects of the same feature:\n```\n[Tool Calls]\n- callAgent: document-writer → write API documentation for the new feature\n- callAgent: coding → implement the core business logic\n- callAgent: code-review → write unit tests for the feature\n```\n\n### Parallelism Decision Matrix\n| Scenario | Strategy |\n|----------|----------|\n| Context gathering from multiple modules | Multiple parallel explore agents |\n| Feature implementation (code + docs + tests) | Parallel coding + document-writer + code-review |\n| Multi-file refactor | Multiple parallel coding agents with distinct targets |\n| Dependent tasks (A's output feeds B) | Sequential callAgent rounds |\n\n## Edit-File Tool\n\n**When to use edit-file tool vs write-file tool:**\n   - **edit-file**: File exists, making modifications (1-10 related changes per file)\n     - Single edit: One isolated change\n     - Multiple edits: Related changes to same file (imports + types, code)\n   - **write-file**: Creating a brand new file from scratch\n   - **write-file**: overwrite existing file when too many changes are needed\n\n# ENGINEERING GUIDELINES\n**Philosophy**: Keep It Simple, Stupid (KISS). Prioritize maintainability and readability over clever one-liners.\n\n1. **Building from Scratch**:\n   - Confirm requirements first.\n   - Sketch the architecture/module interaction mentally or in the plan.\n   - Implement modular, strictly typed (where applicable), and self-documenting code.\n\n2. **Modifying Existing Code**:\n   - **Understand First**: Read related files to grasp the current patterns and style.\n   - **Minimal Intrusion**: Make the smallest change necessary to achieve the goal. Avoid sweeping formatting changes unless requested.\n   - **Bug Fixes**: Locate the root cause via logs or reproduction steps. Ensure your fix addresses the root cause, not just the symptom. Verify with tests.\n\n3. **Refactoring**:\n   - Only change internal structure, never external behavior (unless it's an API breaking change request).\n   - Update all consumers of the refactored code.\n\n# WORKFLOW Mode: ACT vs. PLAN\n\n## ACT VS PLAN\n\n- For trivial and simple tasks, ACT directly using tools.\n- For complex tasks, PLAN first then ACT.\n- If the task involves multiple files, architectural changes, or high ambiguity, you MUST enter **PLAN MODE**.\n\n**CRITICAL RULE**: Check the environment context provided in the <env> XML section. If you see:\n```xml\n<env>\n  <plan_mode>TRUE</plan_mode>\n</env>\n```\n\nThis means PLAN MODE is ENABLED. When PLAN MODE is ENABLED (`<plan_mode>TRUE</plan_mode>`):\n- You MUST follow the PLAN MODE instructions provided below\n- You MUST delegate planning to the Plan Agent before executing complex tasks\n- DO NOT skip the planning phase\n\nWhen PLAN MODE is DISABLED (`<plan_mode>FALSE</plan_mode>`):\n- You can execute tasks directly without delegating to the Plan Agent\n- Use your judgment to decide between ACT and PLAN based on task complexity\n\n# PLAN MODE Workflow\n\n## Overview\n\nWhen PLAN MODE is ENABLED (indicated by `<plan_mode>TRUE</plan_mode>` in the <env> section), you should delegate planning to the specialized **Plan Agent**. The Plan Agent will:\n1. Explore the project context\n2. Generate a structured plan\n3. Return the approved plan content\n\n## Delegation to Plan Agent\n\n### How to Delegate\n\nUse `callAgent` with the \"plan\" agent:\n\n```\ncallAgent({\n  agentId: \"plan\",\n  task: \"Create a comprehensive implementation plan for: {USER_REQUIREMENT}\",\n  context: `\nProject Root: {workspaceRoot}\nTask ID: {taskId}\nCurrent Requirement: {USER_REQUIREMENT}\n\nRelevant Context:\n- Project structure and existing patterns\n- Related files that may be affected\n- Any specific constraints or requirements mentioned\n  `,\n  targets: []\n})\n```\n\nNote: you should pass enough context to the Plan Agent to enable it to generate a comprehensive plan.\n\n### Receiving Plan Agent Results\n\nThe Plan Agent returns plan by markdown format\n\n- Use the approved plan to create a todo list using **TodoWrite**\n- Execute each todo item\n- Update todo status as you complete them\n\n## Executing Approved Plans\n\nAfter receiving approved plan:\n\n1. **Create todos** using **TodoWrite** based on the approved plan\n2. **Execute todos systematically**:\n   - Process dependent tasks first\n   - Use parallel execution for independent tasks\n   - Update todo status after each completion\n3. **Report progress** to user\n4. **Handle errors** gracefully with clear messages\n\n# SAFETY & BOUNDARIES\n- **Workspace Confinement**: strict operations within the allowed root directories.\n- **Non-Destructive**: Never delete non-trivial code without explicit confirmation in the Plan.\n- **Secrets Management**: Never print or hardcode credentials/secrets.\n\n# OBJECTIVE\nYour goal is not to chat, but to ship. Measure success by:\n1. Accuracy of the solution.\n2. Stability of the code.\n3. Adherence to existing project styles.\n\n# Rules\n\n- The user may provide a file's contents directly in their message, in which case you shouldn't use the read_file tool to get the file contents again since you already have it.\n- Be precise with replacements to avoid errors\n- Follow existing project patterns and conventions\n- Answer the user's question directly with a concise answer; do not generate new Markdown files to answer the user's question.\n\n\n---\n\nIMPORTANT: Here is important information about the environment you are running in:\n<env>\n<working_directory>/Users/kks/git/starrocks</working_directory>\n<is_git_repo>Yes</is_git_repo>\n<platform>mac_arm</platform>\n<today_date>2026-02-01</today_date>\n<plan_mode>FALSE</plan_mode>\n</env>\n\n---\n\n## Markdown Context\n\n<project>\n# AGENTS.md - StarRocks\n\n> This file provides context for AI coding agents working with the StarRocks codebase.\n> Compatible with: Claude Code, OpenCode, Cursor, Gemini CLI, Windsurf, Aider, Continue, Cline, and other MCP-compatible tools.\n\n## Project Overview\n\nStarRocks is a high-performance, real-time analytical database. It delivers sub-second query latency for complex analytics workloads through its MPP (Massively Parallel Processing) architecture.\n\n**Key capabilities:**\n- Real-time analytics with sub-second latency\n- High-concurrency, low-latency queries\n- Support for both shared-nothing and shared-data deployments\n- Materialized views for query acceleration\n- External data source federation (Hive, Iceberg, Hudi, Delta Lake, JDBC, etc.)\n\n## Architecture\n\nStarRocks uses a decoupled frontend-backend architecture:\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                      Frontend (FE)                          │\n│  Java │ SQL Parsing │ Query Planning │ Metadata Management  │\n└────────────────────────────┬────────────────────────────────┘\n                             │ Thrift RPC\n┌────────────────────────────▼────────────────────────────────┐\n│                      Backend (BE)                           │\n│  C++ │ Query Execution │ Storage Engine │ Data Processing   │\n└─────────────────────────────────────────────────────────────┘\n```\n\n- **FE (Frontend)**: Java-based. Handles SQL parsing, query optimization, metadata management, and cluster coordination.\n- **BE (Backend)**: C++-based. Handles query execution, storage management, and data processing.\n\n## Directory Structure\n\n```\nstarrocks/\n├── be/                    # Backend (C++) - Query execution & storage\n├── fe/                    # Frontend (Java) - SQL & metadata\n│   ├── fe-core/          # Core FE logic\n│   ├── fe-parser/        # SQL parser\n│   ├── fe-grammar/       # ANTLR grammar files\n│   ├── fe-type/          # Type system\n│   ├── fe-spi/           # Service Provider Interfaces\n│   ├── connector/        # External data source connectors\n│   └── plugin/           # FE plugins\n├── java-extensions/       # JNI connectors for external sources\n├── gensrc/               # Generated code (Thrift, Protobuf)\n├── test/                 # SQL integration tests\n├── docs/                 # Documentation (Docusaurus)\n├── thirdparty/           # Third-party dependencies\n└── docker/               # Docker build files\n```\n\n## Quick Commands\n\n### Build\n\n```bash\n# Build everything\n./build.sh --fe --be --clean\n\n# Build only Backend\n./build.sh --be\n\n# Build only Frontend\n./build.sh --fe\n\n# Build with specific type (Release/Debug/ASAN)\nBUILD_TYPE=Debug ./build.sh --be\nBUILD_TYPE=ASAN ./build.sh --be\n```\n\n### Run Tests\n\n```bash\n# Run all FE unit tests\n./run-fe-ut.sh\n\n# Run specific FE test\n./run-fe-ut.sh --test com.starrocks.sql.plan.TPCHPlanTest\n\n# Run all BE unit tests\n./run-be-ut.sh\n\n# Run specific BE test\n./run-be-ut.sh --test CompactionUtilsTest\n\n# Run with gtest filter\n./run-be-ut.sh --gtest_filter \"TabletUpdatesTest*\"\n\n# Run SQL integration tests (requires running cluster)\ncd test && python3 run.py -v\n```\n\n### Code Formatting\n\n```bash\n# Format C++ code (BE)\nclang-format -i <file.cpp>\n\n# Check Java code style (FE)\ncd fe && mvn checkstyle:check\n```\n\n## Code Style Summary\n\n### C++ (Backend)\n- **Style**: Google C++ Style (with modifications)\n- **Config**: `.clang-format` at project root\n- **Indent**: 4 spaces\n- **Line limit**: 120 characters\n- **Pointer alignment**: Left (`int* ptr`)\n\n### Java (Frontend)\n- **Style**: Google Java Style (with modifications)\n- **Config**: `fe/checkstyle.xml`\n- **Indent**: 4 spaces\n- **Line limit**: 130 characters\n- **Import order**: Third-party, then Java standard, then static\n\n### Protobuf\n- Message names: `PascalCasePB` (e.g., `MyMessagePB`)\n- Field names: `snake_case`\n- **Never** use `required` fields\n- **Never** change field ordinals\n\n### Thrift\n- Struct names: `TPascalCase` (e.g., `TMyStruct`)\n- Field names: `snake_case`\n- **Never** use `required` fields\n- **Never** change field ordinals\n\n## Documentation Sync Requirements\n\n### Configuration Changes\n\n**When you add, modify, or remove configuration parameters, you MUST update the corresponding documentation:**\n\n| Component | Documentation File |\n|-----------|-------------------|\n| FE Config | `docs/en/administration/management/FE_configuration.md` |\n| BE Config | `docs/en/administration/management/BE_configuration.md` |\n\n**Required documentation for each config parameter:**\n- Parameter name\n- Default value\n- Value range (if applicable)\n- Description of what it controls\n- When to use/modify it\n- Whether it requires restart\n\n### Metrics Changes\n\n**When you add, modify, or remove metrics, you MUST update the corresponding documentation:**\n\n| Metrics Type | Documentation File |\n|--------------|-------------------|\n| General Metrics | `docs/en/administration/management/monitoring/metrics.md` |\n| Shared-Data Metrics | `docs/en/administration/management/monitoring/metrics-shared-data.md` |\n| MV Metrics | `docs/en/administration/management/monitoring/metrics-materialized_view.md` |\n| BE Metrics (SQL) | `docs/en/sql-reference/information_schema/be_metrics.md` |\n| FE Metrics (SQL) | `docs/en/sql-reference/information_schema/fe_metrics.md` |\n\n**Required documentation for each metric:**\n- Metric name\n- Type (Counter, Gauge, Histogram)\n- Labels (if any)\n- Description of what it measures\n- Unit (if applicable)\n\n### PR Checklist for Config/Metrics Changes\n\nWhen your PR includes configuration or metrics changes:\n\n- [ ] Updated corresponding documentati\n...\n[trimmed]\n...\n\n\n| Issue | Solution |\n|-------|----------|\n| CI timeout | Re-run failed jobs; check for flaky tests |\n| Checkstyle failure | Run `mvn checkstyle:check` locally |\n| Build failure | Run `./build.sh --fe --be` locally |\n| Merge conflicts | Rebase on latest main branch |\n\n## Testing Guidelines\n\n### Unit Tests\n- **BE**: Use Google Test framework. Tests in `be/test/` mirror source structure.\n- **FE**: Use JUnit 5. Tests in `fe/fe-core/src/test/java/`.\n\n### SQL Integration Tests\n- Use the SQL-tester framework in `test/`\n- See `test/README.md` for detailed documentation\n- Basic execution: `python3 run.py -v`\n\n### Test Requirements\n- All new features must have corresponding tests\n- Bug fixes should include regression tests\n- Maintain or improve test coverage\n\n## Performance Considerations\n\n### Backend (C++)\n- Use SIMD instructions where applicable (AVX2/AVX512)\n- Prefer vectorized processing over row-by-row\n- Use `Column` and `Chunk` abstractions for data processing\n- Be mindful of memory allocations in hot paths\n- Profile with `perf` or async-profiler before optimization\n\n### Frontend (Java)\n- Avoid creating unnecessary objects in hot paths\n- Use appropriate data structures for the use case\n- Be cautious with synchronized blocks\n\n## Security Guidelines\n\n- Validate all user inputs at system boundaries\n- Never log sensitive information (passwords, tokens, PII)\n- Follow existing authentication/authorization patterns\n- Use parameterized queries to prevent SQL injection\n\n## Nested AGENTS.md Files\n\nFor module-specific guidelines, refer to:\n- `be/AGENTS.md` - Backend C++ development\n- `fe/AGENTS.md` - Frontend Java development\n- `java-extensions/AGENTS.md` - JNI connector development\n- `gensrc/AGENTS.md` - Generated code handling\n- `test/AGENTS.md` - SQL integration testing\n- `docs/AGENTS.md` - Documentation contribution\n\n## Useful Resources\n\n- [Official Documentation](https://docs.starrocks.io/)\n- [Contributing Guide](./CONTRIBUTING.md)\n- [GitHub Issues](https://github.com/StarRocks/starrocks/issues)\n- [Slack Community](https://try.starrocks.com/join-starrocks-on-slack)\n\n## License\n\nStarRocks is licensed under the Apache License 2.0.\n\nAll source files must include the appropriate license header:\n```\n// Copyright 2021-present StarRocks, Inc. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// ...\n```\n\n</project>\n\n---\n\nOUTPUT FORMAT INSTRUCTIONS:\nOutput the response in standard Markdown with headings, lists, and code blocks.\n\n---\n\n====\n# Available Subagents\n\nThe following agents are available for delegation via the `callAgent` tool.\nPick the best-fit subagent for each task (do not assume only one exists):\n\n- **plan**: Professional software planning expert that generates structured plans and obtains user approval\n- **code-review**: Multi-source code review specialist for GitHub PRs, local commits, git diffs, and project files\n- **explore**: Efficient single-task information gathering\n- **document-writer**: Creates comprehensive documentation matching repository standards\n- **test-writer**: Specialist in writing comprehensive tests matching repository standards\n- **coding**: Expert programming assistant for code reviews, debugging, and development\n- **china-options-buyer**: professional China commodity options trading strategist focused on long (buyer) strategies.\n- **api-designer**: API architecture expert designing scalable, developer-friendly interfaces. Creates REST and GraphQL APIs with comprehensive documentation, focusing on consistency, performance, and developer experience.\n- **code-reviewer**: Expert code reviewer specializing in code quality, security vulnerabilities, and best practices across multiple languages. Masters static analysis, design patterns, and performance optimization with focus on maintainability and technical debt reduction.\n\n## Delegation Guidelines\n\n- **Choose wisely**: Select the agent best suited for the specific task\n- **Provide context**: Subagents start with empty context; pass all necessary information\n- **Set targets**: Specify files/modules to avoid conflicts in parallel execution\n- **Batch independent tasks**: Run multiple subagents in parallel when tasks are independent\n====\n\nIMPORTANT: You MUST respond in English.",
          "role": "system"
        },
        {
          "content": "What is the relationship between Starrrocks' pipeline and fragments?",
          "role": "user"
        },
        {
          "content": null,
          "role": "assistant",
          "tool_calls": [
            {
              "function": {
                "arguments": "{\"path\":\"docs/\",\"pattern\":\"pipeline fragment\"}",
                "name": "codeSearch"
              },
              "id": "call_MHxmNDFzVzVleUhQNzUzS2R0a0s",
              "type": "function"
            }
          ]
        },
        {
          "content": "{\n  \"success\": true,\n  \"result\": \"No matches found\"\n}",
          "role": "tool",
          "tool_call_id": "call_MHxmNDFzVzVleUhQNzUzS2R0a0s"
        },
        {
          "content": null,
          "role": "assistant",
          "tool_calls": [
            {
              "function": {
                "arguments": "{\"path\":\"be/src/exec/pipeline/\",\"pattern\":\"Pipeline\"}",
                "name": "codeSearch"
              },
              "id": "call_MHxjQkJTbXFRT08zZHhGWDVoTHU",
              "type": "function"
            }
          ]
        },
        {
          "content": "{\n  \"success\": true,\n  \"result\": \"Found 234 matches:\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/exchange/split_local_exchange.h\\n  19: #include \\\"exec/pipeline/source_operator.h\\\"\\n  22: namespace starrocks::pipeline {\\n  72: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/exchange/mem_limited_chunk_queue.h\\n  20: #include \\\"exec/pipeline/exchange/multi_cast_local_exchange.h\\\"\\n  26: namespace starrocks::pipeline {\\n  241: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/pipeline_driver_poller.h\\n  23: #include \\\"pipeline_driver.h\\\"\\n  24: #include \\\"pipeline_driver_queue.h\\\"\\n  27: namespace starrocks::pipeline {\\n  29: class PipelineDriverPoller;\\n  30: using PipelineDriverPollerPtr = std::unique_ptr<PipelineDriverPoller>;\\n  33: class PipelineDriverPoller {\\n  35: explicit PipelineDriverPoller(std::string name, DriverQueue* driver_queue, CpuUtil::CpuIds cpuids,\\n  45: ~PipelineDriverPoller() { shutdown(); }\\n  47: DISALLOW_COPY(PipelineDriverPoller);\\n  92: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/pipeline_metrics.h\\n  25: namespace pipeline {\\n  102: struct PipelineExecutorMetrics {\\n  123: } // namespace pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/hashjoin/hash_joiner_factory.cpp\\n  15: #include \\\"exec/pipeline/hashjoin/hash_joiner_factory.h\\\"\\n  17: namespace starrocks::pipeline {\\n  87: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/audit_statistics_reporter.cpp\\n  15: #include \\\"exec/pipeline/audit_statistics_reporter.h\\\"\\n  25: namespace starrocks::pipeline {\\n  57: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/aggregate/aggregate_distinct_blocking_sink_operator.cpp\\n  20: namespace starrocks::pipeline {\\n  81: if (size >= _aggregator->limit() || (_aggregator->params()->enable_pipeline_share_limit &&\\n  89: if (limit_with_no_agg && _aggregator->params()->enable_pipeline_share_limit) {\\n  105: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/nljoin/spillable_nljoin_build_operator.h\\n  17: #include \\\"exec/pipeline/nljoin/nljoin_build_operator.h\\\"\\n  18: #include \\\"exec/pipeline/spill_process_channel.h\\\"\\n  22: namespace starrocks::pipeline {\\n  67: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/operator_with_dependency.h\\n  19: #include \\\"exec/pipeline/operator.h\\\"\\n  20: #include \\\"exec/pipeline/pipeline_fwd.h\\\"\\n  22: namespace starrocks::pipeline {\\n  29: // of a pipeline, The left operator(HashJoinProbeOperator) can work in streaming-style, so it can appear in the middle\\n  30: // of another pipeline. Denotes pipeline contains left operator and right operator as left pipeline and right pipeline\\n  31: // respectively, so left pipeline always has a data dependency on right pipeline, left pipeline can be scheduled to\\n  32: // execute on core until right pipeline has finished. For a left pipeline, it shall get stuck in left operator.\\n  33: // OperatorWithDependency is introduced so that left operators should inherit from it. When a left pipeline get stuck\\n  34: // in left operators, it should be guarded by PipelineDriverPoller thread that will inspect std::vector<OperatorWithDependency*>\\n  35: // PipelineDriver::_dependencies to ensure that left pipeline is put back into multi-level feedback queue after all\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/exchange/exchange_merge_sort_source_operator.h\\n  19: #include \\\"exec/pipeline/source_operator.h\\\"\\n  25: namespace pipeline {\\n  113: } // namespace pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/fetch_task.cpp\\n  15: #include \\\"exec/pipeline/fetch_task.h\\\"\\n  21: #include \\\"exec/pipeline/fetch_processor.h\\\"\\n  22: #include \\\"exec/pipeline/lookup_request.h\\\"\\n  31: namespace starrocks::pipeline {\\n  186: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/schedule/event_scheduler.cpp\\n  15: #include \\\"exec/pipeline/schedule/event_scheduler.h\\\"\\n  17: #include \\\"exec/pipeline/pipeline_driver.h\\\"\\n  18: #include \\\"exec/pipeline/pipeline_driver_queue.h\\\"\\n  19: #include \\\"exec/pipeline/pipeline_fwd.h\\\"\\n  20: #include \\\"exec/pipeline/schedule/common.h\\\"\\n  21: #include \\\"exec/pipeline/schedule/utils.h\\\"\\n  23: namespace starrocks::pipeline {\\n  45: // The logic in the pipeline poller is basically the same.\\n  72: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/aggregate/spillable_aggregate_blocking_sink_operator.cpp\\n  15: #include \\\"exec/pipeline/aggregate/spillable_aggregate_blocking_sink_operator.h\\\"\\n  22: #include \\\"exec/pipeline/aggregate/aggregate_blocking_sink_operator.h\\\"\\n  23: #include \\\"exec/pipeline/query_context.h\\\"\\n  36: namespace starrocks::pipeline {\\n  121: _aggregator->params()->enable_pipeline_share_limit = false;\\n  361: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/aggregate/aggregate_distinct_streaming_sink_operator.h\\n  21: #include \\\"exec/pipeline/operator.h\\\"\\n  23: namespace starrocks::pipeline {\\n  98: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/nljoin/nljoin_build_operator.cpp\\n  15: #include \\\"exec/pipeline/nljoin/nljoin_build_operator.h\\\"\\n  18: #include \\\"exec/pipeline/operator.h\\\"\\n  22: namespace starrocks::pipeline {\\n  65: Operator::OutputAmplificationType NLJoinBuildOperator::intra_pipeline_amplification_type() const {\\n  69: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/bucket_process_operator.cpp\\n  15: #include \\\"exec/pipeline/bucket_process_operator.h\\\"\\n  19: #include \\\"exec/pipeline/aggregate/spillable_aggregate_blocking_sink_operator.h\\\"\\n  20: #include \\\"exec/pipeline/aggregate/spillable_aggregate_distinct_blocking_operator.h\\\"\\n  21: #include \\\"exec/pipeline/operator.h\\\"\\n  22: #include \\\"exec/pipeline/pipeline_fwd.h\\\"\\n  23: #include \\\"exec/pipeline/spill_process_channel.h\\\"\\n  28: namespace starrocks::pipeline {\\n  216: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/exchange/multi_cast_local_exchange_source_operator.cpp\\n  15: #include \\\"exec/pipeline/exchange/multi_cast_local_exchange_source_operator.h\\\"\\n  17: namespace starrocks::pipeline {\\n  49: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/aggregate/repeat/repeat_operator.cpp\\n  15: #include \\\"exec/pipeline/aggregate/repeat/repeat_operator.h\\\"\\n  21: namespace starrocks::pipeline {\\n  90: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/capture_version_operator.h\\n  17: #include \\\"exec/pipeline/source_operator.h\\\"\\n  20: namespace starrocks::pipeline {\\n  60: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/table_function_operator.cpp\\n  17: namespace starrocks::pipeline {\\n  141: // Just return the chunk whether it's full or not in order to keep the semantics of pipeline\\n  258: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/hashjoin/spillable_hash_join_probe_operator.h\\n  26: #include \\\"exec/pipeline/hashjoin/hash_join_probe_operator.h\\\"\\n  33: namespace starrocks::pipeline {\\n  168: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/exchange/sink_buffer.h\\n  25: #include \\\"exec/pipeline/fragment_context.h\\\"\\n  34: namespace starrocks::pipeline {\\n  98: void attach_observer(RuntimeState* state, PipelineObserver* observer) { _observable.add_observer(state, observer); }\\n  231: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/dict_decode_operator.cpp\\n  15: #include \\\"exec/pipeline/dict_decode_operator.h\\\"\\n  21: namespace starrocks::pipeline {\\n  150: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/exchange/exchange_parallel_merge_source_operator.h\\n  17: #include \\\"exec/pipeline/source_operator.h\\\"\\n  22: namespace pipeline {\\n  50: // can be easily integrated into the pipeline engine. The only thing for this operator need to to is to call\\n  127: } // namespace pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/aggregate/spillable_aggregate_distinct_blocking_operator.h\\n  20: #include \\\"exec/pipeline/aggregate/aggregate_distinct_blocking_sink_operator.h\\\"\\n  21: #include \\\"exec/pipeline/aggregate/aggregate_distinct_blocking_source_operator.h\\\"\\n  22: #include \\\"exec/pipeline/operator.h\\\"\\n  23: #include \\\"exec/pipeline/source_operator.h\\\"\\n  27: namespace starrocks::pipeline {\\n  133: ChunkPipelineAccumulator _accumulator;\\n  158: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/aggregate/sorted_aggregate_streaming_sink_operator.h\\n  20: #include \\\"exec/pipeline/operator.h\\\"\\n  23: namespace starrocks::pipeline {\\n  45: ChunkPipelineAccumulator _accumulator;\\n  69: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/exchange/local_exchange.cpp\\n  15: #include \\\"exec/pipeline/exchange/local_exchange.h\\\"\\n  22: #include \\\"exec/pipeline/exchange/shuffler.h\\\"\\n  27: namespace starrocks::pipeline {\\n  507: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/schedule/timeout_tasks.h\\n  17: #include \\\"exec/pipeline/schedule/observer.h\\\"\\n  18: #include \\\"exec/pipeline/schedule/pipeline_timer.h\\\"\\n  21: namespace starrocks::pipeline {\\n  25: class CheckFragmentTimeout final : public PipelineTimerTask {\\n  35: class RFScanWaitTimeout final : public PipelineTimerTask {\\n  38: void add_observer(RuntimeState* state, PipelineObserver* observer) { _timeout.add_observer(state, observer); }\\n  46: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/sort/local_partition_topn_source.h\\n  17: #include \\\"exec/pipeline/sort/local_partition_topn_context.h\\\"\\n  18: #include \\\"exec/pipeline/source_operator.h\\\"\\n  20: namespace starrocks::pipeline {\\n  63: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/select_operator.cpp\\n  15: #include \\\"exec/pipeline/select_operator.h\\\"\\n  21: namespace starrocks::pipeline {\\n  132: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/nljoin/nljoin_context.h\\n  25: #include \\\"exec/pipeline/context_with_dependency.h\\\"\\n  26: #include \\\"exec/pipeline/spill_process_channel.h\\\"\\n  40: namespace starrocks::pipeline {\\n  187: void attach_probe_observer(RuntimeState* state, pipeline::PipelineObserver* observer) {\\n  190: void attach_build_observer(RuntimeState* state, pipeline::PipelineObserver* observer) {\\n  231: pipeline::Observable _builder_observable;\\n  232: pipeline::Observable _probe_observable;\\n  235: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/aggregate/aggregate_blocking_sink_operator.h\\n  22: #include \\\"exec/pipeline/operator.h\\\"\\n  26: namespace starrocks::pipeline {\\n  120: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/exchange/multi_cast_local_exchange.h\\n  21: #include \\\"exec/pipeline/schedule/observer.h\\\"\\n  22: #include \\\"exec/pipeline/source_operator.h\\\"\\n  27: namespace starrocks::pipeline {\\n  30: // at the end of the pipeline. It works like\\n  31: // [source pipeline]  -> exchange_sink_operator\\n  33: // But for multi cast stream sink, extra pipelines are added with exchanger\\n  35: // [source pipeline] -> mcast_local_sink   -> mcast_local_exchanger  -> [new pipeline1]\\n  36: //                                                     |             -> [new pipeline2]\\n  37: //                                                     |             -> [new pipeline3]\\n  38: // and for each new pipeline, the internal structure is\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/aggregate/sorted_aggregate_streaming_source_operator.cpp\\n  15: #include \\\"exec/pipeline/aggregate/sorted_aggregate_streaming_source_operator.h\\\"\\n  17: #include \\\"exec/pipeline/sort/sort_context.h\\\"\\n  20: namespace starrocks::pipeline {\\n  93: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/exchange/exchange_source_operator.cpp\\n  15: #include \\\"exec/pipeline/exchange/exchange_source_operator.h\\\"\\n  24: namespace starrocks::pipeline {\\n  41: return _stream_recvr->has_output_for_pipeline(_driver_sequence);\\n  50: _stream_recvr->short_circuit_for_pipeline(_driver_sequence);\\n  57: RETURN_IF_ERROR(_stream_recvr->get_chunk_for_pipeline(&chunk, _driver_sequence));\\n  103: if (!_enable_pipeline_level_shuffle) {\\n  108: // then pipeline level shuffle will be performed at sender side (ExchangeSinkOperator), so\\n  124: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/exchange/sink_buffer.cpp\\n  15: #include \\\"exec/pipeline/exchange/sink_buffer.h\\\"\\n  31: namespace starrocks::pipeline {\\n  70: // In some extreme cases, the pipeline driver has not been created yet, and the query is over\\n  125: size_t max_buffer_size = config::pipeline_sink_buffer_size * _sink_ctxs.size();\\n  302: too_much_brpc_process = discontinuous_acked_window_size >= config::pipeline_sink_brpc_dop;\\n  304: too_much_brpc_process = context.num_in_flight_rpcs >= config::pipeline_sink_brpc_dop;\\n  500: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/fetch_processor.h\\n  25: #include \\\"exec/pipeline/fetch_sink_operator.h\\\"\\n  26: #include \\\"exec/pipeline/fetch_task.h\\\"\\n  27: #include \\\"exec/pipeline/lookup_request.h\\\"\\n  34: namespace starrocks::pipeline {\\n  38: // The pipeline pushes chunks that contain row-position metadata produced by a lookup operator.\\n  49: //   submitted and dequeued when their reconstructed chunks are pulled by the pipeline driver.\\n  182: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/lookup_request.cpp\\n  15: #include \\\"exec/pipeline/lookup_request.h\\\"\\n  22: #include \\\"exec/pipeline/lookup_operator.h\\\"\\n  23: #include \\\"exec/pipeline/query_context.h\\\"\\n  24: #include \\\"exec/pipeline/scan/glm_manager.h\\\"\\n  33: namespace starrocks::pipeline {\\n  540: auto glm_ctx = down_cast<pipeline::IcebergGlobalLateMaterilizationContext*>(\\n  644: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/assert_num_rows_operator.cpp\\n  15: #include \\\"exec/pipeline/assert_num_rows_operator.h\\\"\\n  22: namespace starrocks::pipeline {\\n  96: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/nljoin/nljoin_build_operator.h\\n  20: #include \\\"exec/pipeline/nljoin/nljoin_context.h\\\"\\n  21: #include \\\"exec/pipeline/operator.h\\\"\\n  23: namespace starrocks::pipeline {\\n  50: OutputAmplificationType intra_pipeline_amplification_type() const override;\\n  77: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/exchange/multi_cast_local_exchange_sink_operator.cpp\\n  15: #include \\\"exec/pipeline/exchange/multi_cast_local_exchange_sink_operator.h\\\"\\n  17: namespace starrocks::pipeline {\\n  58: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/chunk_accumulate_operator.cpp\\n  15: #include \\\"exec/pipeline/chunk_accumulate_operator.h\\\"\\n  19: namespace starrocks::pipeline {\\n  54: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/audit_statistics_reporter.h\\n  24: namespace starrocks::pipeline {\\n  37: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/exchange/spillable_multi_cast_local_exchange.cpp\\n  21: #include \\\"exec/pipeline/exchange/mem_limited_chunk_queue.h\\\"\\n  22: #include \\\"exec/pipeline/exchange/multi_cast_local_exchange.h\\\"\\n  23: #include \\\"exec/pipeline/exchange/multi_cast_local_exchange_sink_operator.h\\\"\\n  38: namespace starrocks::pipeline {\\n  104: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/aggregate/sorted_aggregate_streaming_sink_operator.cpp\\n  15: #include \\\"exec/pipeline/aggregate/sorted_aggregate_streaming_sink_operator.h\\\"\\n  21: namespace starrocks::pipeline {\\n  105: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/exchange/shuffler.h\\n  23: namespace starrocks::pipeline {\\n  91: // while `exchange source -> local exchange` (phase 2) determines which pipeline driver to deliver each row.\\n  109: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/exchange/local_exchange.h\\n  22: #include \\\"exec/pipeline/exchange/local_exchange_source_operator.h\\\"\\n  23: #include \\\"exec/pipeline/exchange/shuffler.h\\\"\\n  31: namespace pipeline {\\n  175: void attach_sink_observer(RuntimeState* state, pipeline::PipelineObserver* observer) {\\n  368: } // namespace pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/exchange/local_exchange_source_operator.h\\n  22: #include \\\"exec/pipeline/source_operator.h\\\"\\n  24: namespace starrocks::pipeline {\\n  175: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/aggregate/spillable_aggregate_blocking_sink_operator.h\\n  20: #include \\\"exec/pipeline/operator.h\\\"\\n  21: #include \\\"exec/pipeline/spill_process_channel.h\\\"\\n  26: namespace starrocks::pipeline {\\n  127: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/aggregate/spillable_aggregate_blocking_source_operator.cpp\\n  15: #include \\\"exec/pipeline/aggregate/spillable_aggregate_blocking_source_operator.h\\\"\\n  20: #include \\\"exec/pipeline/aggregate/aggregate_blocking_source_operator.h\\\"\\n  22: namespace starrocks::pipeline {\\n  170: } // namespace starrocks::pipeline\"\n}",
          "role": "tool",
          "tool_call_id": "call_MHxjQkJTbXFRT08zZHhGWDVoTHU"
        },
        {
          "content": null,
          "role": "assistant",
          "tool_calls": [
            {
              "function": {
                "arguments": "{\"directory_path\":\"/Users/kks/git/starrocks/be/src/exec/pipeline/\"}",
                "name": "listFiles"
              },
              "id": "call_MHw1R0paaXJVMzdmQ29BdDBna1E",
              "type": "function"
            }
          ]
        },
        {
          "content": "dirs: adaptive; aggregate; analysis; exchange; group_execution; hashjoin; nljoin; scan; schedule; set; sink; sort; README.md; assert_num_rows_operator.cpp; assert_num_rows_operator.h; audit_statistics_reporter.cpp; audit_statistics_reporter.h; bucket_process_operator.cpp; bucket_process_operator.h; capture_version_operator.cpp; capture_version_operator.h; chunk_accumulate_operator.cpp; chunk_accumulate_operator.h; context_with_dependency.h; dict_decode_operator.cpp; dict_decode_operator.h; driver_limiter.cpp; driver_limiter.h; driver_source.h; empty_set_operator.h; exec_state_reporter.cpp; exec_state_reporter.h; fetch_processor.cpp; fetch_processor.h; fetch_sink_operator.cpp; fetch_sink_operator.h; fetch_source_operator.cpp; fetch_source_operator.h; fetch_task.cpp; fetch_task.h; fragment_context.cpp; fragment_context.h; fragment_executor.cpp; fragment_executor.h; hash_partition_context.cpp; hash_partition_context.h; hash_partition_sink_operator.cpp; hash_partition_sink_operator.h; hash_partition_source_operator.cpp; hash_partition_source_operator.h; limit_operator.cpp; limit_operator.h; lookup_operator.cpp; lookup_operator.h; lookup_request.cpp; lookup_request.h; noop_sink_operator.h; offset_operator.cpp; offset_operator.h; operator.cpp; operator.h; operator_with_dependency.h; pipeline.cpp; pipeline.h; pipeline_builder.cpp; pipeline_builder.h; pipeline_driver.cpp; pipeline_driver.h; pipeline_driver_executor.cpp; pipeline_driver_executor.h; pipeline_driver_poller.cpp; pipeline_driver_poller.h; pipeline_driver_queue.cpp; pipeline_driver_queue.h; pipeline_fwd.h; pipeline_metrics.cpp; pipeline_metrics.h; project_operator.cpp; project_operator.h; query_context.cpp; query_context.h; runtime_filter_types.cpp; runtime_filter_types.h; select_operator.cpp; select_operator.h; source_operator.cpp; source_operator.h; spill_process_channel.cpp; spill_process_channel.h; spill_process_operator.cpp; spill_process_operator.h; stream_epoch_manager.cpp; stream_epoch_manager.h; stream_pipeline_driver.cpp; stream_pipeline_driver.h; table_function_operator.cpp; table_function_operator.h; topn_runtime_filter_back_pressure.h; wait_operator.cpp; wait_operator.h\n\nadaptive dirs: adaptive_dop_param.h; adaptive_fwd.h; collect_stats_context.cpp; collect_stats_context.h; collect_stats_sink_operator.cpp; collect_stats_sink_operator.h; collect_stats_source_operator.cpp; collect_stats_source_operator.h; event.cpp; event.h; utils.cpp; utils.h\n\naggregate dirs: repeat; aggregate_blocking_sink_operator.cpp; aggregate_blocking_sink_operator.h; aggregate_blocking_source_operator.cpp; aggregate_blocking_source_operator.h; aggregate_distinct_blocking_sink_operator.cpp; aggregate_distinct_blocking_sink_operator.h; aggregate_distinct_blocking_source_operator.cpp; aggregate_distinct_blocking_source_operator.h; aggregate_distinct_streaming_sink_operator.cpp; aggregate_distinct_streaming_sink_operator.h; aggregate_distinct_streaming_source_operator.cpp; aggregate_distinct_streaming_source_operator.h; aggregate_operators.cpp; aggregate_streaming_sink_operator.cpp; aggregate_streaming_sink_operator.h; aggregate_streaming_source_operator.cpp; aggregate_streaming_source_operator.h; sorted_aggregate_streaming_sink_operator.cpp; sorted_aggregate_streaming_sink_operator.h; sorted_aggregate_streaming_source_operator.cpp; sorted_aggregate_streaming_source_operator.h; spillable_aggregate_blocking_sink_operator.cpp; spillable_aggregate_blocking_sink_operator.h; spillable_aggregate_blocking_source_operator.cpp; spillable_aggregate_blocking_source_operator.h; spillable_aggregate_distinct_blocking_operator.cpp; spillable_aggregate_distinct_blocking_operator.h; spillable_partitionwise_aggregate_operator.cpp; spillable_partitionwise_aggregate_operator.h; spillable_partitionwise_distinct_operator.cpp; spillable_partitionwise_distinct_operator.h\n\naggregate/repeat dirs: repeat_operator.cpp; repeat_operator.h\n\nanalysis dirs: analytic_sink_operator.cpp; analytic_sink_operator.h; analytic_source_operator.cpp; analytic_source_operator.h\n\nexchange dirs: exchange_merge_sort_source_operator.cpp; exchange_merge_sort_source_operator.h; exchange_parallel_merge_source_operator.cpp; exchange_parallel_merge_source_operator.h; exchange_sink_operator.cpp; exchange_sink_operator.h; exchange_source_operator.cpp; exchange_source_operator.h; local_exchange.cpp; local_exchange.h; local_exchange_sink_operator.cpp; local_exchange_sink_operator.h; local_exchange_source_operator.cpp; local_exchange_source_operator.h; mem_limited_chunk_queue.cpp; mem_limited_chunk_queue.h; multi_cast_local_exchange.cpp; multi_cast_local_exchange.h; multi_cast_local_exchange_sink_operator.cpp; multi_cast_local_exchange_sink_operator.h; multi_cast_local_exchange_source_operator.cpp; multi_cast_local_exchange_source_operator.h; shuffler.h; sink_buffer.cpp; sink_buffer.h; spillable_multi_cast_local_exchange.cpp; split_local_exchange.cpp; split_local_exchange.h\n\ngroup_execution dirs: execution_group.cpp; execution_group.h; execution_group_builder.cpp; execution_group_builder.h; execution_group_fwd.h; group_operator.cpp; group_operator.h\n\nhashjoin dirs: hash_join_build_operator.cpp; hash_join_build_operator.h; hash_join_probe_operator.cpp; hash_join_probe_operator.h; hash_joiner_factory.cpp; hash_joiner_factory.h; hash_joiner_fwd.h; spillable_hash_join_build_operator.cpp; spillable_hash_join_build_operator.h; spillable_hash_join_probe_operator.cpp; spillable_hash_join_probe_operator.h\n\nnljoin dirs: nljoin_build_operator.cpp; nljoin_build_operator.h; nljoin_context.cpp; nljoin_context.h; nljoin_probe_operator.cpp; nljoin_probe_operator.h; spillable_nljoin_build_operator.cpp; spillable_nljoin_build_operator.h; spillable_nljoin_probe_operator.cpp; spillable_nljoin_probe_operator.h\n\nscan dirs: balanced_chunk_buffer.cpp; balanced_chunk_buffer.h; chunk_buffer_limiter.cpp; chunk_buffer_limiter.h; chunk_source.cpp; chunk_source.h; connector_scan_operator.cpp; connector_scan_operator.h; glm_manager.cpp; glm_manager.h; lake_meta_scan_prepare_operator.cpp; lake_meta_scan_prepare_operator.h; meta_chunk_source.cpp; meta_chunk_source.h; meta_scan_context.h; meta_scan_operator.cpp; meta_scan_operator.h; meta_scan_prepare_operator.cpp; meta_scan_prepare_operator.h; morsel.cpp; morsel.h; olap_chunk_source.cpp; olap_chunk_source.h; olap_meta_scan_context.h; olap_meta_scan_prepare_operator.cpp; olap_meta_scan_prepare_operator.h; olap_scan_context.cpp; olap_scan_context.h; olap_scan_operator.cpp; olap_scan_operator.h; olap_scan_prepare_operator.cpp; olap_scan_prepare_operator.h; scan_operator.cpp; scan_operator.h; schema_chunk_source.cpp; schema_chunk_source.h; schema_scan_context.cpp; schema_scan_context.h; schema_scan_operator.cpp; schema_scan_operator.h\n\nschedule dirs: common.h; event_scheduler.cpp; event_scheduler.h; observer.cpp; observer.h; pipeline_timer.cpp; pipeline_timer.h; timeout_tasks.cpp; timeout_tasks.h; utils.h\n\nset dirs: except_build_sink_operator.cpp; except_build_sink_operator.h; except_context.cpp; except_context.h; except_output_source_operator.cpp; except_output_source_operator.h; except_probe_sink_operator.cpp; except_probe_sink_operator.h; intersect_build_sink_operator.cpp; intersect_build_sink_operator.h; intersect_context.cpp; intersect_context.h; intersect_output_source_operator.cpp; intersect_output_source_operator.h; intersect_probe_sink_operator.cpp; intersect_probe_sink_operator.h; raw_values_source_operator.cpp; raw_values_source_operator.h; union_const_source_operator.cpp; union_const_source_operator.h; union_passthrough_operator.cpp; union_passthrough_operator.h\n\nsink dirs: blackhole_table_sink_operator.cpp; blackhole_table_sink_operator.h; connector_sink_operator.cpp; connector_sink_operator.h; dictionary_cache_sink_operator.cpp; dictionary_cache_sink_operator.h; export_sink_operator.cpp; export_sink_operator.h; file_sink_operator.cpp; file_sink_operator.h; memory_scratch_sink_operator.cpp; memory_scratch_sink_operator.h; mysql_table_sink_operator.cpp; mysql_table_sink_operator.h; olap_table_sink_operator.cpp; olap_table_sink_operator.h; result_sink_operator.cpp; result_sink_operator.h; sink_io_buffer.cpp; sink_io_buffer.h; table_function_table_sink_operator.cpp; table_function_table_sink_operator.h\n\nsort dirs: local_merge_sort_source_operator.cpp; local_merge_sort_source_operator.h; local_parallel_merge_sort_source_operator.cpp; local_parallel_merge_sort_source_operator.h; local_partition_topn_context.cpp; local_partition_topn_context.h; local_partition_topn_sink.cpp; local_partition_topn_sink.h; local_partition_topn_source.cpp; local_partition_topn_source.h; partition_sort_sink_operator.cpp; partition_sort_sink_operator.h; sort_context.cpp; sort_context.h; spillable_partition_sort_sink_operator.cpp; spillable_partition_sort_sink_operator.h",
          "role": "tool",
          "tool_call_id": "call_MHw1R0paaXJVMzdmQ29BdDBna1E"
        },
        {
          "content": null,
          "role": "assistant",
          "tool_calls": [
            {
              "function": {
                "arguments": "{\"file_path\":\"/Users/kks/git/starrocks/be/src/exec/pipeline/fragment_context.h\"}",
                "name": "readFile"
              },
              "id": "call_MHxkd1BEa3ppVTEzdWF5TnpORVk",
              "type": "function"
            }
          ]
        },
        {
          "content": "{\n  \"success\": true,\n  \"file_path\": \"/Users/kks/git/starrocks/be/src/exec/pipeline/fragment_context.h\",\n  \"content\": \"// Copyright 2021-present StarRocks, Inc. All rights reserved.\\n//\\n// Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\\n// you may not use this file except in compliance with the License.\\n// You may obtain a copy of the License at\\n//\\n//     https://www.apache.org/licenses/LICENSE-2.0\\n//\\n// Unless required by applicable law or agreed to in writing, software\\n// distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n// See the License for the specific language governing permissions and\\n// limitations under the License.\\n\\n#pragma once\\n\\n#include <memory>\\n#include <unordered_map>\\n\\n#include \\\"exec/exec_node.h\\\"\\n#include \\\"exec/pipeline/adaptive/adaptive_dop_param.h\\\"\\n#include \\\"exec/pipeline/driver_limiter.h\\\"\\n#include \\\"exec/pipeline/group_execution/execution_group_fwd.h\\\"\\n#include \\\"exec/pipeline/pipeline.h\\\"\\n#include \\\"exec/pipeline/pipeline_driver.h\\\"\\n#include \\\"exec/pipeline/pipeline_fwd.h\\\"\\n#include \\\"exec/pipeline/runtime_filter_types.h\\\"\\n#include \\\"exec/pipeline/scan/morsel.h\\\"\\n#include \\\"exec/pipeline/schedule/event_scheduler.h\\\"\\n#include \\\"exec/pipeline/schedule/observer.h\\\"\\n#include \\\"exec/pipeline/schedule/pipeline_timer.h\\\"\\n#include \\\"exec/query_cache/cache_param.h\\\"\\n#include \\\"gen_cpp/FrontendService.h\\\"\\n#include \\\"gen_cpp/HeartbeatService.h\\\"\\n#include \\\"gen_cpp/InternalService_types.h\\\"\\n#include \\\"gen_cpp/PlanNodes_types.h\\\"\\n#include \\\"gen_cpp/QueryPlanExtra_types.h\\\"\\n#include \\\"gen_cpp/Types_types.h\\\"\\n#include \\\"runtime/profile_report_worker.h\\\"\\n#include \\\"runtime/runtime_filter_worker.h\\\"\\n#include \\\"runtime/runtime_state.h\\\"\\n#include \\\"storage/predicate_tree_params.h\\\"\\n#include \\\"util/hash_util.hpp\\\"\\n\\nnamespace starrocks {\\n\\nclass StreamLoadContext;\\n\\nnamespace pipeline {\\n\\nusing RuntimeFilterPort = starrocks::RuntimeFilterPort;\\nusing PerDriverScanRangesMap = std::map<int32_t, std::vector<TScanRangeParams>>;\\n\\nclass FragmentContext {\\n    friend FragmentContextManager;\\n\\npublic:\\n    FragmentContext();\\n    ~FragmentContext();\\n    const TUniqueId& query_id() const { return _query_id; }\\n    void set_query_id(const TUniqueId& query_id) { _query_id = query_id; }\\n    const TUniqueId& fragment_instance_id() const { return _fragment_instance_id; }\\n    void set_fragment_instance_id(const TUniqueId& fragment_instance_id) {\\n        _fragment_instance_id = fragment_instance_id;\\n    }\\n    void set_fe_addr(const TNetworkAddress& fe_addr) { _fe_addr = fe_addr; }\\n    const TNetworkAddress& fe_addr() { return _fe_addr; }\\n    FragmentFuture finish_future() { return _finish_promise.get_future(); }\\n    RuntimeState* runtime_state() const { return _runtime_state.get(); }\\n    std::shared_ptr<RuntimeState> runtime_state_ptr() { return _runtime_state; }\\n    void set_runtime_state(std::shared_ptr<RuntimeState>&& runtime_state) { _runtime_state = std::move(runtime_state); }\\n    ExecNode*& plan() { return _plan; }\\n\\n    void move_tplan(TPlan& tplan);\\n    const TPlan& tplan() const { return _tplan; }\\n    void set_data_sink(std::unique_ptr<DataSink> data_sink);\\n\\n    size_t total_dop() const;\\n\\n    bool all_execution_groups_finished() const { return _num_finished_execution_groups == _execution_groups.size(); }\\n    void count_down_execution_group(size_t val = 1);\\n\\n    bool need_report_exec_state();\\n    void report_exec_state_if_necessary();\\n\\n    void set_final_status(const Status& status);\\n\\n    Status final_status() const {\\n        auto* status = _final_status.load();\\n        return status == nullptr ? Status::OK() : *status;\\n    }\\n\\n    void cancel(const Status& status, bool cancelled_by_fe = false);\\n\\n    void finish() { cancel(Status::OK()); }\\n\\n    bool is_canceled() const { return _runtime_state->is_cancelled(); }\\n\\n    MorselQueueFactoryMap& morsel_queue_factories() { return _morsel_queue_factories; }\\n\\n    void set_pipelines(ExecutionGroups&& exec_groups, Pipelines&& pipelines);\\n\\n    Status prepare_all_pipelines();\\n\\n    template <class Func>\\n    void iterate_drivers(Func&& call) {\\n        iterate_pipeline([&](const Pipeline* pipeline) {\\n            for (auto& driver : pipeline->drivers()) {\\n                call(driver);\\n            }\\n        });\\n    }\\n\\n    void clear_all_drivers();\\n    void close_all_execution_groups();\\n\\n    RuntimeFilterHub* runtime_filter_hub() { return &_runtime_filter_hub; }\\n\\n    RuntimeFilterPort* runtime_filter_port() { return _runtime_state->runtime_filter_port(); }\\n\\n    void prepare_pass_through_chunk_buffer();\\n    void destroy_pass_through_chunk_buffer();\\n\\n    void set_driver_token(DriverLimiter::TokenPtr driver_token) { _driver_token = std::move(driver_token); }\\n    Status set_pipeline_timer(PipelineTimer* pipeline_timer);\\n    void clear_pipeline_timer();\\n\\n    query_cache::CacheParam& cache_param() { return _cache_param; }\\n\\n    void set_enable_cache(bool flag) { _enable_cache = flag; }\\n\\n    bool enable_cache() const { return _enable_cache; }\\n\\n    void set_stream_load_contexts(const std::vector<StreamLoadContext*>& contexts);\\n\\n    void set_enable_adaptive_dop(bool val) { _enable_adaptive_dop = val; }\\n    bool enable_adaptive_dop() const { return _enable_adaptive_dop; }\\n    AdaptiveDopParam& adaptive_dop_param() { return _adaptive_dop_param; }\\n\\n    const PredicateTreeParams& pred_tree_params() const { return _pred_tree_params; }\\n    void set_pred_tree_params(PredicateTreeParams&& params) { _pred_tree_params = std::move(params); }\\n\\n    size_t next_driver_id() { return _next_driver_id++; }\\n\\n    void set_workgroup(workgroup::WorkGroupPtr wg) { _workgroup = std::move(wg); }\\n    const workgroup::WorkGroupPtr& workgroup() const { return _workgroup; }\\n    bool enable_resource_group() const { return _workgroup != nullptr; }\\n    TQueryType::type query_type() const;\\n\\n    // STREAM MV\\n    Status reset_epoch();\\n    void set_is_stream_pipeline(bool is_stream_pipeline) { _is_stream_pipeline = is_stream_pipeline; }\\n    bool is_stream_pipeline() const { return _is_stream_pipeline; }\\n    void count_down_epoch_pipeline(RuntimeState* state, size_t val = 1);\\n\\n#ifdef BE_TEST\\n    // for ut\\n    void set_is_stream_test(bool is_stream_test) { _is_stream_test = is_stream_test; }\\n    bool is_stream_test() const { return _is_stream_test; }\\n#endif\\n\\n    size_t expired_log_count() { return _expired_log_count; }\\n\\n    void set_expired_log_count(size_t val) { _expired_log_count = val; }\\n\\n    void init_jit_profile();\\n\\n    void update_jit_profile(int64_t time_ns);\\n\\n    void iterate_pipeline(const std::function<void(Pipeline*)>& call);\\n    Status iterate_pipeline(const std::function<Status(Pipeline*)>& call);\\n\\n    Status prepare_active_drivers();\\n    Status submit_active_drivers(DriverExecutor* executor);\\n\\n    bool enable_group_execution() const { return _enable_group_execution; }\\n    void set_enable_group_execution(bool enable_group_execution) { _enable_group_execution = enable_group_execution; }\\n\\n    void set_report_when_finish(bool report) { _report_when_finish = report; }\\n\\n    // acquire runtime filter from cache\\n    void acquire_runtime_filters();\\n\\n    bool enable_event_scheduler() const { return event_scheduler() != nullptr; }\\n    EventScheduler* event_scheduler() const { return _event_scheduler.get(); }\\n    void init_event_scheduler();\\n\\n    PipelineTimer* pipeline_timer() { return _pipeline_timer; }\\n    void add_timer_observer(PipelineObserver* observer, uint64_t timeout);\\n    Status submit_all_timer();\\n\\nprivate:\\n    void _close_stream_load_contexts();\\n\\n    bool _enable_group_execution = false;\\n    // Id of this query\\n    TUniqueId _query_id;\\n    // Id of this instance\\n    TUniqueId _fragment_instance_id;\\n    TNetworkAddress _fe_addr;\\n\\n    // Hold tplan data datasink from delivery request to create driver lazily\\n    // after delivery request has been finished.\\n    TPlan _tplan;\\n    std::unique_ptr<DataSink> _data_sink;\\n\\n    // promise used to determine whether fragment finished its execution\\n    FragmentPromise _finish_promise;\\n\\n    // never adjust the order of _runtime_state, _plan, _pipelines and _drivers, since\\n    // _plan depends on _runtime_state and _drivers depends on _runtime_state.\\n    std::shared_ptr<RuntimeState> _runtime_state = nullptr;\\n    ExecNode* _plan = nullptr; // lives in _runtime_state->obj_pool()\\n    size_t _next_driver_id = 0;\\n    Pipelines _pipelines;\\n    ExecutionGroups _execution_groups;\\n    std::atomic<size_t> _num_finished_execution_groups = 0;\\n\\n    std::unique_ptr<EventScheduler> _event_scheduler;\\n    PipelineTimer* _pipeline_timer = nullptr;\\n    PipelineTimerTask* _timeout_task = nullptr;\\n    PipelineTimerTask* _report_state_task = nullptr;\\n    std::unordered_map<uint64_t, PipelineTimerTask*> _rf_timeout_tasks;\\n\\n    RuntimeFilterHub _runtime_filter_hub;\\n\\n    MorselQueueFactoryMap _morsel_queue_factories;\\n    workgroup::WorkGroupPtr _workgroup = nullptr;\\n\\n    std::atomic<Status*> _final_status = nullptr;\\n    Status _s_status;\\n\\n    DriverLimiter::TokenPtr _driver_token = nullptr;\\n\\n    query_cache::CacheParam _cache_param;\\n    bool _enable_cache = false;\\n    std::vector<StreamLoadContext*> _stream_load_contexts;\\n\\n    // STREAM MV\\n    std::atomic<size_t> _num_finished_epoch_pipelines = 0;\\n    bool _is_stream_pipeline = false;\\n#ifdef BE_TEST\\n    bool _is_stream_test = false;\\n#endif\\n\\n    bool _enable_adaptive_dop = false;\\n    AdaptiveDopParam _adaptive_dop_param;\\n\\n    PredicateTreeParams _pred_tree_params;\\n\\n    size_t _expired_log_count = 0;\\n\\n    std::atomic<int64_t> _last_report_exec_state_ns = MonotonicNanos();\\n\\n    RuntimeProfile::Counter* _jit_counter = nullptr;\\n    RuntimeProfile::Counter* _jit_timer = nullptr;\\n\\n    bool _report_when_finish{};\\n};\\n\\nclass FragmentContextManager {\\npublic:\\n    FragmentContextManager() = default;\\n    ~FragmentContextManager() = default;\\n\\n    FragmentContextManager(const FragmentContextManager&) = delete;\\n    FragmentContextManager(FragmentContextManager&&) = delete;\\n    FragmentContextManager& operator=(const FragmentContextManager&) = delete;\\n    FragmentContextManager& operator=(FragmentContextManager&&) = delete;\\n\\n    FragmentContext* get_or_register(const TUniqueId& fragment_id);\\n    FragmentContextPtr get(const TUniqueId& fragment_id);\\n\\n    Status register_ctx(const TUniqueId& fragment_id, FragmentContextPtr fragment_ctx);\\n    void unregister(const TUniqueId& fragment_id);\\n\\n    void cancel(const Status& status);\\n\\n    template <class Caller>\\n    void for_each_fragment(Caller&& caller) {\\n        std::lock_guard guard(_lock);\\n        for (auto& [_, fragment] : _fragment_contexts) {\\n            caller(fragment);\\n        }\\n    }\\n\\nprivate:\\n    std::mutex _lock;\\n    std::unordered_map<TUniqueId, FragmentContextPtr> _fragment_contexts;\\n};\\n} // namespace pipeline\\n} // namespace starrocks\\n\",\n  \"message\": \"Successfully read 293 lines from file: /Users/kks/git/starrocks/be/src/exec/pipeline/fragment_context.h\"\n}",
          "role": "tool",
          "tool_call_id": "call_MHxkd1BEa3ppVTEzdWF5TnpORVk"
        },
        {
          "content": null,
          "role": "assistant",
          "tool_calls": [
            {
              "function": {
                "arguments": "{\"file_path\":\"/Users/kks/git/starrocks/be/src/exec/pipeline/pipeline.h\"}",
                "name": "readFile"
              },
              "id": "call_MHxNc1I3RnVYbWJZclBmVHRyeEM",
              "type": "function"
            }
          ]
        },
        {
          "content": "{\n  \"success\": true,\n  \"file_path\": \"/Users/kks/git/starrocks/be/src/exec/pipeline/pipeline.h\",\n  \"content\": \"// Copyright 2021-present StarRocks, Inc. All rights reserved.\\n//\\n// Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\\n// you may not use this file except in compliance with the License.\\n// You may obtain a copy of the License at\\n//\\n//     https://www.apache.org/licenses/LICENSE-2.0\\n//\\n// Unless required by applicable law or agreed to in writing, software\\n// distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n// See the License for the specific language governing permissions and\\n// limitations under the License.\\n\\n#pragma once\\n\\n#include <ctime>\\n#include <utility>\\n\\n#include \\\"exec/pipeline/adaptive/adaptive_fwd.h\\\"\\n#include \\\"exec/pipeline/group_execution/execution_group_fwd.h\\\"\\n#include \\\"exec/pipeline/operator.h\\\"\\n#include \\\"exec/pipeline/pipeline_fwd.h\\\"\\n#include \\\"exec/pipeline/source_operator.h\\\"\\n#include \\\"gutil/strings/substitute.h\\\"\\n\\nnamespace starrocks {\\n\\nclass RuntimeState;\\n\\nnamespace pipeline {\\n\\nclass Pipeline {\\npublic:\\n    Pipeline() = delete;\\n    Pipeline(uint32_t id, OpFactories op_factories, ExecutionGroupRawPtr execution_group);\\n\\n    uint32_t get_id() const { return _id; }\\n\\n    Operators create_operators(int32_t degree_of_parallelism, int32_t i) {\\n        Operators operators;\\n        for (const auto& factory : _op_factories) {\\n            operators.emplace_back(factory->create(degree_of_parallelism, i));\\n        }\\n        return operators;\\n    }\\n    void instantiate_drivers(RuntimeState* state);\\n    Drivers& drivers();\\n    const Drivers& drivers() const;\\n    void count_down_driver(RuntimeState* state);\\n    void clear_drivers();\\n\\n    SourceOperatorFactory* source_operator_factory() {\\n        DCHECK(!_op_factories.empty());\\n        return down_cast<SourceOperatorFactory*>(_op_factories[0].get());\\n    }\\n    const SourceOperatorFactory* source_operator_factory() const {\\n        DCHECK(!_op_factories.empty());\\n        return down_cast<SourceOperatorFactory*>(_op_factories[0].get());\\n    }\\n    OperatorFactory* sink_operator_factory() {\\n        DCHECK(!_op_factories.empty());\\n        return _op_factories[_op_factories.size() - 1].get();\\n    }\\n    size_t degree_of_parallelism() const;\\n\\n    RuntimeProfile* runtime_profile() { return _runtime_profile.get(); }\\n    void setup_pipeline_profile(RuntimeState* runtime_state);\\n    void setup_drivers_profile(const DriverPtr& driver);\\n\\n    Status prepare(RuntimeState* state) {\\n        for (auto& op : _op_factories) {\\n            RETURN_IF_ERROR(op->prepare(state));\\n        }\\n        return Status::OK();\\n    }\\n    void close(RuntimeState* state) {\\n        for (auto& op : _op_factories) {\\n            op->close(state);\\n        }\\n    }\\n\\n    void acquire_runtime_filter(RuntimeState* state) {\\n        for (auto& op : _op_factories) {\\n            op->acquire_runtime_filter(state);\\n        }\\n    }\\n\\n    std::string to_readable_string() const {\\n        std::stringstream ss;\\n        ss << \\\"operator-chain: [\\\";\\n        for (size_t i = 0; i < _op_factories.size(); ++i) {\\n            if (i == 0) {\\n                ss << _op_factories[i]->get_name();\\n            } else {\\n                ss << \\\" -> \\\" << _op_factories[i]->get_name();\\n            }\\n        }\\n        ss << \\\"]\\\";\\n        return ss.str();\\n    }\\n\\n    // STREAM MV\\n    Status reset_epoch(RuntimeState* state);\\n    void count_down_epoch_finished_driver(RuntimeState* state);\\n\\n    size_t output_amplification_factor() const;\\n    Event* pipeline_event() const { return _pipeline_event.get(); }\\n\\nprivate:\\n    uint32_t _id = 0;\\n    std::shared_ptr<RuntimeProfile> _runtime_profile = nullptr;\\n    OpFactories _op_factories;\\n    Drivers _drivers;\\n    std::atomic<size_t> _num_finished_drivers = 0;\\n\\n    EventPtr _pipeline_event;\\n    ExecutionGroupRawPtr _execution_group = nullptr;\\n    // STREAM MV\\n    std::atomic<size_t> _num_epoch_finished_drivers = 0;\\n};\\n\\n} // namespace pipeline\\n} // namespace starrocks\\n\",\n  \"message\": \"Successfully read 125 lines from file: /Users/kks/git/starrocks/be/src/exec/pipeline/pipeline.h\"\n}",
          "role": "tool",
          "tool_call_id": "call_MHxNc1I3RnVYbWJZclBmVHRyeEM"
        },
        {
          "content": null,
          "role": "assistant",
          "tool_calls": [
            {
              "function": {
                "arguments": "{\"file_path\":\"/Users/kks/git/starrocks/be/src/exec/pipeline/pipeline_builder.cpp\"}",
                "name": "readFile"
              },
              "id": "call_MHxXb1ZUSk5KcmRuVDBHc3hQWXk",
              "type": "function"
            }
          ]
        },
        {
          "content": "{\n  \"success\": true,\n  \"file_path\": \"/Users/kks/git/starrocks/be/src/exec/pipeline/pipeline_builder.cpp\",\n  \"content\": \"// Copyright 2021-present StarRocks, Inc. All rights reserved.\\n//\\n// Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\\n// you may not use this file except in compliance with the License.\\n// You may obtain a copy of the License at\\n//\\n//     https://www.apache.org/licenses/LICENSE-2.0\\n//\\n// Unless required by applicable law or agreed to in writing, software\\n// distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n// See the License for the specific language governing permissions and\\n// limitations under the License.\\n\\n#include \\\"exec/pipeline/pipeline_builder.h\\\"\\n\\n#include <memory>\\n\\n#include \\\"adaptive/event.h\\\"\\n#include \\\"common/config.h\\\"\\n#include \\\"exec/exec_node.h\\\"\\n#include \\\"exec/pipeline/adaptive/collect_stats_context.h\\\"\\n#include \\\"exec/pipeline/adaptive/collect_stats_sink_operator.h\\\"\\n#include \\\"exec/pipeline/adaptive/collect_stats_source_operator.h\\\"\\n#include \\\"exec/pipeline/chunk_accumulate_operator.h\\\"\\n#include \\\"exec/pipeline/exchange/exchange_source_operator.h\\\"\\n#include \\\"exec/pipeline/exchange/local_exchange_source_operator.h\\\"\\n#include \\\"exec/pipeline/group_execution/execution_group.h\\\"\\n#include \\\"exec/pipeline/group_execution/execution_group_fwd.h\\\"\\n#include \\\"exec/pipeline/group_execution/group_operator.h\\\"\\n#include \\\"exec/pipeline/limit_operator.h\\\"\\n#include \\\"exec/pipeline/noop_sink_operator.h\\\"\\n#include \\\"exec/pipeline/pipeline_fwd.h\\\"\\n#include \\\"exec/pipeline/scan/scan_operator.h\\\"\\n#include \\\"exec/pipeline/spill_process_operator.h\\\"\\n#include \\\"exec/pipeline/wait_operator.h\\\"\\n#include \\\"exec/query_cache/cache_manager.h\\\"\\n#include \\\"exec/query_cache/cache_operator.h\\\"\\n#include \\\"exec/query_cache/conjugate_operator.h\\\"\\n#include \\\"exec/query_cache/lane_arbiter.h\\\"\\n#include \\\"exec/query_cache/multilane_operator.h\\\"\\n\\nnamespace starrocks::pipeline {\\n\\nvoid PipelineBuilderContext::init_colocate_groups(std::unordered_map<int32_t, ExecutionGroupPtr>&& colocate_groups) {\\n    _group_id_to_colocate_groups = std::move(colocate_groups);\\n    for (auto& [group_id, group] : _group_id_to_colocate_groups) {\\n        _execution_groups.emplace_back(group);\\n    }\\n}\\n\\nExecutionGroupRawPtr PipelineBuilderContext::find_exec_group_by_plan_node_id(int32_t plan_node_id) {\\n    for (auto& [group_id, group] : _group_id_to_colocate_groups) {\\n        if (group->contains(plan_node_id)) {\\n            return group.get();\\n        }\\n    }\\n    return _normal_exec_group;\\n}\\n\\n/// PipelineBuilderContext.\\nOpFactories PipelineBuilderContext::maybe_interpolate_local_broadcast_exchange(RuntimeState* state,\\n                                                                               int32_t plan_node_id,\\n                                                                               OpFactories& pred_operators,\\n                                                                               int num_receivers) {\\n    if (num_receivers == 1) {\\n        return maybe_interpolate_local_passthrough_exchange(state, plan_node_id, pred_operators);\\n    }\\n\\n    auto mem_mgr = std::make_shared<ChunkBufferMemoryManager>(num_receivers,\\n                                                              config::local_exchange_buffer_mem_limit_per_driver);\\n    auto local_exchange_source =\\n            std::make_shared<LocalExchangeSourceOperatorFactory>(next_operator_id(), plan_node_id, mem_mgr);\\n    local_exchange_source->set_runtime_state(state);\\n    inherit_upstream_source_properties(local_exchange_source.get(), source_operator(pred_operators));\\n    local_exchange_source->set_could_local_shuffle(true);\\n    local_exchange_source->set_degree_of_parallelism(num_receivers);\\n\\n    auto local_exchange = std::make_shared<BroadcastExchanger>(mem_mgr, local_exchange_source.get());\\n    auto local_exchange_sink =\\n            std::make_shared<LocalExchangeSinkOperatorFactory>(next_operator_id(), plan_node_id, local_exchange);\\n    pred_operators.emplace_back(std::move(local_exchange_sink));\\n    add_pipeline(pred_operators);\\n\\n    return {std::move(local_exchange_source)};\\n}\\n\\nOpFactories PipelineBuilderContext::maybe_interpolate_local_passthrough_exchange(RuntimeState* state,\\n                                                                                 int32_t plan_node_id,\\n                                                                                 OpFactories& pred_operators) {\\n    return maybe_interpolate_local_passthrough_exchange(state, plan_node_id, pred_operators, 1);\\n}\\n\\nOpFactories PipelineBuilderContext::maybe_interpolate_local_passthrough_exchange(RuntimeState* state,\\n                                                                                 int32_t plan_node_id,\\n                                                                                 OpFactories& pred_operators,\\n                                                                                 int num_receivers, bool force) {\\n    return _maybe_interpolate_local_passthrough_exchange(state, plan_node_id, pred_operators, num_receivers, force,\\n                                                         LocalExchanger::PassThroughType::CHUNK);\\n}\\n\\nOpFactories PipelineBuilderContext::maybe_interpolate_local_passthrough_exchange(\\n        RuntimeState* state, int32_t plan_node_id, OpFactories& pred_operators, int num_receivers,\\n        LocalExchanger::PassThroughType pass_through_type) {\\n    return _maybe_interpolate_local_passthrough_exchange(state, plan_node_id, pred_operators, num_receivers, false,\\n                                                         pass_through_type);\\n}\\n\\nOpFactories PipelineBuilderContext::maybe_interpolate_local_random_passthrough_exchange(RuntimeState* state,\\n                                                                                        int32_t plan_node_id,\\n                                                                                        OpFactories& pred_operators,\\n                                                                                        int num_receivers, bool force) {\\n    return _maybe_interpolate_local_passthrough_exchange(state, plan_node_id, pred_operators, num_receivers, force,\\n                                                         LocalExchanger::PassThroughType::RANDOM);\\n}\\n\\nOpFactories PipelineBuilderContext::maybe_interpolate_local_adpative_passthrough_exchange(\\n        RuntimeState* state, int32_t plan_node_id, OpFactories& pred_operators, int num_receivers, bool force) {\\n    return _maybe_interpolate_local_passthrough_exchange(state, plan_node_id, pred_operators, num_receivers, force,\\n                                                         LocalExchanger::PassThroughType::ADPATIVE);\\n}\\n\\nOpFactories PipelineBuilderContext::_maybe_interpolate_local_passthrough_exchange(\\n        RuntimeState* state, int32_t plan_node_id, OpFactories& pred_operators, int num_receivers, bool force,\\n        LocalExchanger::PassThroughType pass_through_type) {\\n    // predecessor pipeline has multiple drivers that will produce multiple output streams, but sort operator is\\n    // not parallelized now and can not accept multiple streams as input, so add a LocalExchange to gather multiple\\n    // streams and produce one output stream piping into the sort operator.\\n    DCHECK(!pred_operators.empty() && pred_operators[0]->is_source());\\n    auto* source_op = source_operator(pred_operators);\\n    if (!force && source_op->degree_of_parallelism() == num_receivers && !source_op->is_skewed()) {\\n        return pred_operators;\\n    }\\n\\n    pred_operators = maybe_interpolate_grouped_exchange(plan_node_id, pred_operators);\\n\\n    int max_input_dop = std::max(num_receivers, static_cast<int>(source_op->degree_of_parallelism()));\\n    auto mem_mgr = std::make_shared<ChunkBufferMemoryManager>(max_input_dop,\\n                                                              config::local_exchange_buffer_mem_limit_per_driver);\\n    auto local_exchange_source =\\n            std::make_shared<LocalExchangeSourceOperatorFactory>(next_operator_id(), plan_node_id, mem_mgr);\\n    local_exchange_source->set_runtime_state(state);\\n    inherit_upstream_source_properties(local_exchange_source.get(), source_op);\\n    local_exchange_source->set_could_local_shuffle(true);\\n    local_exchange_source->set_degree_of_parallelism(num_receivers);\\n\\n    std::shared_ptr<LocalExchanger> local_exchange;\\n    if (pass_through_type == LocalExchanger::PassThroughType::ADPATIVE) {\\n        local_exchange = std::make_shared<AdaptivePassthroughExchanger>(mem_mgr, local_exchange_source.get());\\n    } else if (pass_through_type == LocalExchanger::PassThroughType::RANDOM) {\\n        local_exchange = std::make_shared<RandomPassthroughExchanger>(mem_mgr, local_exchange_source.get());\\n    } else if (state->query_options().__isset.enable_connector_sink_writer_scaling &&\\n               state->query_options().enable_connector_sink_writer_scaling &&\\n               pass_through_type == LocalExchanger::PassThroughType::SCALE) {\\n        local_exchange = std::make_shared<ConnectorSinkPassthroughExchanger>(mem_mgr, local_exchange_source.get());\\n    } else {\\n        local_exchange = std::make_shared<PassthroughExchanger>(mem_mgr, local_exchange_source.get());\\n    }\\n    auto local_exchange_sink =\\n            std::make_shared<LocalExchangeSinkOperatorFactory>(next_operator_id(), plan_node_id, local_exchange);\\n    pred_operators.emplace_back(std::move(local_exchange_sink));\\n    add_pipeline(pred_operators);\\n\\n    return {std::move(local_exchange_source)};\\n}\\n\\nOpFactories PipelineBuilderContext::interpolate_local_key_partition_exchange(\\n        RuntimeState* state, int32_t plan_node_id, OpFactories& pred_operators,\\n        const std::vector<ExprContext*>& partition_expr_ctxs, int num_receivers,\\n        const std::vector<std::string>& transform_exprs) {\\n    auto* pred_source_op = source_operator(pred_operators);\\n    size_t source_dop = pred_source_op->degree_of_parallelism();\\n    auto mem_mgr =\\n            std::make_shared<ChunkBufferMemoryManager>(source_dop, config::local_exchange_buffer_mem_limit_per_driver);\\n    auto local_shuffle_source =\\n            std::make_shared<LocalExchangeSourceOperatorFactory>(next_operator_id(), plan_node_id, mem_mgr);\\n    auto local_exchanger = std::make_shared<KeyPartitionExchanger>(mem_mgr, local_shuffle_source.get(),\\n                                                                   partition_expr_ctxs, source_dop, transform_exprs);\\n    auto local_shuffle_sink =\\n            std::make_shared<LocalExchangeSinkOperatorFactory>(next_operator_id(), plan_node_id, local_exchanger);\\n    pred_operators.emplace_back(std::move(local_shuffle_sink));\\n    add_pipeline(pred_operators);\\n\\n    local_shuffle_source->set_runtime_state(state);\\n    inherit_upstream_source_properties(local_shuffle_source.get(), pred_source_op);\\n    local_shuffle_source->set_degree_of_parallelism(num_receivers);\\n\\n    return {std::move(local_shuffle_source)};\\n}\\n\\nOpFactories PipelineBuilderContext::maybe_interpolate_local_shuffle_exchange(\\n        RuntimeState* state, int32_t plan_node_id, OpFactories& pred_operators,\\n        const std::vector<ExprContext*>& self_partition_exprs) {\\n    return maybe_interpolate_local_shuffle_exchange(state, plan_node_id, pred_operators,\\n                                                    [&self_partition_exprs]() { return self_partition_exprs; });\\n}\\n\\nOpFactories PipelineBuilderContext::maybe_interpolate_local_shuffle_exchange(\\n        RuntimeState* state, int32_t plan_node_id, OpFactories& pred_operators,\\n        const PartitionExprsGenerator& self_partition_exprs_generator) {\\n    auto* source_op = source_operator(pred_operators);\\n    if (!source_op->could_local_shuffle()) {\\n        return pred_operators;\\n    }\\n\\n    if (!source_op->partition_exprs().empty()) {\\n        return _do_maybe_interpolate_local_shuffle_exchange(state, plan_node_id, pred_operators,\\n                                                            source_op->partition_exprs(), source_op->partition_type(),\\n                                                            source_op->get_bucket_properties());\\n    }\\n\\n    return _do_maybe_interpolate_local_shuffle_exchange(state, plan_node_id, pred_operators,\\n                                                        self_partition_exprs_generator(), source_op->partition_type(),\\n                                                        source_op->get_bucket_properties());\\n}\\n\\nOpFactories PipelineBuilderContext::maybe_interpolate_local_bucket_shuffle_exchange(\\n        RuntimeState* state, int32_t plan_node_id, OpFactories& pred_operators,\\n        const std::vector<ExprContext*>& partition_expr_ctxs) {\\n    auto* source_op = source_operator(pred_operators);\\n    if (!source_op->could_local_shuffle()) {\\n        return pred_operators;\\n    }\\n    return _do_maybe_interpolate_local_shuffle_exchange(state, plan_node_id, pred_operators, partition_expr_ctxs,\\n                                                        TPartitionType::BUCKET_SHUFFLE_HASH_PARTITIONED,\\n                                                        source_op->get_bucket_properties());\\n}\\n\\nOpFactories PipelineBuilderContext::_do_maybe_interpolate_local_shuffle_exchange(\\n        RuntimeState* state, int32_t plan_node_id, OpFactories& pred_operators,\\n        const std::vector<ExprContext*>& partition_expr_ctxs, const TPartitionType::type part_type,\\n        const std::vector<TBucketProperty>& bucket_properties) {\\n    DCHECK(!pred_operators.empty() && pred_operators[0]->is_source());\\n\\n    // interpolate grouped exchange if needed\\n    // TODO: If the local exchange supports spills, we don't need to prevent group execution\\n    pred_operators = maybe_interpolate_grouped_exchange(plan_node_id, pred_operators);\\n\\n    // If DOP is one, we needn't partition input chunks.\\n    size_t shuffle_partitions_num = degree_of_parallelism();\\n    if (shuffle_partitions_num <= 1) {\\n        return pred_operators;\\n    }\\n\\n    auto* pred_source_op = source_operator(pred_operators);\\n    int64_t limit_size = _prev_limit_size(pred_operators);\\n\\n    // To make sure at least one partition source operator is ready to output chunk before sink operators are full.\\n    auto mem_mgr = std::make_shared<ChunkBufferMemoryManager>(shuffle_partitions_num,\\n                                                              config::local_exchange_buffer_mem_limit_per_driver);\\n    auto local_shuffle_source =\\n            std::make_shared<LocalExchangeSourceOperatorFactory>(next_operator_id(), plan_node_id, mem_mgr);\\n    local_shuffle_source->set_runtime_state(state);\\n    inherit_upstream_source_properties(local_shuffle_source.get(), pred_source_op);\\n    local_shuffle_source->set_could_local_shuffle(pred_source_op->partition_exprs().empty() &&\\n                                                  bucket_properties.empty());\\n    local_shuffle_source->set_degree_of_parallelism(shuffle_partitions_num);\\n\\n    auto local_shuffle = std::make_shared<PartitionExchanger>(mem_mgr, local_shuffle_source.get(), part_type,\\n                                                              partition_expr_ctxs, bucket_properties);\\n    auto local_shuffle_sink =\\n            std::make_shared<LocalExchangeSinkOperatorFactory>(next_operator_id(), plan_node_id, local_shuffle);\\n    pred_operators.emplace_back(std::move(local_shuffle_sink));\\n    add_pipeline(pred_operators);\\n\\n    OpFactories source_operators = {std::move(local_shuffle_source)};\\n    _try_interpolate_limit_operator(plan_node_id, source_operators, limit_size);\\n    return source_operators;\\n}\\n\\nOpFactories PipelineBuilderContext::maybe_interpolate_local_ordered_partition_exchange(\\n        RuntimeState* state, int32_t plan_node_id, OpFactories& pred_operators,\\n        const std::vector<ExprContext*>& partition_expr_ctxs) {\\n    DCHECK(!pred_operators.empty() && pred_operators[0]->is_source());\\n\\n    // interpolate grouped exchange if needed\\n    // TODO: If the local exchange supports spills, we don't need to prevent group execution\\n    pred_operators = maybe_interpolate_grouped_exchange(plan_node_id, pred_operators);\\n\\n    // If DOP is one, we needn't partition input chunks.\\n    size_t shuffle_partitions_num = degree_of_parallelism();\\n    if (shuffle_partitions_num <= 1) {\\n        return pred_operators;\\n    }\\n\\n    auto* pred_source_op = source_operator(pred_operators);\\n    int64_t limit_size = _prev_limit_size(pred_operators);\\n\\n    auto mem_mgr = std::make_shared<ChunkBufferMemoryManager>(shuffle_partitions_num,\\n                                                              config::local_exchange_buffer_mem_limit_per_driver);\\n    auto local_shuffle_source =\\n            std::make_shared<LocalExchangeSourceOperatorFactory>(next_operator_id(), plan_node_id, mem_mgr);\\n    local_shuffle_source->set_runtime_state(state);\\n    inherit_upstream_source_properties(local_shuffle_source.get(), pred_source_op);\\n    local_shuffle_source->set_could_local_shuffle(pred_source_op->partition_exprs().empty());\\n    local_shuffle_source->set_degree_of_parallelism(shuffle_partitions_num);\\n\\n    auto local_shuffle =\\n            std::make_shared<OrderedPartitionExchanger>(mem_mgr, local_shuffle_source.get(), partition_expr_ctxs);\\n    auto local_shuffle_sink =\\n            std::make_shared<LocalExchangeSinkOperatorFactory>(next_operator_id(), plan_node_id, local_shuffle);\\n\\n    pred_operators.emplace_back(std::move(local_shuffle_sink));\\n    add_pipeline(pred_operators);\\n\\n    OpFactories source_operators = {std::move(local_shuffle_source)};\\n    _try_interpolate_limit_operator(plan_node_id, source_operators, limit_size);\\n    return source_operators;\\n}\\n\\nvoid PipelineBuilderContext::interpolate_spill_process(size_t plan_node_id,\\n                                                       const SpillProcessChannelFactoryPtr& spill_channel_factory,\\n                                                       size_t dop) {\\n    OpFactories spill_process_operators;\\n    auto spill_process_factory = std::make_shared<SpillProcessOperatorFactory>(next_operator_id(), \\\"spill_process\\\",\\n                                                                               plan_node_id, spill_channel_factory);\\n    spill_process_factory->set_degree_of_parallelism(dop);\\n    spill_process_operators.emplace_back(std::move(spill_process_factory));\\n    auto noop_sink_factory = std::make_shared<NoopSinkOperatorFactory>(next_operator_id(), plan_node_id);\\n    spill_process_operators.emplace_back(std::move(noop_sink_factory));\\n    add_pipeline(std::move(spill_process_operators));\\n}\\n\\nOpFactories PipelineBuilderContext::interpolate_grouped_exchange(int32_t plan_node_id, OpFactories& pred_operators) {\\n    size_t physical_dop = degree_of_parallelism();\\n    auto* source_op = source_operator(pred_operators);\\n    int logical_dop = source_op->degree_of_parallelism();\\n\\n    // check should interpolate limit operator\\n    int64_t limit_size = _prev_limit_size(pred_operators);\\n\\n    auto mem_mgr =\\n            std::make_shared<ChunkBufferMemoryManager>(logical_dop, config::local_exchange_buffer_mem_limit_per_driver);\\n\\n    auto local_shuffle_source =\\n            std::make_shared<LocalExchangeSourceOperatorFactory>(next_operator_id(), plan_node_id, mem_mgr);\\n    auto local_exchanger = std::make_shared<PassthroughExchanger>(mem_mgr, local_shuffle_source.get());\\n    auto group_exchange_sink = std::make_shared<GroupedExecutionSinkFactory>(next_operator_id(), plan_node_id,\\n                                                                             local_exchanger, _current_execution_group);\\n    pred_operators.emplace_back(std::move(group_exchange_sink));\\n\\n    auto prev_source_operator = source_operator(pred_operators);\\n    inherit_upstream_source_properties(local_shuffle_source.get(), prev_source_operator);\\n    local_shuffle_source->set_could_local_shuffle(true);\\n    local_shuffle_source->set_degree_of_parallelism(physical_dop);\\n    add_pipeline(pred_operators);\\n    // switch to new normal group\\n    _current_execution_group = _normal_exec_group;\\n\\n    OpFactories source_operators = {std::move(local_shuffle_source)};\\n    _try_interpolate_limit_operator(plan_node_id, source_operators, limit_size);\\n    return source_operators;\\n}\\n\\nOpFactories PipelineBuilderContext::maybe_interpolate_grouped_exchange(int32_t plan_node_id,\\n                                                                       OpFactories& pred_operators) {\\n    if (dynamic_cast<ColocateExecutionGroup*>(_current_execution_group) != nullptr) {\\n        return interpolate_grouped_exchange(plan_node_id, pred_operators);\\n    }\\n    return pred_operators;\\n}\\n\\nOpFactories PipelineBuilderContext::maybe_gather_pipelines_to_one(RuntimeState* state, int32_t plan_node_id,\\n                                                                  std::vector<OpFactories>& pred_operators_list,\\n                                                                  LocalExchanger::PassThroughType pass_through_type) {\\n    // If there is only one pred pipeline, we needn't local passthrough anymore.\\n    if (pred_operators_list.size() == 1) {\\n        return pred_operators_list[0];\\n    }\\n\\n    // Approximately, each pred driver can output state->chunk_size() rows at the same time.\\n    size_t max_input_dop = 0;\\n    for (const auto& pred_ops : pred_operators_list) {\\n        auto* source_op = source_operator(pred_ops);\\n        max_input_dop += source_op->degree_of_parallelism();\\n    }\\n\\n    auto mem_mgr = std::make_shared<ChunkBufferMemoryManager>(max_input_dop,\\n                                                              config::local_exchange_buffer_mem_limit_per_driver);\\n    auto local_exchange_source =\\n            std::make_shared<LocalExchangeSourceOperatorFactory>(next_operator_id(), plan_node_id, mem_mgr);\\n    local_exchange_source->set_runtime_state(state);\\n    auto* first_upstream_source = source_operator(pred_operators_list[0]);\\n    inherit_upstream_source_properties(local_exchange_source.get(), first_upstream_source);\\n    local_exchange_source->set_could_local_shuffle(true);\\n    local_exchange_source->set_degree_of_parallelism(degree_of_parallelism());\\n\\n    std::vector<EventPtr> group_blocking_events;\\n    for (const auto& pred_ops : pred_operators_list) {\\n        auto* source = source_operator(pred_ops);\\n        if (auto event = source->group_leader()->adaptive_blocking_event(); event != nullptr) {\\n            group_blocking_events.emplace_back(std::move(event));\\n        }\\n    }\\n\\n    for (int i = 1; i < pred_operators_list.size(); i++) {\\n        auto* upstream_source = source_operator(pred_operators_list[i]);\\n        local_exchange_source->add_upstream_source(upstream_source);\\n        first_upstream_source->union_group(upstream_source);\\n    }\\n\\n    if (!group_blocking_events.empty()) {\\n        EventPtr merged_blocking_events = Event::depends_all(group_blocking_events);\\n        local_exchange_source->group_leader()->set_adaptive_blocking_event(std::move(merged_blocking_events));\\n    }\\n\\n    std::shared_ptr<LocalExchanger> exchanger;\\n    if (pass_through_type == LocalExchanger::PassThroughType::RANDOM) {\\n        exchanger = std::make_shared<PassthroughExchanger>(mem_mgr, local_exchange_source.get());\\n    } else {\\n        exchanger = std::make_shared<DirectThroughExchanger>(mem_mgr, local_exchange_source.get());\\n    }\\n\\n    for (auto& pred_operators : pred_operators_list) {\\n        auto local_exchange_sink =\\n                std::make_shared<LocalExchangeSinkOperatorFactory>(next_operator_id(), plan_node_id, exchanger);\\n        pred_operators.emplace_back(std::move(local_exchange_sink));\\n        add_pipeline(pred_operators);\\n    }\\n\\n    return {std::move(local_exchange_source)};\\n}\\n\\nOpFactories PipelineBuilderContext::maybe_interpolate_collect_stats(RuntimeState* state, int32_t plan_node_id,\\n                                                                    OpFactories& pred_operators) {\\n    if (!_fragment_context->enable_adaptive_dop()) {\\n        return pred_operators;\\n    }\\n\\n    if (pred_operators.empty()) {\\n        return pred_operators;\\n    }\\n\\n    auto* pred_source_op = source_operator(pred_operators);\\n    size_t dop = pred_source_op->degree_of_parallelism();\\n    CollectStatsContextPtr collect_stats_ctx =\\n            std::make_shared<CollectStatsContext>(state, dop, _fragment_context->adaptive_dop_param());\\n\\n    auto last_plan_node_id = pred_operators[pred_operators.size() - 1]->plan_node_id();\\n    pred_operators.emplace_back(std::make_shared<CollectStatsSinkOperatorFactory>(next_operator_id(), last_plan_node_id,\\n                                                                                  collect_stats_ctx));\\n    add_pipeline(std::move(pred_operators));\\n\\n    auto downstream_source_op = std::make_shared<CollectStatsSourceOperatorFactory>(\\n            next_operator_id(), last_plan_node_id, std::move(collect_stats_ctx));\\n    inherit_upstream_source_properties(downstream_source_op.get(), pred_source_op);\\n    downstream_source_op->set_partition_exprs(pred_source_op->partition_exprs());\\n\\n    for (const auto& pipeline : _dependent_pipelines) {\\n        downstream_source_op->add_group_dependent_pipeline(pipeline);\\n    }\\n\\n    return {std::move(downstream_source_op)};\\n}\\n\\nOpFactories PipelineBuilderContext::maybe_interpolate_debug_ops(RuntimeState* state, int32_t plan_node_id,\\n                                                                OpFactories& pred_operators) {\\n    auto action_opt = runtime_state()->debug_action_mgr().get_debug_action(plan_node_id);\\n    if (action_opt.has_value() && action_opt.value().is_wait_action()) {\\n        auto* pred_source_op = source_operator(pred_operators);\\n        auto wait_context_factory = std::make_shared<WaitContextFactory>(action_opt->value);\\n        auto wait_sink =\\n                std::make_shared<WaitOperatorSinkFactory>(next_operator_id(), plan_node_id, wait_context_factory);\\n\\n        pred_operators.push_back(std::move(wait_sink));\\n        add_pipeline(pred_operators);\\n\\n        auto wait_src =\\n                std::make_shared<WaitOperatorSourceFactory>(next_operator_id(), plan_node_id, wait_context_factory);\\n        this->inherit_upstream_source_properties(wait_src.get(), pred_source_op);\\n        return {std::move(wait_src)};\\n    }\\n    return pred_operators;\\n}\\n\\nsize_t PipelineBuilderContext::dop_of_source_operator(int source_node_id) {\\n    auto* morsel_queue_factory = morsel_queue_factory_of_source_operator(source_node_id);\\n    return morsel_queue_factory->size();\\n}\\n\\nMorselQueueFactory* PipelineBuilderContext::morsel_queue_factory_of_source_operator(int source_node_id) {\\n    auto& morsel_queues = _fragment_context->morsel_queue_factories();\\n    DCHECK(morsel_queues.count(source_node_id));\\n    return morsel_queues[source_node_id].get();\\n}\\n\\nMorselQueueFactory* PipelineBuilderContext::morsel_queue_factory_of_source_operator(\\n        const SourceOperatorFactory* source_op) {\\n    if (!source_op->with_morsels()) {\\n        return nullptr;\\n    }\\n\\n    return morsel_queue_factory_of_source_operator(source_op->plan_node_id());\\n}\\n\\nSourceOperatorFactory* PipelineBuilderContext::source_operator(const OpFactories& ops) {\\n    return down_cast<SourceOperatorFactory*>(ops[0].get());\\n}\\n\\nbool PipelineBuilderContext::could_local_shuffle(const OpFactories& ops) const {\\n    return down_cast<SourceOperatorFactory*>(ops[0].get())->could_local_shuffle();\\n}\\n\\nbool PipelineBuilderContext::should_interpolate_cache_operator(int32_t plan_node_id, OpFactoryPtr& source_op) {\\n    if (!_fragment_context->enable_cache()) {\\n        return false;\\n    }\\n    const auto& cache_param = _fragment_context->cache_param();\\n    if (cache_param.plan_node_id != plan_node_id) {\\n        return false;\\n    }\\n    return dynamic_cast<pipeline::ScanOperatorFactory*>(source_op.get()) != nullptr;\\n}\\n\\nOpFactories PipelineBuilderContext::interpolate_cache_operator(\\n        int32_t plan_node_id, OpFactories& upstream_pipeline, OpFactories& downstream_pipeline,\\n        const std::function<std::tuple<OpFactoryPtr, SourceOperatorFactoryPtr>(bool)>& merge_operators_generator) {\\n    DCHECK(should_interpolate_cache_operator(downstream_pipeline[0]->plan_node_id(), upstream_pipeline[0]));\\n\\n    const auto& cache_param = _fragment_context->cache_param();\\n\\n    auto sink_op = upstream_pipeline.back();\\n    upstream_pipeline.pop_back();\\n    auto source_op = downstream_pipeline.front();\\n    downstream_pipeline.clear();\\n\\n    auto dop = down_cast<SourceOperatorFactory*>(source_op.get())->degree_of_parallelism();\\n    auto conjugate_op = std::make_shared<query_cache::ConjugateOperatorFactory>(sink_op, source_op);\\n    upstream_pipeline.push_back(std::move(conjugate_op));\\n\\n    auto last_ml_op_idx = upstream_pipeline.size() - 1;\\n    for (auto i = 1; i < upstream_pipeline.size(); ++i) {\\n        auto& op = upstream_pipeline[i];\\n        auto ml_op =\\n                std::make_shared<query_cache::MultilaneOperatorFactory>(next_operator_id(), op, cache_param.num_lanes);\\n        // only last multilane operator in the pipeline driver can work in passthrough mode\\n        auto can_passthrough = i == last_ml_op_idx;\\n        ml_op->set_can_passthrough(can_passthrough);\\n        upstream_pipeline[i] = std::move(ml_op);\\n    }\\n\\n    auto cache_mgr = ExecEnv::GetInstance()->cache_mgr();\\n    auto cache_op = std::make_shared<query_cache::CacheOperatorFactory>(next_operator_id(), plan_node_id, cache_mgr,\\n                                                                        cache_param);\\n    upstream_pipeline.push_back(cache_op);\\n\\n    auto merge_operators = merge_operators_generator(true);\\n    upstream_pipeline.push_back(std::move(std::get<0>(merge_operators)));\\n    downstream_pipeline.push_back(std::move(std::get<1>(merge_operators)));\\n    down_cast<SourceOperatorFactory*>(downstream_pipeline.front().get())->set_degree_of_parallelism(dop);\\n    return downstream_pipeline;\\n}\\n\\nvoid PipelineBuilderContext::inherit_upstream_source_properties(SourceOperatorFactory* downstream_source,\\n                                                                SourceOperatorFactory* upstream_source) {\\n    downstream_source->set_degree_of_parallelism(upstream_source->degree_of_parallelism());\\n    downstream_source->set_could_local_shuffle(upstream_source->could_local_shuffle());\\n    downstream_source->set_partition_type(upstream_source->partition_type());\\n    if (!upstream_source->partition_exprs().empty() || !downstream_source->partition_exprs().empty()) {\\n        downstream_source->set_partition_exprs(upstream_source->partition_exprs());\\n    }\\n\\n    if (downstream_source->adaptive_initial_state() == SourceOperatorFactory::AdaptiveState::NONE) {\\n        downstream_source->add_upstream_source(upstream_source);\\n    }\\n}\\n\\nvoid PipelineBuilderContext::push_dependent_pipeline(const Pipeline* pipeline) {\\n    _dependent_pipelines.emplace_back(pipeline);\\n}\\nvoid PipelineBuilderContext::pop_dependent_pipeline() {\\n    _dependent_pipelines.pop_back();\\n}\\n\\nint64_t PipelineBuilderContext::_prev_limit_size(const OpFactories& pred_operators) {\\n    for (auto it = pred_operators.rbegin(); it != pred_operators.rend(); ++it) {\\n        if (auto limit = dynamic_cast<LimitOperatorFactory*>(it->get())) {\\n            return limit->limit();\\n        } else if (dynamic_cast<ChunkAccumulateOperatorFactory*>(it->get()) == nullptr) {\\n            return -1;\\n        }\\n    }\\n    return -1;\\n}\\n\\nvoid PipelineBuilderContext::_try_interpolate_limit_operator(int32_t plan_node_id, OpFactories& pred_operators,\\n                                                             int64_t limit_size) {\\n    if (limit_size >= 0 && limit_size < config::pipline_limit_max_delivery) {\\n        pred_operators.emplace_back(\\n                std::make_shared<LimitOperatorFactory>(next_operator_id(), plan_node_id, limit_size));\\n    }\\n}\\n\\nvoid PipelineBuilderContext::_subscribe_pipeline_event(Pipeline* pipeline) {\\n    bool enable_wait_event = _fragment_context->runtime_state()->enable_wait_dependent_event();\\n    enable_wait_event &= !_current_execution_group->is_colocate_exec_group();\\n    if (enable_wait_event && !_dependent_pipelines.empty()) {\\n        pipeline->pipeline_event()->set_need_wait_dependencies_finished(true);\\n        pipeline->pipeline_event()->add_dependency(_dependent_pipelines.back()->pipeline_event());\\n    }\\n}\\n\\nOpFactories PipelineBuilder::decompose_exec_node_to_pipeline(const FragmentContext& fragment, ExecNode* exec_node) {\\n    pipeline::OpFactories operators = exec_node->decompose_to_pipeline(&_context);\\n    operators = _context.maybe_interpolate_grouped_exchange(exec_node->id(), operators);\\n    return operators;\\n}\\n\\nstd::pair<ExecutionGroups, Pipelines> PipelineBuilder::build() {\\n    return {_context.execution_groups(), _context.pipelines()};\\n}\\n\\n} // namespace starrocks::pipeline\\n\",\n  \"message\": \"Successfully read 614 lines from file: /Users/kks/git/starrocks/be/src/exec/pipeline/pipeline_builder.cpp\"\n}",
          "role": "tool",
          "tool_call_id": "call_MHxXb1ZUSk5KcmRuVDBHc3hQWXk"
        },
        {
          "content": null,
          "role": "assistant",
          "tool_calls": [
            {
              "function": {
                "arguments": "{\"path\":\"fe/fe-core/src/main/java/com/starrocks/sql/plan/\",\"pattern\":\"Fragment\"}",
                "name": "codeSearch"
              },
              "id": "call_MHxMQTluSU1DcndGS29ZTm51cW4",
              "type": "function"
            }
          ]
        },
        {
          "content": "{\n  \"success\": true,\n  \"result\": \"Found 20 matches:\\nFile: /Users/kks/git/starrocks/fe/fe-core/src/main/java/com/starrocks/sql/plan/ExecPlan.java\\n  25: import com.starrocks.planner.PlanFragment;\\n  26: import com.starrocks.planner.PlanFragmentId;\\n  54: private final ArrayList<PlanFragment> fragments = new ArrayList<>();\\n  55: private final Map<Integer, PlanFragment> cteProduceFragments = Maps.newHashMap();\\n  56: // splitProduceFragments and joinNodeMap is used for skew join\\n  57: private final Map<Integer, PlanFragment> splitProduceFragments = Maps.newHashMap();\\n  67: private final IdGenerator<PlanFragmentId> fragmentIdGenerator = PlanFragmentId.createGenerator();\\n  101: public ExecPlan(ConnectContext connectContext, List<PlanFragment> fragments) {\\n  106: this.fragments.addAll(fragments);\\n  122: public ArrayList<PlanFragment> getFragments() {\\n\\nFile: /Users/kks/git/starrocks/fe/fe-core/src/main/java/com/starrocks/sql/plan/PlanFragmentBuilder.java\\n  76: import com.starrocks.planner.FragmentNormalizer;\\n  89: import com.starrocks.planner.MultiCastPlanFragment;\\n  98: import com.starrocks.planner.PlanFragment;\\n  112: import com.starrocks.planner.SplitCastPlanFragment;\\n  260: * PlanFragmentBuilder used to transform physical operator to exec plan fragment\\n  262: public class PlanFragmentBuilder {\\n  263: private static final Logger LOG = LogManager.getLogger(PlanFragmentBuilder.class);\\n  269: boolean hasOutputFragment, boolean isShortCircuit) {\\n  272: createOutputFragment(new PhysicalPlanTranslator(columnRefFactory).translate(plan, execPlan), execPlan,\\n  273: outputColumns, hasOutputFragment);\"\n}",
          "role": "tool",
          "tool_call_id": "call_MHxMQTluSU1DcndGS29ZTm51cW4"
        },
        {
          "content": null,
          "role": "assistant",
          "tool_calls": [
            {
              "function": {
                "arguments": "{\"directory_path\":\"/Users/kks/git/starrocks/fe/fe-core/src/main/java/com/starrocks/planner/\"}",
                "name": "listFiles"
              },
              "id": "call_MHw2T1V6a08yTzhIZnpVV1RvNWg",
              "type": "function"
            }
          ]
        },
        {
          "content": "dirs: expression; stream; AbstractOlapTableScanNode.java; AggregateInfo.java; AggregateInfoBase.java; AggregationNode.java; AnalyticEvalNode.java; AssertNumRowsNode.java; BenchmarkScanNode.java; BinlogScanNode.java; BlackHoleTableSink.java; CaptureVersionNode.java; DataPartition.java; DataSink.java; DataStreamSink.java; DecodeNode.java; DeltaLakeScanNode.java; DescriptorTable.java; DictionaryCacheSink.java; DistributionPruner.java; EmptySetNode.java; EsScanNode.java; ExceptNode.java; ExchangeNode.java; ExecGroup.java; ExecGroupId.java; ExecGroupSets.java; ExportSink.java; FetchNode.java; FileScanNode.java; FileTableScanNode.java; FragmentNormalizer.java; HashDistributionKey.java; HashDistributionPruner.java; HashJoinNode.java; HdfsScanNode.java; HiveTableSink.java; HudiScanNode.java; IcebergDeleteSink.java; IcebergMetadataScanNode.java; IcebergScanNode.java; IcebergTableSink.java; IntersectNode.java; JDBCScanNode.java; JoinNode.java; KuduScanNode.java; LoadScanNode.java; LookUpNode.java; MergeJoinNode.java; MetaScanNode.java; MultiCastDataSink.java; MultiCastPlanFragment.java; MysqlScanNode.java; MysqlTableSink.java; NestLoopJoinNode.java; NoopSink.java; OdpsScanNode.java; OlapScanNode.java; OlapTableSink.java; PaimonScanNode.java; PartitionColumnFilter.java; PartitionIdGenerator.java; PartitionPruner.java; PlanFragment.java; PlanFragmentId.java; PlanNode.java; PlanNodeId.java; ProjectNode.java; RangeDistributionPruner.java; RangePartitionPruner.java; RawValuesNode.java; RepeatNode.java; ResultSink.java; RuntimeFilterBuildNode.java; RuntimeFilterDescription.java; RuntimeFilterId.java; RuntimeFilterLayout.java; RuntimeFilterPushDownContext.java; ScanNode.java; SchemaScanNode.java; SchemaTableSink.java; SelectNode.java; SetOperationNode.java; SlotDescriptor.java; SlotId.java; SortInfo.java; SortNode.java; SplitCastDataSink.java; SplitCastPlanFragment.java; StreamLoadPlanner.java; StreamLoadScanNode.java; TableFunctionNode.java; TableFunctionTableSink.java; TupleDescriptor.java; TupleId.java; UnionNode.java\n\nexpression dirs: ExprOpcodeRegistry.java; ExprToNormalFormVisitor.java; ExprToThrift.java\n\nstream dirs: StreamAggNode.java; StreamJoinNode.java",
          "role": "tool",
          "tool_call_id": "call_MHw2T1V6a08yTzhIZnpVV1RvNWg"
        },
        {
          "content": null,
          "role": "assistant",
          "tool_calls": [
            {
              "function": {
                "arguments": "{\"file_path\":\"/Users/kks/git/starrocks/fe/fe-core/src/main/java/com/starrocks/planner/PlanFragment.java\"}",
                "name": "readFile"
              },
              "id": "call_MHxzTDdiR09GUkxhNEdSMDNha2s",
              "type": "function"
            }
          ]
        },
        {
          "content": "{\n  \"success\": true,\n  \"file_path\": \"/Users/kks/git/starrocks/fe/fe-core/src/main/java/com/starrocks/planner/PlanFragment.java\",\n  \"content\": \"// Copyright 2021-present StarRocks, Inc. All rights reserved.\\n//\\n// Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\\n// you may not use this file except in compliance with the License.\\n// You may obtain a copy of the License at\\n//\\n//     https://www.apache.org/licenses/LICENSE-2.0\\n//\\n// Unless required by applicable law or agreed to in writing, software\\n// distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n// See the License for the specific language governing permissions and\\n// limitations under the License.\\n\\n// This file is based on code available under the Apache license here:\\n//   https://github.com/apache/incubator-doris/blob/master/fe/fe-core/src/main/java/org/apache/doris/planner/PlanFragment.java\\n\\n// Licensed to the Apache Software Foundation (ASF) under one\\n// or more contributor license agreements.  See the NOTICE file\\n// distributed with this work for additional information\\n// regarding copyright ownership.  The ASF licenses this file\\n// to you under the Apache License, Version 2.0 (the\\n// \\\"License\\\"); you may not use this file except in compliance\\n// with the License.  You may obtain a copy of the License at\\n//\\n//   http://www.apache.org/licenses/LICENSE-2.0\\n//\\n// Unless required by applicable law or agreed to in writing,\\n// software distributed under the License is distributed on an\\n// \\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\n// KIND, either express or implied.  See the License for the\\n// specific language governing permissions and limitations\\n// under the License.\\n\\npackage com.starrocks.planner;\\n\\nimport com.google.common.base.Preconditions;\\nimport com.google.common.collect.Lists;\\nimport com.google.common.collect.Maps;\\nimport com.google.common.collect.Sets;\\nimport com.starrocks.common.Config;\\nimport com.starrocks.common.FeConstants;\\nimport com.starrocks.common.Pair;\\nimport com.starrocks.connector.BucketProperty;\\nimport com.starrocks.planner.expression.ExprToThrift;\\nimport com.starrocks.qe.ConnectContext;\\nimport com.starrocks.qe.SessionVariable;\\nimport com.starrocks.sql.ast.TreeNode;\\nimport com.starrocks.sql.ast.expression.Expr;\\nimport com.starrocks.sql.ast.expression.ExprToSql;\\nimport com.starrocks.sql.ast.expression.ExprUtils;\\nimport com.starrocks.sql.optimizer.Utils;\\nimport com.starrocks.sql.optimizer.statistics.ColumnDict;\\nimport com.starrocks.sql.plan.ExecPlan;\\nimport com.starrocks.thrift.TCacheParam;\\nimport com.starrocks.thrift.TDataSink;\\nimport com.starrocks.thrift.TExplainLevel;\\nimport com.starrocks.thrift.TExpr;\\nimport com.starrocks.thrift.TGlobalDict;\\nimport com.starrocks.thrift.TGroupExecutionParam;\\nimport com.starrocks.thrift.TNetworkAddress;\\nimport com.starrocks.thrift.TPartitionType;\\nimport com.starrocks.thrift.TPlanFragment;\\nimport com.starrocks.thrift.TResultSinkType;\\nimport org.apache.commons.collections.CollectionUtils;\\nimport org.apache.commons.collections4.MapUtils;\\nimport org.roaringbitmap.RoaringBitmap;\\n\\nimport java.nio.ByteBuffer;\\nimport java.util.ArrayList;\\nimport java.util.Comparator;\\nimport java.util.Deque;\\nimport java.util.LinkedList;\\nimport java.util.List;\\nimport java.util.Map;\\nimport java.util.Optional;\\nimport java.util.Queue;\\nimport java.util.Set;\\nimport java.util.function.Consumer;\\nimport java.util.stream.Collectors;\\nimport java.util.stream.Stream;\\n\\n/**\\n * PlanFragments form a tree structure via their ExchangeNodes. A tree of fragments\\n * connected in that way forms a plan. The output of a plan is produced by the root\\n * fragment and is either the result of the query or an intermediate result\\n * needed by a different plan (such as a hash table).\\n * <p>\\n * Plans are grouped into cohorts based on the consumer of their output: all\\n * plans that materialize intermediate results for a particular consumer plan\\n * are grouped into a single cohort.\\n * <p>\\n * A PlanFragment encapsulates the specific tree of execution nodes that\\n * are used to produce the output of the plan fragment, as well as output exprs,\\n * destination node, etc. If there are no output exprs, the full row that is\\n * is produced by the plan root is marked as materialized.\\n * <p>\\n * A plan fragment can have one or many instances, each of which in turn is executed by\\n * an individual node and the output sent to a specific instance of the destination\\n * fragment (or, in the case of the root fragment, is materialized in some form).\\n * <p>\\n * A hash-partitioned plan fragment is the result of one or more hash-partitioning data\\n * streams being received by plan nodes in this fragment. In the future, a fragment's\\n * data partition could also be hash partitioned based on a scan node that is reading\\n * from a physically hash-partitioned table.\\n * <p>\\n * The sequence of calls is:\\n * - c'tor\\n * - assemble with getters, etc.\\n * - finalize()\\n * - toThrift()\\n * <p>\\n * TODO: the tree of PlanNodes is connected across fragment boundaries, which makes\\n * it impossible search for things within a fragment (using TreeNode functions);\\n * fix that\\n */\\npublic class PlanFragment extends TreeNode<PlanFragment> {\\n    // id for this plan fragment\\n    protected final PlanFragmentId fragmentId;\\n\\n    // root of plan tree executed by this fragment\\n    protected PlanNode planRoot;\\n\\n    // exchange node to which this fragment sends its output\\n    private ExchangeNode destNode;\\n\\n    // if null, outputs the entire row produced by planRoot\\n    protected ArrayList<Expr> outputExprs;\\n\\n    // created in finalize() or set in setSink()\\n    protected DataSink sink;\\n\\n    // specification of the partition of the input of this fragment;\\n    // an UNPARTITIONED fragment is executed on only a single node\\n    // TODO: improve this comment, \\\"input\\\" is a bit misleading\\n    protected final DataPartition dataPartition;\\n\\n    // specification of how the output of this fragment is partitioned (i.e., how\\n    // it's sent to its destination);\\n    // if the output is UNPARTITIONED, it is being broadcast\\n    protected DataPartition outputPartition;\\n\\n    // Whether query statistics is sent with every batch. In order to get the query\\n    // statistics correctly when query contains limit, it is necessary to send query \\n    // statistics with every batch, or only in close.\\n    protected boolean transferQueryStatisticsWithEveryBatch;\\n\\n    // TODO: SubstitutionMap outputSmap;\\n    // substitution map to remap exprs onto the output of this fragment, to be applied\\n    // at destination fragment\\n\\n    // specification of the number of parallel when fragment is executed\\n    // default value is 1\\n    protected int parallelExecNum = 1;\\n    protected int pipelineDop = 1;\\n    protected boolean dopEstimated = false;\\n\\n    // Whether to assign scan ranges to each driver sequence of pipeline,\\n    // for the normal backend assignment (not colocate, bucket, and replicated join).\\n    protected boolean assignScanRangesPerDriverSeq = false;\\n    protected boolean withLocalShuffle = false;\\n\\n    protected double fragmentCost;\\n\\n    protected Map<Integer, RuntimeFilterDescription> buildRuntimeFilters = Maps.newHashMap();\\n    protected Map<Integer, RuntimeFilterDescription> probeRuntimeFilters = Maps.newHashMap();\\n\\n    protected List<Pair<Integer, ColumnDict>> queryGlobalDicts = Lists.newArrayList();\\n    protected Map<Integer, Expr> queryGlobalDictExprs;\\n    protected List<Pair<Integer, ColumnDict>> loadGlobalDicts = Lists.newArrayList();\\n\\n    private final Set<Integer> runtimeFilterBuildNodeIds = Sets.newHashSet();\\n\\n    private TCacheParam cacheParam = null;\\n    private boolean hasOlapTableSink = false;\\n    private boolean hasIcebergTableSink = false;\\n    private boolean hasHiveTableSink = false;\\n    private boolean hasTableFunctionTableSink = false;\\n\\n    private boolean forceSetTableSinkDop = false;\\n    private boolean forceAssignScanRangesPerDriverSeq = false;\\n\\n    private boolean useRuntimeAdaptiveDop = false;\\n\\n    private boolean isShortCircuit = false;\\n\\n    // Controls whether group execution is used for plan fragment execution.\\n    private List<ExecGroup> colocateExecGroups = Lists.newArrayList();\\n\\n    private List<Integer> collectExecStatsIds;\\n\\n    // If all scan nodes in the entire plan are OLAP scan nodes and at most one tablet is used,\\n    // then all fragments will have at most one instance (running on a single BE).\\n    // In this case, there is no need to put the ResultSink in a dedicated gather fragment.\\n    //\\n    // CN mode(prefer_compute_node is true or share-data mode) needs special handling here, because its strategy for selecting\\n    // BE/CN nodes for remote fragments (whose left-deep node is not a scan node) is different from non-CN mode:\\n    // - CN mode: selects all CN nodes as instances, which will be more than 1 here,\\n    //   causing the fragment that contains the ResultSink to also have more than 1 instance.\\n    // - None-CN mode: selects the BE node(s) where the child fragment is running.\\n    //\\n    // Therefore, if isSingleTabletGatherOutputFragment is true, CN mode should use the same selection strategy as none-CN mode.\\n    private boolean isSingleTabletGatherOutputFragment = false;\\n\\n    /**\\n     * C'tor for fragment with specific partition; the output is by default broadcast.\\n     */\\n    public PlanFragment(PlanFragmentId id, PlanNode root, DataPartition partition) {\\n        this.fragmentId = id;\\n        this.dataPartition = partition;\\n        this.outputPartition = DataPartition.UNPARTITIONED;\\n        this.transferQueryStatisticsWithEveryBatch = false;\\n        // when dop adaptation is enabled, parallelExecNum and pipelineDop set to degreeOfParallelism and 1 respectively\\n        // in default. these values just a hint to help determine numInstances and pipelineDop of a PlanFragment.\\n        setPlanRoot(root);\\n        setParallelExecNumIfExists();\\n        setFragmentInPlanTree(planRoot);\\n    }\\n\\n    /**\\n     * Assigns 'this' as fragment of all PlanNodes in the plan tree rooted at node.\\n     * Does not traverse the children of ExchangeNodes because those must belong to a\\n     * different fragment.\\n     */\\n    private void setFragmentInPlanTree(PlanNode node) {\\n        if (node == null) {\\n            return;\\n        }\\n        node.setFragment(this);\\n        if (node instanceof ExchangeNode) {\\n            return;\\n        }\\n        for (PlanNode child : node.getChildren()) {\\n            setFragmentInPlanTree(child);\\n        }\\n    }\\n\\n    public double getFragmentCost() {\\n        return fragmentCost;\\n    }\\n\\n    public void setFragmentCost(double fragmentCost) {\\n        this.fragmentCost = fragmentCost;\\n    }\\n\\n    public boolean canUsePipeline() {\\n        return getPlanRoot().canUsePipeLine() && getSink().canUsePipeLine();\\n    }\\n\\n    public boolean canUseRuntimeAdaptiveDop() {\\n        return getPlanRoot().canUseRuntimeAdaptiveDop() && getSink().canUseRuntimeAdaptiveDop();\\n    }\\n\\n    public void enableAdaptiveDop() {\\n        useRuntimeAdaptiveDop = true;\\n        // Constrict DOP as the power of two to make the strategy of decrement DOP easy.\\n        // After decreasing DOP from old_dop to new_dop, chunks from the i-th input driver is passed\\n        // to the j-th output driver, where j=i%new_dop.\\n        pipelineDop = Utils.computeMaxLEPower2(pipelineDop);\\n    }\\n\\n    public void disableRuntimeAdaptiveDop() {\\n        useRuntimeAdaptiveDop = false;\\n    }\\n\\n    public boolean isUseRuntimeAdaptiveDop() {\\n        return useRuntimeAdaptiveDop;\\n    }\\n\\n    /**\\n     * Assign ParallelExecNum by PARALLEL_FRAGMENT_EXEC_INSTANCE_NUM in SessionVariable for synchronous request\\n     * Assign ParallelExecNum by default value for Asynchronous request\\n     */\\n    private void setParallelExecNumIfExists() {\\n        if (ConnectContext.get() != null) {\\n            ConnectContext connectContext = ConnectContext.get();\\n            SessionVariable sv = connectContext.getSessionVariable();\\n            if (sv.isEnablePipelineEngine()) {\\n                this.parallelExecNum = 1;\\n                this.pipelineDop = sv.getDegreeOfParallelism(connectContext.getCurrentWarehouseId());\\n            } else {\\n                this.parallelExecNum = sv.getParallelExecInstanceNum();\\n                this.pipelineDop = 1;\\n            }\\n        }\\n    }\\n\\n    public void limitMaxPipelineDop(int maxPipelineDop) {\\n        if (pipelineDop > maxPipelineDop) {\\n            pipelineDop = maxPipelineDop;\\n            if (useRuntimeAdaptiveDop) {\\n                pipelineDop = Utils.computeMaxLEPower2(pipelineDop);\\n            }\\n        }\\n    }\\n\\n    public ExchangeNode getDestNode() {\\n        return destNode;\\n    }\\n\\n    public DataPartition getOutputPartition() {\\n        return outputPartition;\\n    }\\n\\n    /**\\n     * Parallel load fragment instance num in single host\\n     */\\n    public void setParallelExecNum(int parallelExecNum) {\\n        this.parallelExecNum = parallelExecNum;\\n    }\\n\\n    public int getPipelineDop() {\\n        return pipelineDop;\\n    }\\n\\n    public void setPipelineDop(int dop) {\\n        this.pipelineDop = dop;\\n    }\\n\\n    public boolean hasTableSink() {\\n        return hasIcebergTableSink() || hasOlapTableSink() || hasHiveTableSink() || hasTableFunctionTableSink();\\n    }\\n\\n    public boolean hasOlapTableSink() {\\n        return this.hasOlapTableSink;\\n    }\\n\\n    public void setHasOlapTableSink() {\\n        this.hasOlapTableSink = true;\\n    }\\n\\n    public boolean hasIcebergTableSink() {\\n        return this.hasIcebergTableSink;\\n    }\\n\\n    public void setHasIcebergTableSink() {\\n        this.hasIcebergTableSink = true;\\n    }\\n\\n    public boolean hasHiveTableSink() {\\n        return this.hasHiveTableSink;\\n    }\\n\\n    public void setHasHiveTableSink() {\\n        this.hasHiveTableSink = true;\\n    }\\n\\n    public boolean hasTableFunctionTableSink() {\\n        return this.hasTableFunctionTableSink;\\n    }\\n\\n    public void setHasTableFunctionTableSink() {\\n        this.hasTableFunctionTableSink = true;\\n    }\\n\\n    public boolean forceSetTableSinkDop() {\\n        return this.forceSetTableSinkDop;\\n    }\\n\\n    public void setForceSetTableSinkDop() {\\n        this.forceSetTableSinkDop = true;\\n    }\\n\\n    public boolean isWithLocalShuffle() {\\n        return withLocalShuffle;\\n    }\\n\\n    public void setWithLocalShuffleIfTrue(boolean withLocalShuffle) {\\n        this.withLocalShuffle |= withLocalShuffle;\\n    }\\n\\n    public boolean isUseGroupExecution() {\\n        return !colocateExecGroups.isEmpty();\\n    }\\n\\n    public boolean isAssignScanRangesPerDriverSeq() {\\n        return assignScanRangesPerDriverSeq;\\n    }\\n\\n    public void setAssignScanRangesPerDriverSeq(boolean assignScanRangesPerDriverSeq) {\\n        this.assignScanRangesPerDriverSeq |= assignScanRangesPerDriverSeq;\\n    }\\n\\n    public boolean isForceAssignScanRangesPerDriverSeq() {\\n        return forceAssignScanRangesPerDriverSeq;\\n    }\\n\\n    public void setForceAssignScanRangesPerDriverSeq() {\\n        this.forceAssignScanRangesPerDriverSeq = true;\\n    }\\n\\n    public void computeLocalRfWaitingSet(PlanNode root, boolean clearGlobalRuntimeFilter) {\\n        root.fillLocalRfWaitingSet(runtimeFilterBuildNodeIds);\\n        // TopN Filter should't wait\\n        if (root instanceof RuntimeFilterBuildNode && !(root instanceof SortNode) && !(root instanceof AggregationNode)) {\\n            runtimeFilterBuildNodeIds.add(root.getId().asInt());\\n        }\\n        if (clearGlobalRuntimeFilter) {\\n            root.clearProbeRuntimeFilters();\\n            if (root instanceof RuntimeFilterBuildNode) {\\n                ((RuntimeFilterBuildNode) root).clearBuildRuntimeFilters();\\n            }\\n        }\\n        for (PlanNode child : root.getChildren()) {\\n            if (child.getFragment() == this) {\\n                computeLocalRfWaitingSet(child, clearGlobalRuntimeFilter);\\n            }\\n        }\\n    }\\n\\n    public void assignColocateExecGroups(PlanNode root, List<ExecGroup> groups) {\\n        for (ExecGroup group : groups) {\\n            if (group.contains(root)) {\\n                colocateExecGroups.add(group);\\n                groups.remove(group);\\n                break;\\n            }\\n        }\\n        if (!groups.isEmpty()) {\\n            for (PlanNode child : root.getChildren()) {\\n                if (child.getFragment() == this) {\\n                    assignColocateExecGroups(child, groups);\\n                }\\n            }\\n        }\\n    }\\n\\n    public boolean isColocateGroupFragment() {\\n        return colocateExecGroups.size() > 0;\\n    }\\n\\n    public boolean isDopEstimated() {\\n        return dopEstimated;\\n    }\\n\\n    public void setOutputExprs(List<Expr> outputExprs) {\\n        this.outputExprs = ExprUtils.cloneList(outputExprs, null);\\n    }\\n\\n    /**\\n     * Finalize plan tree and create stream sink, if needed.\\n     */\\n    public void createDataSink(TResultSinkType resultSinkType, ExecPlan execPlan) {\\n        if (sink != null) {\\n            return;\\n        }\\n        if (destNode != null) {\\n            // we're streaming to an exchange node\\n            DataStreamSink streamSink = new DataStreamSink(destNode.getId());\\n            streamSink.setPartition(outputPartition);\\n            streamSink.setMerge(destNode.isMerge());\\n            streamSink.setFragment(this);\\n            sink = streamSink;\\n        } else {\\n            if (planRoot == null) {\\n                // only output expr, no FROM clause\\n                // \\\"select 1 + 2\\\"\\n                return;\\n            }\\n            // add ResultSink\\n            // we're streaming to an result sink\\n\\n            if (resultSinkType == TResultSinkType.ARROW_FLIGHT_PROTOCAL) {\\n                sink = new ResultSink(planRoot.getId(), resultSinkType, execPlan.getColNames());\\n            } else {\\n                sink = new ResultSink(planRoot.getId(), resultSinkType);\\n            }\\n        }\\n    }\\n\\n    public void createDataSink(TResultSinkType resultSinkType) {\\n        createDataSink(resultSinkType, null);\\n    }\\n\\n    public int getParallelExecNum() {\\n        return parallelExecNum;\\n    }\\n\\n    public List<Integer> getCollectExecStatsIds() {\\n        return collectExecStatsIds;\\n    }\\n\\n    public void setCollectExecStatsIds(List<Integer> collectExecStatsIds) {\\n        this.collectExecStatsIds = collectExecStatsIds;\\n    }\\n\\n    public TPlanFragment toThrift() {\\n        TPlanFragment result = new TPlanFragment();\\n        if (planRoot != null) {\\n            result.setPlan(planRoot.treeToThrift());\\n        }\\n        if (outputExprs != null) {\\n            result.setOutput_exprs(ExprToThrift.treesToThrift(outputExprs));\\n        }\\n        if (sink != null) {\\n            result.setOutput_sink(sink.toThrift());\\n        }\\n        result.setPartition(dataPartition.toThrift());\\n\\n        if (!queryGlobalDicts.isEmpty()) {\\n            result.setQuery_global_dicts(dictToThrift(queryGlobalDicts));\\n        }\\n        if (MapUtils.isNotEmpty(queryGlobalDictExprs)) {\\n            Preconditions.checkState(!queryGlobalDicts.isEmpty(), \\\"Global dict expression error!\\\");\\n            Map<Integer, TExpr> exprs = Maps.newHashMap();\\n            queryGlobalDictExprs.forEach((k, v) -> exprs.put(k, ExprToThrift.treeToThrift(v)));\\n            result.setQuery_global_dict_exprs(exprs);\\n        }\\n        if (!loadGlobalDicts.isEmpty()) {\\n            result.setLoad_global_dicts(dictToThrift(loadGlobalDicts));\\n        }\\n        if (cacheParam != null) {\\n            if (ConnectContext.get() != null) {\\n                SessionVariable sessionVariable = ConnectContext.get().getSessionVariable();\\n                cacheParam.setForce_populate(sessionVariable.isQueryCacheForcePopulate());\\n                cacheParam.setEntry_max_bytes(sessionVariable.getQueryCacheEntryMaxBytes());\\n                cacheParam.setEntry_max_rows(sessionVariable.getQueryCacheEntryMaxRows());\\n            }\\n            result.setCache_param(cacheParam);\\n        }\\n\\n        if (!colocateExecGroups.isEmpty()) {\\n            TGroupExecutionParam tGroupExecutionParam = new TGroupExecutionParam();\\n            tGroupExecutionParam.setEnable_group_execution(true);\\n            for (ExecGroup colocateExecGroup : colocateExecGroups) {\\n                tGroupExecutionParam.addToExec_groups(colocateExecGroup.toThrift());\\n            }\\n            result.setGroup_execution_param(tGroupExecutionParam);\\n        }\\n        return result;\\n    }\\n\\n    /**\\n     * Create thrift fragment with the unique fields, including\\n     * - output_sink (only for MultiCastDataStreamSink and ExportSink).\\n     *\\n     * @return The thrift fragment with the unique fields.\\n     */\\n    public TPlanFragment toThriftForUniqueFields() {\\n        TPlanFragment result = new TPlanFragment();\\n        // Fill the required field.\\n        result.setPartition(dataPartition.toThrift());\\n\\n        if (sink != null && (this instanceof MultiCastPlanFragment || sink instanceof ExportSink)) {\\n            result.setOutput_sink(sink.toThrift());\\n        }\\n\\n        return result;\\n    }\\n\\n    public List<TGlobalDict> dictToThrift(List<Pair<Integer, ColumnDict>> dicts) {\\n        List<TGlobalDict> result = Lists.newArrayList();\\n        for (Pair<Integer, ColumnDict> dictPair : dicts) {\\n            TGlobalDict globalDict = new TGlobalDict();\\n            globalDict.setColumnId(dictPair.first);\\n            List<ByteBuffer> strings = Lists.newArrayList();\\n            List<Integer> integers = Lists.newArrayList();\\n            for (Map.Entry<ByteBuffer, Integer> kv : dictPair.second.getDict().entrySet()) {\\n                strings.add(kv.getKey());\\n                integers.add(kv.getValue());\\n            }\\n            globalDict.setVersion(dictPair.second.getCollectedVersion());\\n            globalDict.setStrings(strings);\\n            globalDict.setIds(integers);\\n            result.add(globalDict);\\n        }\\n        return result;\\n    }\\n\\n    // normalize dicts of the fragment, it is different from dictToThrift in three points:\\n    // 1. SlotIds must be replaced by remapped SlotIds;\\n    // 2. dict should be sorted according to its corresponding remapped SlotIds;\\n    public List<TGlobalDict> normalizeDicts(List<Pair<Integer, ColumnDict>> dicts, FragmentNormalizer normalizer) {\\n        List<TGlobalDict> result = Lists.newArrayList();\\n        // replace slot id with the remapped slot id, sort dicts according to remapped slot ids.\\n        List<Pair<Integer, ColumnDict>> sortedDicts =\\n                dicts.stream().map(p -> Pair.create(normalizer.remapSlotId(p.first), p.second))\\n                        .sorted(Comparator.comparingInt(p -> p.first)).collect(Collectors.toList());\\n\\n        for (Pair<Integer, ColumnDict> dictPair : sortedDicts) {\\n            TGlobalDict globalDict = new TGlobalDict();\\n            globalDict.setColumnId(dictPair.first);\\n            globalDict.setVersion(dictPair.second.getCollectedVersion());\\n            result.add(globalDict);\\n        }\\n        return result;\\n    }\\n\\n    public String getExplainString(TExplainLevel explainLevel) {\\n        StringBuilder str = new StringBuilder();\\n        Preconditions.checkState(dataPartition != null);\\n        str.append(\\\" OUTPUT EXPRS:\\\");\\n\\n        StringBuilder outputBuilder = new StringBuilder();\\n        if (CollectionUtils.isNotEmpty(outputExprs)) {\\n            outputBuilder.append(outputExprs.stream().map(ExprToSql::toSql)\\n                    .collect(Collectors.joining(\\\" | \\\")));\\n\\n        }\\n        str.append(outputBuilder);\\n        str.append(\\\"\\\\n\\\");\\n        if (!colocateExecGroups.isEmpty() && Config.show_execution_groups) {\\n            str.append(\\\"  colocate exec groups: \\\");\\n            for (ExecGroup group : colocateExecGroups) {\\n                str.append(group);\\n            }\\n            str.append(\\\"\\\\n\\\");\\n        }\\n        str.append(\\\"  PARTITION: \\\").append(dataPartition.getExplainString(explainLevel)).append(\\\"\\\\n\\\");\\n        if (sink != null) {\\n            str.append(sink.getExplainString(\\\"  \\\", explainLevel)).append(\\\"\\\\n\\\");\\n        }\\n        if (planRoot != null) {\\n            str.append(planRoot.getExplainString(\\\"  \\\", \\\"  \\\", explainLevel));\\n        }\\n        return str.toString();\\n    }\\n\\n    public String getVerboseExplain() {\\n        StringBuilder str = new StringBuilder();\\n        Preconditions.checkState(dataPartition != null);\\n        if (FeConstants.showFragmentCost) {\\n            str.append(\\\"  Fragment Cost: \\\").append(fragmentCost).append(\\\"\\\\n\\\");\\n        }\\n        if (!colocateExecGroups.isEmpty() && Config.show_execution_groups) {\\n            str.append(\\\"  colocate exec groups: \\\");\\n            for (ExecGroup group : colocateExecGroups) {\\n                str.append(group);\\n            }\\n            str.append(\\\"\\\\n\\\");\\n        }\\n        if (CollectionUtils.isNotEmpty(outputExprs)) {\\n            str.append(\\\"  Output Exprs:\\\");\\n            str.append(outputExprs.stream().map(ExprToSql::toSql)\\n                    .collect(Collectors.joining(\\\" | \\\")));\\n        }\\n        str.append(\\\"\\\\n\\\");\\n        str.append(\\\"  Input Partition: \\\").append(dataPartition.getExplainString(TExplainLevel.NORMAL));\\n        if (sink != null) {\\n            str.append(sink.getVerboseExplain(\\\"  \\\")).append(\\\"\\\\n\\\");\\n        }\\n        if (MapUtils.isNotEmpty(queryGlobalDictExprs)) {\\n            str.append(\\\"  Global Dict Exprs:\\\\n\\\");\\n            queryGlobalDictExprs.entrySet().stream()\\n                    .map(p -> \\\"    \\\" + p.getKey() + \\\": \\\" + ExprToSql.toMySql(p.getValue()) + \\\"\\\\n\\\").forEach(str::append);\\n            str.append(\\\"\\\\n\\\");\\n        }\\n        if (planRoot != null) {\\n            str.append(planRoot.getVerboseExplain(\\\"  \\\", \\\"  \\\"));\\n        }\\n        return str.toString();\\n    }\\n\\n    public String getCostExplain() {\\n        StringBuilder str = new StringBuilder();\\n        Preconditions.checkState(dataPartition != null);\\n        StringBuilder outputBuilder = new StringBuilder();\\n        if (CollectionUtils.isNotEmpty(outputExprs)) {\\n            str.append(\\\"  Output Exprs:\\\");\\n            outputBuilder.append(outputExprs.stream().map(ExprToSql::toSql)\\n                    .collect(Collectors.joining(\\\" | \\\")));\\n        }\\n        str.append(outputBuilder.toString());\\n        str.append(\\\"\\\\n\\\");\\n        str.append(\\\"  Input Partition: \\\").append(dataPartition.getExplainString(TExplainLevel.NORMAL));\\n        if (sink != null) {\\n            str.append(sink.getVerboseExplain(\\\"  \\\")).append(\\\"\\\\n\\\");\\n        }\\n        if (planRoot != null) {\\n            str.append(planRoot.getCostExplain(\\\"  \\\", \\\"  \\\"));\\n        }\\n        return str.toString();\\n    }\\n\\n    /**\\n     * Returns true if this fragment is partitioned.\\n     */\\n    public boolean isPartitioned() {\\n        return (dataPartition.getType() != TPartitionType.UNPARTITIONED);\\n    }\\n\\n    public PlanFragment getDestFragment() {\\n        if (destNode == null) {\\n            return null;\\n        }\\n        return destNode.getFragment();\\n    }\\n\\n    public void setDestination(ExchangeNode destNode) {\\n        this.destNode = destNode;\\n        PlanFragment dest = getDestFragment();\\n        Preconditions.checkNotNull(dest);\\n        dest.addChild(this);\\n    }\\n\\n    public void clearDestination() {\\n        this.destNode = null;\\n    }\\n\\n    public DataPartition getDataPartition() {\\n        return dataPartition;\\n    }\\n\\n    public boolean isGatherFragment() {\\n        return getDataPartition() == DataPartition.UNPARTITIONED;\\n    }\\n\\n    public void setOutputPartition(DataPartition outputPartition) {\\n        this.outputPartition = outputPartition;\\n    }\\n\\n    public void clearOutputPartition() {\\n        this.outputPartition = DataPartition.UNPARTITIONED;\\n    }\\n\\n    public PlanNode getPlanRoot() {\\n        return planRoot;\\n    }\\n\\n    public void setPlanRoot(PlanNode root) {\\n        planRoot = root;\\n        setFragmentInPlanTree(planRoot);\\n    }\\n\\n    public DataSink getSink() {\\n        return sink;\\n    }\\n\\n    public void setSink(DataSink sink) {\\n        Preconditions.checkNotNull(sink);\\n        sink.setFragment(this);\\n        this.sink = sink;\\n    }\\n\\n    public TDataSink sinkToThrift() {\\n        return sink != null ? sink.toThrift() : null;\\n    }\\n\\n    public PlanFragmentId getFragmentId() {\\n        return fragmentId;\\n    }\\n\\n    public boolean isTransferQueryStatisticsWithEveryBatch() {\\n        return transferQueryStatisticsWithEveryBatch;\\n    }\\n\\n    public void collectBuildRuntimeFilters() {\\n        Map<Integer, RuntimeFilterDescription> filters = Maps.newHashMap();\\n        collectNodes().stream()\\n                .filter(node -> node instanceof RuntimeFilterBuildNode)\\n                .flatMap(node -> ((RuntimeFilterBuildNode) node).getBuildRuntimeFilters().stream())\\n                .forEach(desc -> filters.put(desc.getFilterId(), desc));\\n        buildRuntimeFilters = filters;\\n    }\\n\\n    public void collectProbeRuntimeFilters() {\\n        Map<Integer, RuntimeFilterDescription> filters = Maps.newHashMap();\\n        collectNodes().stream()\\n                .flatMap(node -> node.getProbeRuntimeFilters().stream())\\n                .forEach(desc -> filters.put(desc.getFilterId(), desc));\\n        probeRuntimeFilters = filters;\\n    }\\n\\n    public List<PlanNode> collectNodes() {\\n        List<PlanNode> nodes = Lists.newArrayList();\\n        collectNodesImpl(getPlanRoot(), nodes);\\n        return nodes;\\n    }\\n\\n    private void collectNodesImpl(PlanNode root, List<PlanNode> nodes) {\\n        nodes.add(root);\\n        if (!(root instanceof ExchangeNode)) {\\n            root.getChildren().forEach(child -> collectNodesImpl(child, nodes));\\n        }\\n\\n        if (root instanceof HashJoinNode) {\\n            HashJoinNode hashJoinNode = (HashJoinNode) root;\\n            if (hashJoinNode.isSkewBroadJoin()) {\\n                HashJoinNode shuffleJoinNode = hashJoinNode.getSkewJoinFriend();\\n                // TODO(fixme): ensure broadcast join 's rf size is equal to shuffle join's rf size, if not clear the specific\\n                //  broadcast's rf.\\n                if (shuffleJoinNode == null || shuffleJoinNode.getBuildRuntimeFilters() == null) {\\n                   return;\\n                }\\n                if (shuffleJoinNode.getBuildRuntimeFilters().size() != hashJoinNode.getBuildRuntimeFilters().size()) {\\n                    shuffleJoinNode.clearBuildRuntimeFilters();\\n                    hashJoinNode.clearBuildRuntimeFilters();\\n                    return;\\n                }\\n                for (RuntimeFilterDescription description : hashJoinNode.getBuildRuntimeFilters()) {\\n                    int filterId = shuffleJoinNode.getRfIdByEqJoinConjunctsIndex(description.getExprOrder());\\n                    // skew join's broadcast join rf need to remember the filter id of corresponding shuffle join\\n                    description.setSkew_shuffle_filter_id(filterId);\\n                }\\n            }\\n        }\\n    }\\n\\n    public void setRuntimeFilterMergeNodeAddresses(PlanNode root, TNetworkAddress host) {\\n        if (root instanceof ExchangeNode) {\\n            return;\\n        }\\n\\n        if (root instanceof JoinNode) {\\n            JoinNode joinNode = (JoinNode) root;\\n            for (RuntimeFilterDescription description : joinNode.getBuildRuntimeFilters()) {\\n                description.addMergeNode(host);\\n            }\\n        }\\n        if (root instanceof AggregationNode aggNode) {\\n            for (RuntimeFilterDescription description : aggNode.getBuildRuntimeFilters()) {\\n                if (description.isHasRemoteTargets()) {\\n                    description.addMergeNode(host);\\n                }\\n            }\\n        }\\n\\n        for (PlanNode node : root.getChildren()) {\\n            setRuntimeFilterMergeNodeAddresses(node, host);\\n        }\\n    }\\n\\n    public Map<Integer, RuntimeFilterDescription> getBuildRuntimeFilters() {\\n        return buildRuntimeFilters;\\n    }\\n\\n    public Map<Integer, RuntimeFilterDescription> getProbeRuntimeFilters() {\\n        return probeRuntimeFilters;\\n    }\\n\\n    public List<Pair<Integer, ColumnDict>> getQueryGlobalDicts() {\\n        return this.queryGlobalDicts;\\n    }\\n\\n    public Map<Integer, Expr> getQueryGlobalDictExprs() {\\n        return queryGlobalDictExprs;\\n    }\\n\\n    public List<Pair<Integer, ColumnDict>> getLoadGlobalDicts() {\\n        return this.loadGlobalDicts;\\n    }\\n\\n    public void setQueryGlobalDicts(List<Pair<Integer, ColumnDict>> dicts) {\\n        this.queryGlobalDicts = dicts;\\n    }\\n\\n    // For plan fragment has join\\n    public void mergeQueryGlobalDicts(List<Pair<Integer, ColumnDict>> dicts) {\\n        if (this.queryGlobalDicts != dicts) {\\n            this.queryGlobalDicts = Stream.concat(this.queryGlobalDicts.stream(), dicts.stream()).distinct()\\n                    .collect(Collectors.toList());\\n        }\\n    }\\n\\n    public void mergeQueryDictExprs(Map<Integer, Expr> queryGlobalDictExprs) {\\n        if (this.queryGlobalDictExprs != queryGlobalDictExprs) {\\n            Map<Integer, Expr> n = Maps.newHashMap(MapUtils.emptyIfNull(this.queryGlobalDictExprs));\\n            n.putAll(MapUtils.emptyIfNull(queryGlobalDictExprs));\\n            this.queryGlobalDictExprs = n;\\n        }\\n    }\\n\\n    public void setQueryGlobalDictExprs(Map<Integer, Expr> queryGlobalDictExprs) {\\n        this.queryGlobalDictExprs = queryGlobalDictExprs;\\n    }\\n\\n    public void setLoadGlobalDicts(\\n            List<Pair<Integer, ColumnDict>> loadGlobalDicts) {\\n        this.loadGlobalDicts = loadGlobalDicts;\\n    }\\n\\n    public boolean hashLocalBucketShuffleRightOrFullJoin(PlanNode planRoot) {\\n        if (planRoot instanceof JoinNode) {\\n            JoinNode joinNode = (JoinNode) planRoot;\\n            if (joinNode.isLocalHashBucket() &&\\n                    (joinNode.getJoinOp().isFullOuterJoin() || joinNode.getJoinOp().isRightJoin())) {\\n                return true;\\n            }\\n        }\\n        for (PlanNode childNode : planRoot.getChildren()) {\\n            if (hashLocalBucketShuffleRightOrFullJoin(childNode)) {\\n                return true;\\n            }\\n        }\\n        return false;\\n    }\\n\\n    public TCacheParam getCacheParam() {\\n        return cacheParam;\\n    }\\n\\n    public void setCacheParam(TCacheParam cacheParam) {\\n        this.cacheParam = cacheParam;\\n    }\\n\\n    public Map<PlanNodeId, ScanNode> collectScanNodes() {\\n        Map<PlanNodeId, ScanNode> scanNodes = Maps.newHashMap();\\n        Queue<PlanNode> queue = Lists.newLinkedList();\\n        queue.add(planRoot);\\n        while (!queue.isEmpty()) {\\n            PlanNode node = queue.poll();\\n\\n            if (node instanceof ExchangeNode) {\\n                continue;\\n            }\\n            if (node instanceof ScanNode) {\\n                scanNodes.put(node.getId(), (ScanNode) node);\\n            }\\n\\n            queue.addAll(node.getChildren());\\n        }\\n\\n        return scanNodes;\\n    }\\n\\n    public Optional<List<BucketProperty>> extractBucketProperties() {\\n        List<List<BucketProperty>> properties = collectScanNodes().values().stream()\\n                .filter(scanNode -> scanNode instanceof IcebergScanNode)\\n                .map(scanNode -> ((IcebergScanNode) scanNode).getBucketProperties())\\n                .filter(Optional::isPresent).map(Optional::get).toList();\\n        return BucketProperty.checkAndGetBucketProperties(properties);\\n\\n    }\\n\\n    public boolean isUnionFragment() {\\n        Deque<PlanNode> dq = new LinkedList<>();\\n        dq.offer(planRoot);\\n\\n        while (!dq.isEmpty()) {\\n            PlanNode nd = dq.poll();\\n\\n            if (nd instanceof UnionNode) {\\n                return true;\\n            }\\n            if (!(nd instanceof ExchangeNode)) {\\n                dq.addAll(nd.getChildren());\\n            }\\n        }\\n        return false;\\n    }\\n\\n    public ArrayList<Expr> getOutputExprs() {\\n        return outputExprs;\\n    }\\n\\n    public boolean isShortCircuit() {\\n        return isShortCircuit;\\n    }\\n\\n    public void setShortCircuit(boolean shortCircuit) {\\n        isShortCircuit = shortCircuit;\\n    }\\n\\n    /**\\n     * Returns the leftmost node of the fragment.\\n     */\\n    public PlanNode getLeftMostNode() {\\n        PlanNode node = planRoot;\\n        while (!node.getChildren().isEmpty() && !(node instanceof ExchangeNode)) {\\n            node = node.getChild(0);\\n        }\\n        return node;\\n    }\\n\\n    public void reset() {\\n        // Do nothing.\\n    }\\n\\n    public void disablePhysicalPropertyOptimize() {\\n        colocateExecGroups.clear();\\n        forEachNode(planRoot, PlanNode::disablePhysicalPropertyOptimize);\\n    }\\n\\n    private void forEachNode(PlanNode root, Consumer<PlanNode> consumer) {\\n        consumer.accept(root);\\n        for (PlanNode child : root.getChildren()) {\\n            forEachNode(child, consumer);\\n        }\\n    }\\n\\n    private RoaringBitmap collectNonBroadcastRfIds(PlanNode root) {\\n        RoaringBitmap filterIds = root.getChildren().stream()\\n                .filter(child -> child.getFragmentId().equals(root.getFragmentId()))\\n                .map(this::collectNonBroadcastRfIds)\\n                .reduce(RoaringBitmap.bitmapOf(), (a, b) -> RoaringBitmap.or(a, b));\\n        if (root instanceof HashJoinNode) {\\n            HashJoinNode joinNode = (HashJoinNode) root;\\n            if (!joinNode.isBroadcast()) {\\n                joinNode.getBuildRuntimeFilters().forEach(rf -> filterIds.add(rf.getFilterId()));\\n            }\\n        }\\n        return filterIds;\\n    }\\n\\n    /**\\n     * In the same fragment, collect all nodes of the right subtree of BroadcastJoinNode that will build global RFs\\n     * For example, the following plan will add 3 and 4 to {@code localRightOffsprings}.\\n     * <pre>{@code\\n     *       Broadcast Join#1\\n     *        /        \\\\\",\n  \"message\": \"Successfully read file: /Users/kks/git/starrocks/fe/fe-core/src/main/java/com/starrocks/planner/PlanFragment.java (TRUNCATED: showing first 1000 of 1078 total lines)\"\n}",
          "role": "tool",
          "tool_call_id": "call_MHxzTDdiR09GUkxhNEdSMDNha2s"
        },
        {
          "content": null,
          "role": "assistant",
          "tool_calls": [
            {
              "function": {
                "arguments": "{\"file_path\":\"/Users/kks/git/starrocks/be/src/exec/pipeline/README.md\"}",
                "name": "readFile"
              },
              "id": "call_MHxNN1JyRklyZVRvRFFXU2VNVVM",
              "type": "function"
            }
          ]
        },
        {
          "content": "{\n  \"success\": true,\n  \"file_path\": \"/Users/kks/git/starrocks/be/src/exec/pipeline/README.md\",\n  \"content\": \"Pipeline Engine\\n\\n## Pipeline\\n\\n## Operator\\n\\n## OperatorFactory\\n1. Create operators from OperatorFactory, One OperatorFactory could have many operators.\\n2. Could store some read-only operator status in OperatorFactory, and Operator\\nonly store const reference\",\n  \"message\": \"Successfully read 10 lines from file: /Users/kks/git/starrocks/be/src/exec/pipeline/README.md\"\n}",
          "role": "tool",
          "tool_call_id": "call_MHxNN1JyRklyZVRvRFFXU2VNVVM"
        }
      ],
      "model": "gemini-3-flash-preview",
      "reasoning": {
        "effort": "low"
      },
      "reasoning_effort": "low",
      "stream": true,
      "stream_options": {
        "include_usage": true
      },
      "temperature": 1.0,
      "tools": [
        {
          "function": {
            "description": "Execute shell commands safely on the system.\n\nThis tool allows you to run shell commands with built-in safety restrictions. Choose commands based on the Platform info in the environment context:\n\n**Platform-specific command reference:**\n\n| Task | macOS/Linux | Windows |\n|------|-------------|---------|\n| List files | ls -la | dir |\n| Find files | find, fd | dir /s, where |\n| Search content | grep, rg | findstr |\n| Show file | cat, head, tail | type |\n| Current directory | pwd | cd |\n| Environment vars | env, export | set |\n| Process list | ps aux | tasklist |\n| Kill process | kill | taskkill |\n| Network info | ifconfig, ip | ipconfig |\n| Download file | curl, wget | curl, Invoke-WebRequest |\n| Archive | tar, zip | tar, Compress-Archive |\n| Package manager | brew (mac), apt (linux) | winget, choco |\n\n**Cross-platform commands:**\n- Git operations (git)\n- Node.js (node, npm, yarn, pnpm, bun)\n- Build tools (make, cargo, go)\n- Python (python, pip)\n\nThe command will be executed in the current working directory.\n\n**Background execution:**\nUse `run_in_background: true` to run long-running commands in the background. The command will continue running even if it produces no output for an extended period. Use this for:\n- Development servers\n- Long-running build processes\n- Continuous processes\n\nOutput can be read using `cat` or `tail -f` on the output file path returned in the result.",
            "name": "bash",
            "parameters": {
              "additionalProperties": false,
              "properties": {
                "command": {
                  "type": "string"
                },
                "runInBackground": {
                  "default": false,
                  "type": "boolean"
                }
              },
              "required": [
                "command"
              ],
              "type": "object"
            }
          },
          "type": "function"
        },
        {
          "function": {
            "description": "Call a registered sub-agent for a focused task. Subagents start with empty context.\n\n**Parameters**\n- `agentId`: The agent to call (see Available Subagents in system prompt)\n- `task`: Clear outcome description (2-5 sentences)\n- `context`: All necessary artifacts (file contents, schemas, requirements)\n- `targets`: Files/modules this agent will touch (for conflict avoidance)\n\n**Key Rules**\n- Subagents have zero history - pass all needed context\n- Use targets to enable safe parallel execution\n- For parallel calls, issue multiple callAgent in same response",
            "name": "callAgent",
            "parameters": {
              "additionalProperties": false,
              "properties": {
                "agentId": {
                  "type": "string"
                },
                "context": {
                  "type": "string"
                },
                "targets": {
                  "items": {
                    "type": "string"
                  },
                  "type": "array"
                },
                "task": {
                  "type": "string"
                }
              },
              "required": [
                "agentId",
                "task",
                "context"
              ],
              "type": "object"
            }
          },
          "type": "function"
        },
        {
          "function": {
            "description": "Use this tool to read the contents of an existing file.\n\nThis tool will return the complete file content as a string by default.\nYou can optionally specify a starting line and number of lines to read a specific portion of the file.\n\nThe file path should be an absolute path.",
            "name": "readFile",
            "parameters": {
              "additionalProperties": false,
              "properties": {
                "file_path": {
                  "type": "string"
                },
                "line_count": {
                  "type": "number"
                },
                "start_line": {
                  "type": "number"
                }
              },
              "required": [
                "file_path"
              ],
              "type": "object"
            }
          },
          "type": "function"
        },
        {
          "function": {
            "description": "Use this tool when you need to find files containing specific patterns.\n\nUse this to find code patterns, function definitions, variable usage, or any text in the codebase.",
            "name": "codeSearch",
            "parameters": {
              "additionalProperties": false,
              "properties": {
                "file_types": {
                  "items": {
                    "type": "string"
                  },
                  "type": "array"
                },
                "path": {
                  "type": "string"
                },
                "pattern": {
                  "type": "string"
                }
              },
              "required": [
                "pattern",
                "path"
              ],
              "type": "object"
            }
          },
          "type": "function"
        },
        {
          "function": {
            "description": "- Fast file pattern matching tool that works with any codebase size\n- Supports glob patterns like \"**/*.js\" or \"src/**/*.ts\"\n- Returns matching file paths sorted by modification time\n- Use this tool when you need to find files by name patterns",
            "name": "glob",
            "parameters": {
              "additionalProperties": false,
              "properties": {
                "path": {
                  "type": "string"
                },
                "pattern": {
                  "type": "string"
                }
              },
              "required": [
                "pattern"
              ],
              "type": "object"
            }
          },
          "type": "function"
        },
        {
          "function": {
            "description": "Perform Language Server Protocol (LSP) operations like go-to-definition, references, hover, and symbols.\n\nProvide a file path and a 1-based line/character position as shown in editors.",
            "name": "lsp",
            "parameters": {
              "additionalProperties": false,
              "properties": {
                "character": {
                  "type": "number"
                },
                "filePath": {
                  "type": "string"
                },
                "line": {
                  "type": "number"
                },
                "operation": {
                  "enum": [
                    "goToDefinition",
                    "findReferences",
                    "hover",
                    "documentSymbol",
                    "workspaceSymbol",
                    "goToImplementation",
                    "prepareCallHierarchy",
                    "incomingCalls",
                    "outgoingCalls"
                  ],
                  "type": "string"
                },
                "query": {
                  "type": "string"
                }
              },
              "required": [
                "operation",
                "filePath"
              ],
              "type": "object"
            }
          },
          "type": "function"
        },
        {
          "function": {
            "description": "Use this tool to list files and directories in a specified directory.\n\nThis tool will return a concise string format showing directories and files grouped by their parent directory. It's useful for exploring directory structure, finding files, or understanding project organization.\n\nThe directory path must be absolute.",
            "name": "listFiles",
            "parameters": {
              "additionalProperties": false,
              "properties": {
                "directory_path": {
                  "type": "string"
                },
                "max_depth": {
                  "type": "number"
                }
              },
              "required": [
                "directory_path"
              ],
              "type": "object"
            }
          },
          "type": "function"
        },
        {
          "function": {
            "description": "Creates and manages todo items for task tracking and progress management in the current session.\nUse this tool to create and manage todo items for tracking tasks and progress. This tool provides comprehensive todo management:\n\n## When to Use This Tool\n\nUse this tool proactively in these scenarios:\n\n1. **Complex multi-step tasks** - When a task requires 3 or more distinct steps or actions\n2. **Non-trivial and complex tasks** - Tasks that require careful planning or multiple operations\n3. **User explicitly requests todo list** - When the user directly asks you to use the todo list\n4. **User provides multiple tasks** - When users provide a list of things to be done (numbered or comma-separated)\n5. **After receiving new instructions** - Immediately capture user requirements as todos\n6. **When you start working on a task** - Mark it as in_progress BEFORE beginning work. Ideally you should only have one todo as in_progress at a time\n7. **After completing a task** - Mark it as completed and add any new follow-up tasks discovered during implementation\n\n## When NOT to Use This Tool\n\nSkip using this tool when:\n1. There is only a single, straightforward task\n2. The task is trivial and tracking it provides no organizational benefit\n3. The task can be completed in less than 3 trivial steps\n4. The task is purely conversational or informational\n\n## Task States and Management\n\n1. **Task States**: Use these states to track progress:\n   - pending: Task not yet started\n   - in_progress: Currently working on (limit to ONE task at a time)\n   - completed: Task finished successfully\n\n2. **Task Management**:\n   - Update task status in real-time as you work\n   - Mark tasks complete IMMEDIATELY after finishing (don't batch completions)\n   - Only have ONE task in_progress at any time\n   - Complete current tasks before starting new ones\n   - Remove tasks that are no longer relevant from the list entirely\n\n3. **Task Completion Requirements**:\n   - ONLY mark a task as completed when you have FULLY accomplished it\n   - If you encounter errors, blockers, or cannot finish, keep the task as in_progress\n   - When blocked, create a new task describing what needs to be resolved\n   - Never mark a task as completed if:\n     - Tests are failing\n     - Implementation is partial\n     - You encountered unresolved errors\n     - You couldn't find necessary files or dependencies\n\n4. **Task Breakdown**:\n   - Create specific, actionable items\n   - Break complex tasks into smaller, manageable steps\n   - Use clear, descriptive task names\n\n## Tool Capabilities\n\n- **Create new todos**: Add tasks with content, priority, and status\n- **Update existing todos**: Modify any aspect of a todo (status, priority, content)\n- **Delete todos**: Remove completed or irrelevant tasks\n- **Batch operations**: Update multiple todos in a single operation\n- **Clear all todos**: Reset the entire todo list\n\nWhen in doubt, use this tool. Being proactive with task management demonstrates attentiveness and ensures you complete all requirements successfully.",
            "name": "todoWrite",
            "parameters": {
              "additionalProperties": false,
              "properties": {
                "todos": {
                  "items": {
                    "additionalProperties": false,
                    "properties": {
                      "content": {
                        "type": "string"
                      },
                      "id": {
                        "type": "string"
                      },
                      "status": {
                        "enum": [
                          "pending",
                          "in_progress",
                          "completed"
                        ],
                        "type": "string"
                      }
                    },
                    "required": [
                      "content",
                      "status",
                      "id"
                    ],
                    "type": "object"
                  },
                  "type": "array"
                }
              },
              "required": [
                "todos"
              ],
              "type": "object"
            }
          },
          "type": "function"
        },
        {
          "function": {
            "description": "Use this tool to write content to a file.\n\nThis tool will create or overwrite a file with the provided content. It will automatically create directories if they don't exist. It's useful for creating new files or updating existing files in the project.\n\nThe file path should be an absolute path.",
            "name": "writeFile",
            "parameters": {
              "additionalProperties": false,
              "properties": {
                "content": {
                  "type": "string"
                },
                "file_path": {
                  "type": "string"
                }
              },
              "required": [
                "file_path",
                "content"
              ],
              "type": "object"
            }
          },
          "type": "function"
        },
        {
          "function": {
            "description": "Edit an existing file with one or more text replacements.\n\nCRITICAL RULES:\n1. All old_string values must match EXACTLY - including spaces, tabs, and newlines\n2. Use readFile tool FIRST to see the exact content\n3. Include 3-5 lines of context before and after each change\n4. For file creation, use write-file tool instead\n\nHOW TO CHOOSE THE NUMBER OF EDITS:\n\nSingle Edit (edits.length = 1):\n✅ One isolated change\n✅ Simple fix or update in one location\n✅ Experimental/uncertain replacement\n✅ When you need to verify the result before continuing\n\nExample: \"Add an import statement\" → 1 edit\n\nMultiple Edits (edits.length = 2-10):\n✅ Related changes to the same file\n✅ Batch updates with similar patterns (e.g., rename all occurrences)\n✅ Refactoring that touches multiple parts (imports + function + types)\n✅ You're confident all replacements will work\n\nExample: \"Add import + update function + update type\" → 3 edits\n\nIMPORTANT: Maximum 10 edits per call. For larger refactorings, make multiple calls.\n\nBest practice workflow:\n1. Use readFile to see current content\n2. Identify all changes you want to make to this file\n3. For each change, copy EXACT text with context\n4. Create edit block(s) with old_string and new_string\n5. Call edit-file with your edit(s)",
            "name": "editFile",
            "parameters": {
              "additionalProperties": false,
              "properties": {
                "edits": {
                  "items": {
                    "additionalProperties": false,
                    "properties": {
                      "new_string": {
                        "type": "string"
                      },
                      "old_string": {
                        "type": "string"
                      }
                    },
                    "required": [
                      "old_string",
                      "new_string"
                    ],
                    "type": "object"
                  },
                  "type": "array"
                },
                "file_path": {
                  "type": "string"
                }
              },
              "required": [
                "file_path",
                "edits"
              ],
              "type": "object"
            }
          },
          "type": "function"
        },
        {
          "function": {
            "description": "Search the web for comprehensive and up-to-date information.\n\nQuery Optimization Guidelines:\n- Extract the core topic/entity from questions (e.g., \"who is Elon Musk\" → \"Elon Musk\", \"what is React\" → \"React\")\n- For comparisons (e.g., \"X vs Y\"), search for both terms together to get comparison results\n- Remove question words (who, what, when, where, why, how) but keep context words that add meaning\n- Keep technical terms, version numbers, and specific qualifiers (e.g., \"React 19 features\" stays as-is)\n- For \"latest\" or \"recent\" queries, include temporal keywords (e.g., \"latest AI models 2025\")\n- Preserve programming language/framework context (e.g., \"error handling in Rust\" → \"Rust error handling\")\n- For debugging queries, keep error messages and stack traces intact\n- Use multiple searches only when topics are completely unrelated, not for comparisons",
            "name": "webSearch",
            "parameters": {
              "additionalProperties": false,
              "properties": {
                "query": {
                  "type": "string"
                }
              },
              "required": [
                "query"
              ],
              "type": "object"
            }
          },
          "type": "function"
        },
        {
          "function": {
            "description": "Fetch and extract content from a web url",
            "name": "webFetch",
            "parameters": {
              "additionalProperties": false,
              "properties": {
                "url": {
                  "type": "string"
                }
              },
              "required": [
                "url"
              ],
              "type": "object"
            }
          },
          "type": "function"
        }
      ],
      "top_k": 64,
      "top_p": 0.949999988079071
    }
  },
  "response": {
    "type": "stream",
    "status": 200,
    "headers": {
      "x-github-request-id": "C970:3856B5:12BBE7:194960:697FBE6E",
      "date": "Sun, 01 Feb 2026 20:58:28 GMT",
      "x-request-id": "00000-3abb5512-9adf-47ac-8e97-0238f58d1c9a",
      "strict-transport-security": "max-age=31536000",
      "transfer-encoding": "chunked",
      "content-security-policy": "default-src 'none'; sandbox",
      "x-github-backend": "Kubernetes",
      "content-type": "text/event-stream",
      "cache-control": "no-cache"
    },
    "sse_events": [
      {
        "event": null,
        "data": "{\"choices\":[{\"index\":0,\"delta\":{\"content\":\"In StarRocks, the relationship between **Fragments** and **Pipelines** is one of **hierarchical decomposition**: a Fragment is a distributed unit of execution that is further broken down into one or more Pipelines for local execution on\",\"role\":\"assistant\"}}],\"created\":1769979508,\"id\":\"b75_aeNfrYyXug_BhPWRCQ\",\"usage\":{\"completion_tokens\":0,\"prompt_tokens\":0,\"prompt_tokens_details\":{\"cached_tokens\":0},\"total_tokens\":0,\"reasoning_tokens\":0},\"model\":\"gemini-3-flash-preview\"}"
      },
      {
        "event": null,
        "data": "{\"choices\":[{\"index\":0,\"delta\":{\"content\":\" a Backend (BE).\\n\\n### 1. PlanFragment (The Distributed Unit)\\nA **PlanFragment** is the unit of distribution and scheduling in StarRocks.\\n*   **Logical Scope**: It encapsulates a portion of the SQL execution plan (a tree of `PlanNodes`).\\n*   **Connectivity**: Fragments are connected\",\"role\":\"assistant\"}}],\"created\":1769979508,\"id\":\"b75_aeNfrYyXug_BhPWRCQ\",\"usage\":{\"completion_tokens\":0,\"prompt_tokens\":0,\"prompt_tokens_details\":{\"cached_tokens\":0},\"total_tokens\":0,\"reasoning_tokens\":0},\"model\":\"gemini-3-flash-preview\"}"
      },
      {
        "event": null,
        "data": "{\"choices\":[{\"index\":0,\"delta\":{\"content\":\" to each other via **Exchange Nodes**. Data is shuffled or broadcasted between fragments across the network.\\n*   **Instantiation**: When a query runs, a `PlanFragment` is instantiated into multiple **Fragment Instances**, each running on a specific Backend.\\n\\n### 2. Pipeline (The Local Execution Unit)\\n\",\"role\":\"assistant\"}}],\"created\":1769979508,\"id\":\"b75_aeNfrYyXug_BhPWRCQ\",\"usage\":{\"completion_tokens\":0,\"prompt_tokens\":0,\"prompt_tokens_details\":{\"cached_tokens\":0},\"total_tokens\":0,\"reasoning_tokens\":0},\"model\":\"gemini-3-flash-preview\"}"
      },
      {
        "event": null,
        "data": "{\"choices\":[{\"index\":0,\"delta\":{\"content\":\"A **Pipeline** is the unit of local execution within a Fragment Instance.\\n*   **Decomposition**: When a Fragment Instance starts on a BE, the `PipelineBuilder` decomposes the fragment's `ExecNode` tree into a set of `Pipelines`.\\n*   **Structure**: Each Pipeline consists of a\",\"role\":\"assistant\"}}],\"created\":1769979509,\"id\":\"b75_aeNfrYyXug_BhPWRCQ\",\"usage\":{\"completion_tokens\":0,\"prompt_tokens\":0,\"prompt_tokens_details\":{\"cached_tokens\":0},\"total_tokens\":0,\"reasoning_tokens\":0},\"model\":\"gemini-3-flash-preview\"}"
      },
      {
        "event": null,
        "data": "{\"choices\":[{\"index\":0,\"delta\":{\"content\":\" chain of **Operators** (Source -\\u003e Transformation -\\u003e Sink).\\n*   **Pipelining**: Within a pipeline, data (in Chunks) flows through operators in a push/pull fashion without blocking.\\n*   **Boundaries**: Pipelines are separated by **Pipeline Breakers** (blocking operators). For example, a\",\"role\":\"assistant\"}}],\"created\":1769979509,\"id\":\"b75_aeNfrYyXug_BhPWRCQ\",\"usage\":{\"completion_tokens\":0,\"prompt_tokens\":0,\"prompt_tokens_details\":{\"cached_tokens\":0},\"total_tokens\":0,\"reasoning_tokens\":0},\"model\":\"gemini-3-flash-preview\"}"
      },
      {
        "event": null,
        "data": "{\"choices\":[{\"index\":0,\"delta\":{\"content\":\" Hash Join splits a fragment into:\\n    1.  A **Build Pipeline**: Reads data and builds a hash table (Sink).\\n    2.  A **Probe Pipeline**: Reads data from the other side (Source) and probes the hash table.\\n\\n### 3. Key Relationships\\n\\n| Feature | Plan\",\"role\":\"assistant\"}}],\"created\":1769979510,\"id\":\"b75_aeNfrYyXug_BhPWRCQ\",\"usage\":{\"completion_tokens\":0,\"prompt_tokens\":0,\"prompt_tokens_details\":{\"cached_tokens\":0},\"total_tokens\":0,\"reasoning_tokens\":0},\"model\":\"gemini-3-flash-preview\"}"
      },
      {
        "event": null,
        "data": "{\"choices\":[{\"index\":0,\"delta\":{\"content\":\"Fragment | Pipeline |\\n| :--- | :--- | :--- |\\n| **Defined in** | Frontend (Java) | Backend (C++) |\\n| **Granularity** | Coarse (Distributed sub-tree) | Fine (Local data flow segment) |\\n| **Parallelism** | Parallelized\",\"role\":\"assistant\"}}],\"created\":1769979510,\"id\":\"b75_aeNfrYyXug_BhPWRCQ\",\"usage\":{\"completion_tokens\":0,\"prompt_tokens\":0,\"prompt_tokens_details\":{\"cached_tokens\":0},\"total_tokens\":0,\"reasoning_tokens\":0},\"model\":\"gemini-3-flash-preview\"}"
      },
      {
        "event": null,
        "data": "{\"choices\":[{\"index\":0,\"delta\":{\"content\":\" via multiple instances on different nodes. | Parallelized via multiple **Drivers** (instantiations of the pipeline) on different CPU cores. |\\n| **Communication** | Network RPC (via ExchangeNodes). | Memory buffers/Local Exchange (via Sink/Source operators). |\\n| **Lifecycle** | Managed by `FragmentContext\",\"role\":\"assistant\"}}],\"created\":1769979510,\"id\":\"b75_aeNfrYyXug_BhPWRCQ\",\"usage\":{\"completion_tokens\":0,\"prompt_tokens\":0,\"prompt_tokens_details\":{\"cached_tokens\":0},\"total_tokens\":0,\"reasoning_tokens\":0},\"model\":\"gemini-3-flash-preview\"}"
      },
      {
        "event": null,
        "data": "{\"choices\":[{\"index\":0,\"delta\":{\"content\":\"`. | Managed by `PipelineDriver` and scheduled by the Pipeline Engine. |\\n\\n### Summary of the Hierarchy\\n1.  **Query**: The total SQL execution.\\n2.  **Fragment**: A segment of the query that can run on multiple nodes.\\n3.  **Fragment Instance**: The execution of a fragment on\",\"role\":\"assistant\"}}],\"created\":1769979511,\"id\":\"b75_aeNfrYyXug_BhPWRCQ\",\"usage\":{\"completion_tokens\":0,\"prompt_tokens\":0,\"prompt_tokens_details\":{\"cached_tokens\":0},\"total_tokens\":0,\"reasoning_tokens\":0},\"model\":\"gemini-3-flash-preview\"}"
      },
      {
        "event": null,
        "data": "{\"choices\":[{\"index\":0,\"delta\":{\"content\":\" a single node.\\n4.  **Pipeline**: A chain of operators within an instance, separated by blocking points.\\n5.  **Pipeline Driver**: A thread-executable task representing one parallel unit of a Pipeline.\\n\\nIn short, **Fragments manage \\\"Where\\\" data moves across the cluster, while Pipelines manage \\\"\",\"role\":\"assistant\"}}],\"created\":1769979511,\"id\":\"b75_aeNfrYyXug_BhPWRCQ\",\"usage\":{\"completion_tokens\":0,\"prompt_tokens\":0,\"prompt_tokens_details\":{\"cached_tokens\":0},\"total_tokens\":0,\"reasoning_tokens\":0},\"model\":\"gemini-3-flash-preview\"}"
      },
      {
        "event": null,
        "data": "{\"choices\":[{\"finish_reason\":\"stop\",\"index\":0,\"delta\":{\"content\":\"How\\\" data flows through CPU cores within a single node.**\",\"role\":\"assistant\",\"reasoning_opaque\":\"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\"}}],\"created\":1769979511,\"id\":\"b75_aeNfrYyXug_BhPWRCQ\",\"usage\":{\"completion_tokens\":647,\"prompt_tokens\":40966,\"prompt_tokens_details\":{\"cached_tokens\":39999},\"total_tokens\":42155,\"reasoning_tokens\":542},\"model\":\"gemini-3-flash-preview\"}"
      },
      {
        "event": null,
        "data": "[DONE]"
      }
    ]
  },
  "test_input": {
    "model": "gemini-3-flash-preview",
    "messages": [
      {
        "role": "system",
        "content": "\nYou are TalkCody, an expert Coding Planner and Lead Engineer. Your mandate is to orchestrate complex software tasks, manage sub-agents, and execute code changes with precision. You operate within a defined workspace and must strictly adhere to project constraints defined in AGENTS.md.\n\n# CORE IDENTITY & INTERACTION\n- **Orchestrator**: You clarify ambiguity immediately, then drive the task to completion.\n- **Directness**: Respond in the user's language. Omit conversational filler. Your output must be dense with utility (code, plans, or direct answers).\n- **Context Aware**: Your \"Source of Truth\" is the file system and AGENTS.md. Do not hallucinate APIs or dependencies.\n\n# TOOL USAGE STRATEGY\n\n## Concurrency & Batch Tool Calls\n\n**return as many tool calls as possible in a single response**.\n\n### Read Operations - ALWAYS Batch Together\n```\nI need to understand the authentication system. Making all read calls at once:\n\n[Tool Calls]\n- read-file: /src/auth/login.ts\n- read-file: /src/auth/register.ts\n- read-file: /src/auth/middleware.ts\n- read-file: /src/auth/types.ts\n- read-file: /src/lib/jwt.ts\n- read-file: /src/auth/session.ts\n- read-file: /src/auth/permissions.ts\n- glob: /src/auth/**/*.test.ts\n```\n\n### Write/Edit Operations - Batch Different Files\n```\nCreating 5 new components. Making all write calls at once:\n\n[Tool Calls]\n- write-file: /src/components/Button.tsx\n- write-file: /src/components/Input.tsx\n- write-file: /src/components/Card.tsx\n- write-file: /src/components/Modal.tsx\n- write-file: /src/components/Table.tsx\n```\n\n(Multiple edits to different files):\n```\n[Tool Calls]\n- edit-file: /src/app/page.tsx\n- edit-file: /src/app/layout.tsx\n- edit-file: /src/lib/utils.ts\n```\n\n## Decision Matrix\n\n### 1. Direct Execution\nUse standard tools (`readFile`, `editFile`, `codeSearch`) for straightforward tasks.\n- **Scope**: Single-file edits, simple bug fixes, or quick info gathering.\n- **Logic**: When the task is clearly bounded and you have all necessary context.\n\n### 2. Strategic Delegation (`callAgent`)\nActively consider delegation for enhanced efficiency and quality. When using `callAgent`, **always** consider whether multiple sub-agents can work in parallel to maximize efficiency.\n- **Complex Analysis**: Multi-domain analysis (architecture, testing, security, performance) benefits from specialized focus.\n- **Parallel Processing**: Independent modules or components can be handled simultaneously for faster completion.\n- **Expertise Areas**: Tasks requiring deep domain knowledge (AI integration, database design, security audits) benefit from focused sub-agents.\n- **Large Scope**: When comprehensive analysis across multiple files/systems is needed.\n\n## Delegation Protocol\n- **Gather First**: Use `codeSearch`, `readFile`, `glob` tool or explore agent to gather context *before* calling an agent to ensure you pass valid context.\n- **Explicit Payload**:\n  - `context`: Dump relevant file contents/search results here. Do not assume the agent knows previous chat history.\n  - `targets`: List specific files to avoid overwrite conflicts.\n- **Write Tasks**: When delegating multi-file or multi-module modifications, break them into clearly separated, independent tasks to enable parallel execution without file conflicts.\n- **Task Decomposition**: Each sub-agent should handle a complete, self-contained modification unit (e.g., one component, one service, one feature module) with exact file boundaries.\n- **Dependency Management**: If tasks have dependencies, clearly define execution sequence or use separate delegation rounds.\n\n## callAgent Parallelism Strategy\n\n**ALWAYS maximize parallelism** - spawn multiple subagents in a single response when tasks are independent.\n\n### Strategy 1: Same Agent Type, Different Modules\nUse multiple instances of the same agent to gather context from different areas:\n```\n[Tool Calls]\n- callAgent: explore → gather context from /src/auth\n- callAgent: explore → gather context from /src/api\n- callAgent: explore → gather context from /src/db\n```\n\n### Strategy 2: Different Agent Types, Different Concerns\nUse specialized agents in parallel for different aspects of the same feature:\n```\n[Tool Calls]\n- callAgent: document-writer → write API documentation for the new feature\n- callAgent: coding → implement the core business logic\n- callAgent: code-review → write unit tests for the feature\n```\n\n### Parallelism Decision Matrix\n| Scenario | Strategy |\n|----------|----------|\n| Context gathering from multiple modules | Multiple parallel explore agents |\n| Feature implementation (code + docs + tests) | Parallel coding + document-writer + code-review |\n| Multi-file refactor | Multiple parallel coding agents with distinct targets |\n| Dependent tasks (A's output feeds B) | Sequential callAgent rounds |\n\n## Edit-File Tool\n\n**When to use edit-file tool vs write-file tool:**\n   - **edit-file**: File exists, making modifications (1-10 related changes per file)\n     - Single edit: One isolated change\n     - Multiple edits: Related changes to same file (imports + types, code)\n   - **write-file**: Creating a brand new file from scratch\n   - **write-file**: overwrite existing file when too many changes are needed\n\n# ENGINEERING GUIDELINES\n**Philosophy**: Keep It Simple, Stupid (KISS). Prioritize maintainability and readability over clever one-liners.\n\n1. **Building from Scratch**:\n   - Confirm requirements first.\n   - Sketch the architecture/module interaction mentally or in the plan.\n   - Implement modular, strictly typed (where applicable), and self-documenting code.\n\n2. **Modifying Existing Code**:\n   - **Understand First**: Read related files to grasp the current patterns and style.\n   - **Minimal Intrusion**: Make the smallest change necessary to achieve the goal. Avoid sweeping formatting changes unless requested.\n   - **Bug Fixes**: Locate the root cause via logs or reproduction steps. Ensure your fix addresses the root cause, not just the symptom. Verify with tests.\n\n3. **Refactoring**:\n   - Only change internal structure, never external behavior (unless it's an API breaking change request).\n   - Update all consumers of the refactored code.\n\n# WORKFLOW Mode: ACT vs. PLAN\n\n## ACT VS PLAN\n\n- For trivial and simple tasks, ACT directly using tools.\n- For complex tasks, PLAN first then ACT.\n- If the task involves multiple files, architectural changes, or high ambiguity, you MUST enter **PLAN MODE**.\n\n**CRITICAL RULE**: Check the environment context provided in the <env> XML section. If you see:\n```xml\n<env>\n  <plan_mode>TRUE</plan_mode>\n</env>\n```\n\nThis means PLAN MODE is ENABLED. When PLAN MODE is ENABLED (`<plan_mode>TRUE</plan_mode>`):\n- You MUST follow the PLAN MODE instructions provided below\n- You MUST delegate planning to the Plan Agent before executing complex tasks\n- DO NOT skip the planning phase\n\nWhen PLAN MODE is DISABLED (`<plan_mode>FALSE</plan_mode>`):\n- You can execute tasks directly without delegating to the Plan Agent\n- Use your judgment to decide between ACT and PLAN based on task complexity\n\n# PLAN MODE Workflow\n\n## Overview\n\nWhen PLAN MODE is ENABLED (indicated by `<plan_mode>TRUE</plan_mode>` in the <env> section), you should delegate planning to the specialized **Plan Agent**. The Plan Agent will:\n1. Explore the project context\n2. Generate a structured plan\n3. Return the approved plan content\n\n## Delegation to Plan Agent\n\n### How to Delegate\n\nUse `callAgent` with the \"plan\" agent:\n\n```\ncallAgent({\n  agentId: \"plan\",\n  task: \"Create a comprehensive implementation plan for: {USER_REQUIREMENT}\",\n  context: `\nProject Root: {workspaceRoot}\nTask ID: {taskId}\nCurrent Requirement: {USER_REQUIREMENT}\n\nRelevant Context:\n- Project structure and existing patterns\n- Related files that may be affected\n- Any specific constraints or requirements mentioned\n  `,\n  targets: []\n})\n```\n\nNote: you should pass enough context to the Plan Agent to enable it to generate a comprehensive plan.\n\n### Receiving Plan Agent Results\n\nThe Plan Agent returns plan by markdown format\n\n- Use the approved plan to create a todo list using **TodoWrite**\n- Execute each todo item\n- Update todo status as you complete them\n\n## Executing Approved Plans\n\nAfter receiving approved plan:\n\n1. **Create todos** using **TodoWrite** based on the approved plan\n2. **Execute todos systematically**:\n   - Process dependent tasks first\n   - Use parallel execution for independent tasks\n   - Update todo status after each completion\n3. **Report progress** to user\n4. **Handle errors** gracefully with clear messages\n\n# SAFETY & BOUNDARIES\n- **Workspace Confinement**: strict operations within the allowed root directories.\n- **Non-Destructive**: Never delete non-trivial code without explicit confirmation in the Plan.\n- **Secrets Management**: Never print or hardcode credentials/secrets.\n\n# OBJECTIVE\nYour goal is not to chat, but to ship. Measure success by:\n1. Accuracy of the solution.\n2. Stability of the code.\n3. Adherence to existing project styles.\n\n# Rules\n\n- The user may provide a file's contents directly in their message, in which case you shouldn't use the read_file tool to get the file contents again since you already have it.\n- Be precise with replacements to avoid errors\n- Follow existing project patterns and conventions\n- Answer the user's question directly with a concise answer; do not generate new Markdown files to answer the user's question.\n\n\n---\n\nIMPORTANT: Here is important information about the environment you are running in:\n<env>\n<working_directory>/Users/kks/git/starrocks</working_directory>\n<is_git_repo>Yes</is_git_repo>\n<platform>mac_arm</platform>\n<today_date>2026-02-01</today_date>\n<plan_mode>FALSE</plan_mode>\n</env>\n\n---\n\n## Markdown Context\n\n<project>\n# AGENTS.md - StarRocks\n\n> This file provides context for AI coding agents working with the StarRocks codebase.\n> Compatible with: Claude Code, OpenCode, Cursor, Gemini CLI, Windsurf, Aider, Continue, Cline, and other MCP-compatible tools.\n\n## Project Overview\n\nStarRocks is a high-performance, real-time analytical database. It delivers sub-second query latency for complex analytics workloads through its MPP (Massively Parallel Processing) architecture.\n\n**Key capabilities:**\n- Real-time analytics with sub-second latency\n- High-concurrency, low-latency queries\n- Support for both shared-nothing and shared-data deployments\n- Materialized views for query acceleration\n- External data source federation (Hive, Iceberg, Hudi, Delta Lake, JDBC, etc.)\n\n## Architecture\n\nStarRocks uses a decoupled frontend-backend architecture:\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                      Frontend (FE)                          │\n│  Java │ SQL Parsing │ Query Planning │ Metadata Management  │\n└────────────────────────────┬────────────────────────────────┘\n                             │ Thrift RPC\n┌────────────────────────────▼────────────────────────────────┐\n│                      Backend (BE)                           │\n│  C++ │ Query Execution │ Storage Engine │ Data Processing   │\n└─────────────────────────────────────────────────────────────┘\n```\n\n- **FE (Frontend)**: Java-based. Handles SQL parsing, query optimization, metadata management, and cluster coordination.\n- **BE (Backend)**: C++-based. Handles query execution, storage management, and data processing.\n\n## Directory Structure\n\n```\nstarrocks/\n├── be/                    # Backend (C++) - Query execution & storage\n├── fe/                    # Frontend (Java) - SQL & metadata\n│   ├── fe-core/          # Core FE logic\n│   ├── fe-parser/        # SQL parser\n│   ├── fe-grammar/       # ANTLR grammar files\n│   ├── fe-type/          # Type system\n│   ├── fe-spi/           # Service Provider Interfaces\n│   ├── connector/        # External data source connectors\n│   └── plugin/           # FE plugins\n├── java-extensions/       # JNI connectors for external sources\n├── gensrc/               # Generated code (Thrift, Protobuf)\n├── test/                 # SQL integration tests\n├── docs/                 # Documentation (Docusaurus)\n├── thirdparty/           # Third-party dependencies\n└── docker/               # Docker build files\n```\n\n## Quick Commands\n\n### Build\n\n```bash\n# Build everything\n./build.sh --fe --be --clean\n\n# Build only Backend\n./build.sh --be\n\n# Build only Frontend\n./build.sh --fe\n\n# Build with specific type (Release/Debug/ASAN)\nBUILD_TYPE=Debug ./build.sh --be\nBUILD_TYPE=ASAN ./build.sh --be\n```\n\n### Run Tests\n\n```bash\n# Run all FE unit tests\n./run-fe-ut.sh\n\n# Run specific FE test\n./run-fe-ut.sh --test com.starrocks.sql.plan.TPCHPlanTest\n\n# Run all BE unit tests\n./run-be-ut.sh\n\n# Run specific BE test\n./run-be-ut.sh --test CompactionUtilsTest\n\n# Run with gtest filter\n./run-be-ut.sh --gtest_filter \"TabletUpdatesTest*\"\n\n# Run SQL integration tests (requires running cluster)\ncd test && python3 run.py -v\n```\n\n### Code Formatting\n\n```bash\n# Format C++ code (BE)\nclang-format -i <file.cpp>\n\n# Check Java code style (FE)\ncd fe && mvn checkstyle:check\n```\n\n## Code Style Summary\n\n### C++ (Backend)\n- **Style**: Google C++ Style (with modifications)\n- **Config**: `.clang-format` at project root\n- **Indent**: 4 spaces\n- **Line limit**: 120 characters\n- **Pointer alignment**: Left (`int* ptr`)\n\n### Java (Frontend)\n- **Style**: Google Java Style (with modifications)\n- **Config**: `fe/checkstyle.xml`\n- **Indent**: 4 spaces\n- **Line limit**: 130 characters\n- **Import order**: Third-party, then Java standard, then static\n\n### Protobuf\n- Message names: `PascalCasePB` (e.g., `MyMessagePB`)\n- Field names: `snake_case`\n- **Never** use `required` fields\n- **Never** change field ordinals\n\n### Thrift\n- Struct names: `TPascalCase` (e.g., `TMyStruct`)\n- Field names: `snake_case`\n- **Never** use `required` fields\n- **Never** change field ordinals\n\n## Documentation Sync Requirements\n\n### Configuration Changes\n\n**When you add, modify, or remove configuration parameters, you MUST update the corresponding documentation:**\n\n| Component | Documentation File |\n|-----------|-------------------|\n| FE Config | `docs/en/administration/management/FE_configuration.md` |\n| BE Config | `docs/en/administration/management/BE_configuration.md` |\n\n**Required documentation for each config parameter:**\n- Parameter name\n- Default value\n- Value range (if applicable)\n- Description of what it controls\n- When to use/modify it\n- Whether it requires restart\n\n### Metrics Changes\n\n**When you add, modify, or remove metrics, you MUST update the corresponding documentation:**\n\n| Metrics Type | Documentation File |\n|--------------|-------------------|\n| General Metrics | `docs/en/administration/management/monitoring/metrics.md` |\n| Shared-Data Metrics | `docs/en/administration/management/monitoring/metrics-shared-data.md` |\n| MV Metrics | `docs/en/administration/management/monitoring/metrics-materialized_view.md` |\n| BE Metrics (SQL) | `docs/en/sql-reference/information_schema/be_metrics.md` |\n| FE Metrics (SQL) | `docs/en/sql-reference/information_schema/fe_metrics.md` |\n\n**Required documentation for each metric:**\n- Metric name\n- Type (Counter, Gauge, Histogram)\n- Labels (if any)\n- Description of what it measures\n- Unit (if applicable)\n\n### PR Checklist for Config/Metrics Changes\n\nWhen your PR includes configuration or metrics changes:\n\n- [ ] Updated corresponding documentati\n...\n[trimmed]\n...\n\n\n| Issue | Solution |\n|-------|----------|\n| CI timeout | Re-run failed jobs; check for flaky tests |\n| Checkstyle failure | Run `mvn checkstyle:check` locally |\n| Build failure | Run `./build.sh --fe --be` locally |\n| Merge conflicts | Rebase on latest main branch |\n\n## Testing Guidelines\n\n### Unit Tests\n- **BE**: Use Google Test framework. Tests in `be/test/` mirror source structure.\n- **FE**: Use JUnit 5. Tests in `fe/fe-core/src/test/java/`.\n\n### SQL Integration Tests\n- Use the SQL-tester framework in `test/`\n- See `test/README.md` for detailed documentation\n- Basic execution: `python3 run.py -v`\n\n### Test Requirements\n- All new features must have corresponding tests\n- Bug fixes should include regression tests\n- Maintain or improve test coverage\n\n## Performance Considerations\n\n### Backend (C++)\n- Use SIMD instructions where applicable (AVX2/AVX512)\n- Prefer vectorized processing over row-by-row\n- Use `Column` and `Chunk` abstractions for data processing\n- Be mindful of memory allocations in hot paths\n- Profile with `perf` or async-profiler before optimization\n\n### Frontend (Java)\n- Avoid creating unnecessary objects in hot paths\n- Use appropriate data structures for the use case\n- Be cautious with synchronized blocks\n\n## Security Guidelines\n\n- Validate all user inputs at system boundaries\n- Never log sensitive information (passwords, tokens, PII)\n- Follow existing authentication/authorization patterns\n- Use parameterized queries to prevent SQL injection\n\n## Nested AGENTS.md Files\n\nFor module-specific guidelines, refer to:\n- `be/AGENTS.md` - Backend C++ development\n- `fe/AGENTS.md` - Frontend Java development\n- `java-extensions/AGENTS.md` - JNI connector development\n- `gensrc/AGENTS.md` - Generated code handling\n- `test/AGENTS.md` - SQL integration testing\n- `docs/AGENTS.md` - Documentation contribution\n\n## Useful Resources\n\n- [Official Documentation](https://docs.starrocks.io/)\n- [Contributing Guide](./CONTRIBUTING.md)\n- [GitHub Issues](https://github.com/StarRocks/starrocks/issues)\n- [Slack Community](https://try.starrocks.com/join-starrocks-on-slack)\n\n## License\n\nStarRocks is licensed under the Apache License 2.0.\n\nAll source files must include the appropriate license header:\n```\n// Copyright 2021-present StarRocks, Inc. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// ...\n```\n\n</project>\n\n---\n\nOUTPUT FORMAT INSTRUCTIONS:\nOutput the response in standard Markdown with headings, lists, and code blocks.\n\n---\n\n====\n# Available Subagents\n\nThe following agents are available for delegation via the `callAgent` tool.\nPick the best-fit subagent for each task (do not assume only one exists):\n\n- **plan**: Professional software planning expert that generates structured plans and obtains user approval\n- **code-review**: Multi-source code review specialist for GitHub PRs, local commits, git diffs, and project files\n- **explore**: Efficient single-task information gathering\n- **document-writer**: Creates comprehensive documentation matching repository standards\n- **test-writer**: Specialist in writing comprehensive tests matching repository standards\n- **coding**: Expert programming assistant for code reviews, debugging, and development\n- **china-options-buyer**: professional China commodity options trading strategist focused on long (buyer) strategies.\n- **api-designer**: API architecture expert designing scalable, developer-friendly interfaces. Creates REST and GraphQL APIs with comprehensive documentation, focusing on consistency, performance, and developer experience.\n- **code-reviewer**: Expert code reviewer specializing in code quality, security vulnerabilities, and best practices across multiple languages. Masters static analysis, design patterns, and performance optimization with focus on maintainability and technical debt reduction.\n\n## Delegation Guidelines\n\n- **Choose wisely**: Select the agent best suited for the specific task\n- **Provide context**: Subagents start with empty context; pass all necessary information\n- **Set targets**: Specify files/modules to avoid conflicts in parallel execution\n- **Batch independent tasks**: Run multiple subagents in parallel when tasks are independent\n====\n\nIMPORTANT: You MUST respond in English.",
        "providerOptions": {
          "anthropic": {
            "cacheControl": {
              "type": "ephemeral"
            }
          }
        }
      },
      {
        "role": "user",
        "content": "What is the relationship between Starrrocks' pipeline and fragments?",
        "providerOptions": null
      },
      {
        "role": "assistant",
        "content": [
          {
            "type": "tool-call",
            "toolCallId": "call_MHxmNDFzVzVleUhQNzUzS2R0a0s",
            "toolName": "codeSearch",
            "input": {
              "path": "docs/",
              "pattern": "pipeline fragment"
            }
          }
        ],
        "providerOptions": null
      },
      {
        "role": "tool",
        "content": [
          {
            "type": "tool-result",
            "toolCallId": "call_MHxmNDFzVzVleUhQNzUzS2R0a0s",
            "toolName": "codeSearch",
            "output": {
              "type": "text",
              "value": "{\n  \"success\": true,\n  \"result\": \"No matches found\"\n}"
            }
          }
        ],
        "providerOptions": null
      },
      {
        "role": "assistant",
        "content": [
          {
            "type": "tool-call",
            "toolCallId": "call_MHxjQkJTbXFRT08zZHhGWDVoTHU",
            "toolName": "codeSearch",
            "input": {
              "path": "be/src/exec/pipeline/",
              "pattern": "Pipeline"
            }
          }
        ],
        "providerOptions": null
      },
      {
        "role": "tool",
        "content": [
          {
            "type": "tool-result",
            "toolCallId": "call_MHxjQkJTbXFRT08zZHhGWDVoTHU",
            "toolName": "codeSearch",
            "output": {
              "type": "text",
              "value": "{\n  \"success\": true,\n  \"result\": \"Found 234 matches:\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/exchange/split_local_exchange.h\\n  19: #include \\\"exec/pipeline/source_operator.h\\\"\\n  22: namespace starrocks::pipeline {\\n  72: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/exchange/mem_limited_chunk_queue.h\\n  20: #include \\\"exec/pipeline/exchange/multi_cast_local_exchange.h\\\"\\n  26: namespace starrocks::pipeline {\\n  241: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/pipeline_driver_poller.h\\n  23: #include \\\"pipeline_driver.h\\\"\\n  24: #include \\\"pipeline_driver_queue.h\\\"\\n  27: namespace starrocks::pipeline {\\n  29: class PipelineDriverPoller;\\n  30: using PipelineDriverPollerPtr = std::unique_ptr<PipelineDriverPoller>;\\n  33: class PipelineDriverPoller {\\n  35: explicit PipelineDriverPoller(std::string name, DriverQueue* driver_queue, CpuUtil::CpuIds cpuids,\\n  45: ~PipelineDriverPoller() { shutdown(); }\\n  47: DISALLOW_COPY(PipelineDriverPoller);\\n  92: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/pipeline_metrics.h\\n  25: namespace pipeline {\\n  102: struct PipelineExecutorMetrics {\\n  123: } // namespace pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/hashjoin/hash_joiner_factory.cpp\\n  15: #include \\\"exec/pipeline/hashjoin/hash_joiner_factory.h\\\"\\n  17: namespace starrocks::pipeline {\\n  87: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/audit_statistics_reporter.cpp\\n  15: #include \\\"exec/pipeline/audit_statistics_reporter.h\\\"\\n  25: namespace starrocks::pipeline {\\n  57: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/aggregate/aggregate_distinct_blocking_sink_operator.cpp\\n  20: namespace starrocks::pipeline {\\n  81: if (size >= _aggregator->limit() || (_aggregator->params()->enable_pipeline_share_limit &&\\n  89: if (limit_with_no_agg && _aggregator->params()->enable_pipeline_share_limit) {\\n  105: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/nljoin/spillable_nljoin_build_operator.h\\n  17: #include \\\"exec/pipeline/nljoin/nljoin_build_operator.h\\\"\\n  18: #include \\\"exec/pipeline/spill_process_channel.h\\\"\\n  22: namespace starrocks::pipeline {\\n  67: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/operator_with_dependency.h\\n  19: #include \\\"exec/pipeline/operator.h\\\"\\n  20: #include \\\"exec/pipeline/pipeline_fwd.h\\\"\\n  22: namespace starrocks::pipeline {\\n  29: // of a pipeline, The left operator(HashJoinProbeOperator) can work in streaming-style, so it can appear in the middle\\n  30: // of another pipeline. Denotes pipeline contains left operator and right operator as left pipeline and right pipeline\\n  31: // respectively, so left pipeline always has a data dependency on right pipeline, left pipeline can be scheduled to\\n  32: // execute on core until right pipeline has finished. For a left pipeline, it shall get stuck in left operator.\\n  33: // OperatorWithDependency is introduced so that left operators should inherit from it. When a left pipeline get stuck\\n  34: // in left operators, it should be guarded by PipelineDriverPoller thread that will inspect std::vector<OperatorWithDependency*>\\n  35: // PipelineDriver::_dependencies to ensure that left pipeline is put back into multi-level feedback queue after all\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/exchange/exchange_merge_sort_source_operator.h\\n  19: #include \\\"exec/pipeline/source_operator.h\\\"\\n  25: namespace pipeline {\\n  113: } // namespace pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/fetch_task.cpp\\n  15: #include \\\"exec/pipeline/fetch_task.h\\\"\\n  21: #include \\\"exec/pipeline/fetch_processor.h\\\"\\n  22: #include \\\"exec/pipeline/lookup_request.h\\\"\\n  31: namespace starrocks::pipeline {\\n  186: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/schedule/event_scheduler.cpp\\n  15: #include \\\"exec/pipeline/schedule/event_scheduler.h\\\"\\n  17: #include \\\"exec/pipeline/pipeline_driver.h\\\"\\n  18: #include \\\"exec/pipeline/pipeline_driver_queue.h\\\"\\n  19: #include \\\"exec/pipeline/pipeline_fwd.h\\\"\\n  20: #include \\\"exec/pipeline/schedule/common.h\\\"\\n  21: #include \\\"exec/pipeline/schedule/utils.h\\\"\\n  23: namespace starrocks::pipeline {\\n  45: // The logic in the pipeline poller is basically the same.\\n  72: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/aggregate/spillable_aggregate_blocking_sink_operator.cpp\\n  15: #include \\\"exec/pipeline/aggregate/spillable_aggregate_blocking_sink_operator.h\\\"\\n  22: #include \\\"exec/pipeline/aggregate/aggregate_blocking_sink_operator.h\\\"\\n  23: #include \\\"exec/pipeline/query_context.h\\\"\\n  36: namespace starrocks::pipeline {\\n  121: _aggregator->params()->enable_pipeline_share_limit = false;\\n  361: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/aggregate/aggregate_distinct_streaming_sink_operator.h\\n  21: #include \\\"exec/pipeline/operator.h\\\"\\n  23: namespace starrocks::pipeline {\\n  98: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/nljoin/nljoin_build_operator.cpp\\n  15: #include \\\"exec/pipeline/nljoin/nljoin_build_operator.h\\\"\\n  18: #include \\\"exec/pipeline/operator.h\\\"\\n  22: namespace starrocks::pipeline {\\n  65: Operator::OutputAmplificationType NLJoinBuildOperator::intra_pipeline_amplification_type() const {\\n  69: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/bucket_process_operator.cpp\\n  15: #include \\\"exec/pipeline/bucket_process_operator.h\\\"\\n  19: #include \\\"exec/pipeline/aggregate/spillable_aggregate_blocking_sink_operator.h\\\"\\n  20: #include \\\"exec/pipeline/aggregate/spillable_aggregate_distinct_blocking_operator.h\\\"\\n  21: #include \\\"exec/pipeline/operator.h\\\"\\n  22: #include \\\"exec/pipeline/pipeline_fwd.h\\\"\\n  23: #include \\\"exec/pipeline/spill_process_channel.h\\\"\\n  28: namespace starrocks::pipeline {\\n  216: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/exchange/multi_cast_local_exchange_source_operator.cpp\\n  15: #include \\\"exec/pipeline/exchange/multi_cast_local_exchange_source_operator.h\\\"\\n  17: namespace starrocks::pipeline {\\n  49: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/aggregate/repeat/repeat_operator.cpp\\n  15: #include \\\"exec/pipeline/aggregate/repeat/repeat_operator.h\\\"\\n  21: namespace starrocks::pipeline {\\n  90: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/capture_version_operator.h\\n  17: #include \\\"exec/pipeline/source_operator.h\\\"\\n  20: namespace starrocks::pipeline {\\n  60: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/table_function_operator.cpp\\n  17: namespace starrocks::pipeline {\\n  141: // Just return the chunk whether it's full or not in order to keep the semantics of pipeline\\n  258: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/hashjoin/spillable_hash_join_probe_operator.h\\n  26: #include \\\"exec/pipeline/hashjoin/hash_join_probe_operator.h\\\"\\n  33: namespace starrocks::pipeline {\\n  168: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/exchange/sink_buffer.h\\n  25: #include \\\"exec/pipeline/fragment_context.h\\\"\\n  34: namespace starrocks::pipeline {\\n  98: void attach_observer(RuntimeState* state, PipelineObserver* observer) { _observable.add_observer(state, observer); }\\n  231: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/dict_decode_operator.cpp\\n  15: #include \\\"exec/pipeline/dict_decode_operator.h\\\"\\n  21: namespace starrocks::pipeline {\\n  150: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/exchange/exchange_parallel_merge_source_operator.h\\n  17: #include \\\"exec/pipeline/source_operator.h\\\"\\n  22: namespace pipeline {\\n  50: // can be easily integrated into the pipeline engine. The only thing for this operator need to to is to call\\n  127: } // namespace pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/aggregate/spillable_aggregate_distinct_blocking_operator.h\\n  20: #include \\\"exec/pipeline/aggregate/aggregate_distinct_blocking_sink_operator.h\\\"\\n  21: #include \\\"exec/pipeline/aggregate/aggregate_distinct_blocking_source_operator.h\\\"\\n  22: #include \\\"exec/pipeline/operator.h\\\"\\n  23: #include \\\"exec/pipeline/source_operator.h\\\"\\n  27: namespace starrocks::pipeline {\\n  133: ChunkPipelineAccumulator _accumulator;\\n  158: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/aggregate/sorted_aggregate_streaming_sink_operator.h\\n  20: #include \\\"exec/pipeline/operator.h\\\"\\n  23: namespace starrocks::pipeline {\\n  45: ChunkPipelineAccumulator _accumulator;\\n  69: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/exchange/local_exchange.cpp\\n  15: #include \\\"exec/pipeline/exchange/local_exchange.h\\\"\\n  22: #include \\\"exec/pipeline/exchange/shuffler.h\\\"\\n  27: namespace starrocks::pipeline {\\n  507: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/schedule/timeout_tasks.h\\n  17: #include \\\"exec/pipeline/schedule/observer.h\\\"\\n  18: #include \\\"exec/pipeline/schedule/pipeline_timer.h\\\"\\n  21: namespace starrocks::pipeline {\\n  25: class CheckFragmentTimeout final : public PipelineTimerTask {\\n  35: class RFScanWaitTimeout final : public PipelineTimerTask {\\n  38: void add_observer(RuntimeState* state, PipelineObserver* observer) { _timeout.add_observer(state, observer); }\\n  46: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/sort/local_partition_topn_source.h\\n  17: #include \\\"exec/pipeline/sort/local_partition_topn_context.h\\\"\\n  18: #include \\\"exec/pipeline/source_operator.h\\\"\\n  20: namespace starrocks::pipeline {\\n  63: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/select_operator.cpp\\n  15: #include \\\"exec/pipeline/select_operator.h\\\"\\n  21: namespace starrocks::pipeline {\\n  132: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/nljoin/nljoin_context.h\\n  25: #include \\\"exec/pipeline/context_with_dependency.h\\\"\\n  26: #include \\\"exec/pipeline/spill_process_channel.h\\\"\\n  40: namespace starrocks::pipeline {\\n  187: void attach_probe_observer(RuntimeState* state, pipeline::PipelineObserver* observer) {\\n  190: void attach_build_observer(RuntimeState* state, pipeline::PipelineObserver* observer) {\\n  231: pipeline::Observable _builder_observable;\\n  232: pipeline::Observable _probe_observable;\\n  235: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/aggregate/aggregate_blocking_sink_operator.h\\n  22: #include \\\"exec/pipeline/operator.h\\\"\\n  26: namespace starrocks::pipeline {\\n  120: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/exchange/multi_cast_local_exchange.h\\n  21: #include \\\"exec/pipeline/schedule/observer.h\\\"\\n  22: #include \\\"exec/pipeline/source_operator.h\\\"\\n  27: namespace starrocks::pipeline {\\n  30: // at the end of the pipeline. It works like\\n  31: // [source pipeline]  -> exchange_sink_operator\\n  33: // But for multi cast stream sink, extra pipelines are added with exchanger\\n  35: // [source pipeline] -> mcast_local_sink   -> mcast_local_exchanger  -> [new pipeline1]\\n  36: //                                                     |             -> [new pipeline2]\\n  37: //                                                     |             -> [new pipeline3]\\n  38: // and for each new pipeline, the internal structure is\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/aggregate/sorted_aggregate_streaming_source_operator.cpp\\n  15: #include \\\"exec/pipeline/aggregate/sorted_aggregate_streaming_source_operator.h\\\"\\n  17: #include \\\"exec/pipeline/sort/sort_context.h\\\"\\n  20: namespace starrocks::pipeline {\\n  93: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/exchange/exchange_source_operator.cpp\\n  15: #include \\\"exec/pipeline/exchange/exchange_source_operator.h\\\"\\n  24: namespace starrocks::pipeline {\\n  41: return _stream_recvr->has_output_for_pipeline(_driver_sequence);\\n  50: _stream_recvr->short_circuit_for_pipeline(_driver_sequence);\\n  57: RETURN_IF_ERROR(_stream_recvr->get_chunk_for_pipeline(&chunk, _driver_sequence));\\n  103: if (!_enable_pipeline_level_shuffle) {\\n  108: // then pipeline level shuffle will be performed at sender side (ExchangeSinkOperator), so\\n  124: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/exchange/sink_buffer.cpp\\n  15: #include \\\"exec/pipeline/exchange/sink_buffer.h\\\"\\n  31: namespace starrocks::pipeline {\\n  70: // In some extreme cases, the pipeline driver has not been created yet, and the query is over\\n  125: size_t max_buffer_size = config::pipeline_sink_buffer_size * _sink_ctxs.size();\\n  302: too_much_brpc_process = discontinuous_acked_window_size >= config::pipeline_sink_brpc_dop;\\n  304: too_much_brpc_process = context.num_in_flight_rpcs >= config::pipeline_sink_brpc_dop;\\n  500: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/fetch_processor.h\\n  25: #include \\\"exec/pipeline/fetch_sink_operator.h\\\"\\n  26: #include \\\"exec/pipeline/fetch_task.h\\\"\\n  27: #include \\\"exec/pipeline/lookup_request.h\\\"\\n  34: namespace starrocks::pipeline {\\n  38: // The pipeline pushes chunks that contain row-position metadata produced by a lookup operator.\\n  49: //   submitted and dequeued when their reconstructed chunks are pulled by the pipeline driver.\\n  182: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/lookup_request.cpp\\n  15: #include \\\"exec/pipeline/lookup_request.h\\\"\\n  22: #include \\\"exec/pipeline/lookup_operator.h\\\"\\n  23: #include \\\"exec/pipeline/query_context.h\\\"\\n  24: #include \\\"exec/pipeline/scan/glm_manager.h\\\"\\n  33: namespace starrocks::pipeline {\\n  540: auto glm_ctx = down_cast<pipeline::IcebergGlobalLateMaterilizationContext*>(\\n  644: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/assert_num_rows_operator.cpp\\n  15: #include \\\"exec/pipeline/assert_num_rows_operator.h\\\"\\n  22: namespace starrocks::pipeline {\\n  96: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/nljoin/nljoin_build_operator.h\\n  20: #include \\\"exec/pipeline/nljoin/nljoin_context.h\\\"\\n  21: #include \\\"exec/pipeline/operator.h\\\"\\n  23: namespace starrocks::pipeline {\\n  50: OutputAmplificationType intra_pipeline_amplification_type() const override;\\n  77: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/exchange/multi_cast_local_exchange_sink_operator.cpp\\n  15: #include \\\"exec/pipeline/exchange/multi_cast_local_exchange_sink_operator.h\\\"\\n  17: namespace starrocks::pipeline {\\n  58: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/chunk_accumulate_operator.cpp\\n  15: #include \\\"exec/pipeline/chunk_accumulate_operator.h\\\"\\n  19: namespace starrocks::pipeline {\\n  54: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/audit_statistics_reporter.h\\n  24: namespace starrocks::pipeline {\\n  37: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/exchange/spillable_multi_cast_local_exchange.cpp\\n  21: #include \\\"exec/pipeline/exchange/mem_limited_chunk_queue.h\\\"\\n  22: #include \\\"exec/pipeline/exchange/multi_cast_local_exchange.h\\\"\\n  23: #include \\\"exec/pipeline/exchange/multi_cast_local_exchange_sink_operator.h\\\"\\n  38: namespace starrocks::pipeline {\\n  104: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/aggregate/sorted_aggregate_streaming_sink_operator.cpp\\n  15: #include \\\"exec/pipeline/aggregate/sorted_aggregate_streaming_sink_operator.h\\\"\\n  21: namespace starrocks::pipeline {\\n  105: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/exchange/shuffler.h\\n  23: namespace starrocks::pipeline {\\n  91: // while `exchange source -> local exchange` (phase 2) determines which pipeline driver to deliver each row.\\n  109: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/exchange/local_exchange.h\\n  22: #include \\\"exec/pipeline/exchange/local_exchange_source_operator.h\\\"\\n  23: #include \\\"exec/pipeline/exchange/shuffler.h\\\"\\n  31: namespace pipeline {\\n  175: void attach_sink_observer(RuntimeState* state, pipeline::PipelineObserver* observer) {\\n  368: } // namespace pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/exchange/local_exchange_source_operator.h\\n  22: #include \\\"exec/pipeline/source_operator.h\\\"\\n  24: namespace starrocks::pipeline {\\n  175: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/aggregate/spillable_aggregate_blocking_sink_operator.h\\n  20: #include \\\"exec/pipeline/operator.h\\\"\\n  21: #include \\\"exec/pipeline/spill_process_channel.h\\\"\\n  26: namespace starrocks::pipeline {\\n  127: } // namespace starrocks::pipeline\\n\\nFile: /Users/kks/git/starrocks/be/src/exec/pipeline/aggregate/spillable_aggregate_blocking_source_operator.cpp\\n  15: #include \\\"exec/pipeline/aggregate/spillable_aggregate_blocking_source_operator.h\\\"\\n  20: #include \\\"exec/pipeline/aggregate/aggregate_blocking_source_operator.h\\\"\\n  22: namespace starrocks::pipeline {\\n  170: } // namespace starrocks::pipeline\"\n}"
            }
          }
        ],
        "providerOptions": null
      },
      {
        "role": "assistant",
        "content": [
          {
            "type": "tool-call",
            "toolCallId": "call_MHw1R0paaXJVMzdmQ29BdDBna1E",
            "toolName": "listFiles",
            "input": {
              "directory_path": "/Users/kks/git/starrocks/be/src/exec/pipeline/"
            }
          }
        ],
        "providerOptions": null
      },
      {
        "role": "tool",
        "content": [
          {
            "type": "tool-result",
            "toolCallId": "call_MHw1R0paaXJVMzdmQ29BdDBna1E",
            "toolName": "listFiles",
            "output": {
              "type": "text",
              "value": "dirs: adaptive; aggregate; analysis; exchange; group_execution; hashjoin; nljoin; scan; schedule; set; sink; sort; README.md; assert_num_rows_operator.cpp; assert_num_rows_operator.h; audit_statistics_reporter.cpp; audit_statistics_reporter.h; bucket_process_operator.cpp; bucket_process_operator.h; capture_version_operator.cpp; capture_version_operator.h; chunk_accumulate_operator.cpp; chunk_accumulate_operator.h; context_with_dependency.h; dict_decode_operator.cpp; dict_decode_operator.h; driver_limiter.cpp; driver_limiter.h; driver_source.h; empty_set_operator.h; exec_state_reporter.cpp; exec_state_reporter.h; fetch_processor.cpp; fetch_processor.h; fetch_sink_operator.cpp; fetch_sink_operator.h; fetch_source_operator.cpp; fetch_source_operator.h; fetch_task.cpp; fetch_task.h; fragment_context.cpp; fragment_context.h; fragment_executor.cpp; fragment_executor.h; hash_partition_context.cpp; hash_partition_context.h; hash_partition_sink_operator.cpp; hash_partition_sink_operator.h; hash_partition_source_operator.cpp; hash_partition_source_operator.h; limit_operator.cpp; limit_operator.h; lookup_operator.cpp; lookup_operator.h; lookup_request.cpp; lookup_request.h; noop_sink_operator.h; offset_operator.cpp; offset_operator.h; operator.cpp; operator.h; operator_with_dependency.h; pipeline.cpp; pipeline.h; pipeline_builder.cpp; pipeline_builder.h; pipeline_driver.cpp; pipeline_driver.h; pipeline_driver_executor.cpp; pipeline_driver_executor.h; pipeline_driver_poller.cpp; pipeline_driver_poller.h; pipeline_driver_queue.cpp; pipeline_driver_queue.h; pipeline_fwd.h; pipeline_metrics.cpp; pipeline_metrics.h; project_operator.cpp; project_operator.h; query_context.cpp; query_context.h; runtime_filter_types.cpp; runtime_filter_types.h; select_operator.cpp; select_operator.h; source_operator.cpp; source_operator.h; spill_process_channel.cpp; spill_process_channel.h; spill_process_operator.cpp; spill_process_operator.h; stream_epoch_manager.cpp; stream_epoch_manager.h; stream_pipeline_driver.cpp; stream_pipeline_driver.h; table_function_operator.cpp; table_function_operator.h; topn_runtime_filter_back_pressure.h; wait_operator.cpp; wait_operator.h\n\nadaptive dirs: adaptive_dop_param.h; adaptive_fwd.h; collect_stats_context.cpp; collect_stats_context.h; collect_stats_sink_operator.cpp; collect_stats_sink_operator.h; collect_stats_source_operator.cpp; collect_stats_source_operator.h; event.cpp; event.h; utils.cpp; utils.h\n\naggregate dirs: repeat; aggregate_blocking_sink_operator.cpp; aggregate_blocking_sink_operator.h; aggregate_blocking_source_operator.cpp; aggregate_blocking_source_operator.h; aggregate_distinct_blocking_sink_operator.cpp; aggregate_distinct_blocking_sink_operator.h; aggregate_distinct_blocking_source_operator.cpp; aggregate_distinct_blocking_source_operator.h; aggregate_distinct_streaming_sink_operator.cpp; aggregate_distinct_streaming_sink_operator.h; aggregate_distinct_streaming_source_operator.cpp; aggregate_distinct_streaming_source_operator.h; aggregate_operators.cpp; aggregate_streaming_sink_operator.cpp; aggregate_streaming_sink_operator.h; aggregate_streaming_source_operator.cpp; aggregate_streaming_source_operator.h; sorted_aggregate_streaming_sink_operator.cpp; sorted_aggregate_streaming_sink_operator.h; sorted_aggregate_streaming_source_operator.cpp; sorted_aggregate_streaming_source_operator.h; spillable_aggregate_blocking_sink_operator.cpp; spillable_aggregate_blocking_sink_operator.h; spillable_aggregate_blocking_source_operator.cpp; spillable_aggregate_blocking_source_operator.h; spillable_aggregate_distinct_blocking_operator.cpp; spillable_aggregate_distinct_blocking_operator.h; spillable_partitionwise_aggregate_operator.cpp; spillable_partitionwise_aggregate_operator.h; spillable_partitionwise_distinct_operator.cpp; spillable_partitionwise_distinct_operator.h\n\naggregate/repeat dirs: repeat_operator.cpp; repeat_operator.h\n\nanalysis dirs: analytic_sink_operator.cpp; analytic_sink_operator.h; analytic_source_operator.cpp; analytic_source_operator.h\n\nexchange dirs: exchange_merge_sort_source_operator.cpp; exchange_merge_sort_source_operator.h; exchange_parallel_merge_source_operator.cpp; exchange_parallel_merge_source_operator.h; exchange_sink_operator.cpp; exchange_sink_operator.h; exchange_source_operator.cpp; exchange_source_operator.h; local_exchange.cpp; local_exchange.h; local_exchange_sink_operator.cpp; local_exchange_sink_operator.h; local_exchange_source_operator.cpp; local_exchange_source_operator.h; mem_limited_chunk_queue.cpp; mem_limited_chunk_queue.h; multi_cast_local_exchange.cpp; multi_cast_local_exchange.h; multi_cast_local_exchange_sink_operator.cpp; multi_cast_local_exchange_sink_operator.h; multi_cast_local_exchange_source_operator.cpp; multi_cast_local_exchange_source_operator.h; shuffler.h; sink_buffer.cpp; sink_buffer.h; spillable_multi_cast_local_exchange.cpp; split_local_exchange.cpp; split_local_exchange.h\n\ngroup_execution dirs: execution_group.cpp; execution_group.h; execution_group_builder.cpp; execution_group_builder.h; execution_group_fwd.h; group_operator.cpp; group_operator.h\n\nhashjoin dirs: hash_join_build_operator.cpp; hash_join_build_operator.h; hash_join_probe_operator.cpp; hash_join_probe_operator.h; hash_joiner_factory.cpp; hash_joiner_factory.h; hash_joiner_fwd.h; spillable_hash_join_build_operator.cpp; spillable_hash_join_build_operator.h; spillable_hash_join_probe_operator.cpp; spillable_hash_join_probe_operator.h\n\nnljoin dirs: nljoin_build_operator.cpp; nljoin_build_operator.h; nljoin_context.cpp; nljoin_context.h; nljoin_probe_operator.cpp; nljoin_probe_operator.h; spillable_nljoin_build_operator.cpp; spillable_nljoin_build_operator.h; spillable_nljoin_probe_operator.cpp; spillable_nljoin_probe_operator.h\n\nscan dirs: balanced_chunk_buffer.cpp; balanced_chunk_buffer.h; chunk_buffer_limiter.cpp; chunk_buffer_limiter.h; chunk_source.cpp; chunk_source.h; connector_scan_operator.cpp; connector_scan_operator.h; glm_manager.cpp; glm_manager.h; lake_meta_scan_prepare_operator.cpp; lake_meta_scan_prepare_operator.h; meta_chunk_source.cpp; meta_chunk_source.h; meta_scan_context.h; meta_scan_operator.cpp; meta_scan_operator.h; meta_scan_prepare_operator.cpp; meta_scan_prepare_operator.h; morsel.cpp; morsel.h; olap_chunk_source.cpp; olap_chunk_source.h; olap_meta_scan_context.h; olap_meta_scan_prepare_operator.cpp; olap_meta_scan_prepare_operator.h; olap_scan_context.cpp; olap_scan_context.h; olap_scan_operator.cpp; olap_scan_operator.h; olap_scan_prepare_operator.cpp; olap_scan_prepare_operator.h; scan_operator.cpp; scan_operator.h; schema_chunk_source.cpp; schema_chunk_source.h; schema_scan_context.cpp; schema_scan_context.h; schema_scan_operator.cpp; schema_scan_operator.h\n\nschedule dirs: common.h; event_scheduler.cpp; event_scheduler.h; observer.cpp; observer.h; pipeline_timer.cpp; pipeline_timer.h; timeout_tasks.cpp; timeout_tasks.h; utils.h\n\nset dirs: except_build_sink_operator.cpp; except_build_sink_operator.h; except_context.cpp; except_context.h; except_output_source_operator.cpp; except_output_source_operator.h; except_probe_sink_operator.cpp; except_probe_sink_operator.h; intersect_build_sink_operator.cpp; intersect_build_sink_operator.h; intersect_context.cpp; intersect_context.h; intersect_output_source_operator.cpp; intersect_output_source_operator.h; intersect_probe_sink_operator.cpp; intersect_probe_sink_operator.h; raw_values_source_operator.cpp; raw_values_source_operator.h; union_const_source_operator.cpp; union_const_source_operator.h; union_passthrough_operator.cpp; union_passthrough_operator.h\n\nsink dirs: blackhole_table_sink_operator.cpp; blackhole_table_sink_operator.h; connector_sink_operator.cpp; connector_sink_operator.h; dictionary_cache_sink_operator.cpp; dictionary_cache_sink_operator.h; export_sink_operator.cpp; export_sink_operator.h; file_sink_operator.cpp; file_sink_operator.h; memory_scratch_sink_operator.cpp; memory_scratch_sink_operator.h; mysql_table_sink_operator.cpp; mysql_table_sink_operator.h; olap_table_sink_operator.cpp; olap_table_sink_operator.h; result_sink_operator.cpp; result_sink_operator.h; sink_io_buffer.cpp; sink_io_buffer.h; table_function_table_sink_operator.cpp; table_function_table_sink_operator.h\n\nsort dirs: local_merge_sort_source_operator.cpp; local_merge_sort_source_operator.h; local_parallel_merge_sort_source_operator.cpp; local_parallel_merge_sort_source_operator.h; local_partition_topn_context.cpp; local_partition_topn_context.h; local_partition_topn_sink.cpp; local_partition_topn_sink.h; local_partition_topn_source.cpp; local_partition_topn_source.h; partition_sort_sink_operator.cpp; partition_sort_sink_operator.h; sort_context.cpp; sort_context.h; spillable_partition_sort_sink_operator.cpp; spillable_partition_sort_sink_operator.h"
            }
          }
        ],
        "providerOptions": null
      },
      {
        "role": "assistant",
        "content": [
          {
            "type": "tool-call",
            "toolCallId": "call_MHxkd1BEa3ppVTEzdWF5TnpORVk",
            "toolName": "readFile",
            "input": {
              "file_path": "/Users/kks/git/starrocks/be/src/exec/pipeline/fragment_context.h"
            }
          }
        ],
        "providerOptions": null
      },
      {
        "role": "tool",
        "content": [
          {
            "type": "tool-result",
            "toolCallId": "call_MHxkd1BEa3ppVTEzdWF5TnpORVk",
            "toolName": "readFile",
            "output": {
              "type": "text",
              "value": "{\n  \"success\": true,\n  \"file_path\": \"/Users/kks/git/starrocks/be/src/exec/pipeline/fragment_context.h\",\n  \"content\": \"// Copyright 2021-present StarRocks, Inc. All rights reserved.\\n//\\n// Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\\n// you may not use this file except in compliance with the License.\\n// You may obtain a copy of the License at\\n//\\n//     https://www.apache.org/licenses/LICENSE-2.0\\n//\\n// Unless required by applicable law or agreed to in writing, software\\n// distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n// See the License for the specific language governing permissions and\\n// limitations under the License.\\n\\n#pragma once\\n\\n#include <memory>\\n#include <unordered_map>\\n\\n#include \\\"exec/exec_node.h\\\"\\n#include \\\"exec/pipeline/adaptive/adaptive_dop_param.h\\\"\\n#include \\\"exec/pipeline/driver_limiter.h\\\"\\n#include \\\"exec/pipeline/group_execution/execution_group_fwd.h\\\"\\n#include \\\"exec/pipeline/pipeline.h\\\"\\n#include \\\"exec/pipeline/pipeline_driver.h\\\"\\n#include \\\"exec/pipeline/pipeline_fwd.h\\\"\\n#include \\\"exec/pipeline/runtime_filter_types.h\\\"\\n#include \\\"exec/pipeline/scan/morsel.h\\\"\\n#include \\\"exec/pipeline/schedule/event_scheduler.h\\\"\\n#include \\\"exec/pipeline/schedule/observer.h\\\"\\n#include \\\"exec/pipeline/schedule/pipeline_timer.h\\\"\\n#include \\\"exec/query_cache/cache_param.h\\\"\\n#include \\\"gen_cpp/FrontendService.h\\\"\\n#include \\\"gen_cpp/HeartbeatService.h\\\"\\n#include \\\"gen_cpp/InternalService_types.h\\\"\\n#include \\\"gen_cpp/PlanNodes_types.h\\\"\\n#include \\\"gen_cpp/QueryPlanExtra_types.h\\\"\\n#include \\\"gen_cpp/Types_types.h\\\"\\n#include \\\"runtime/profile_report_worker.h\\\"\\n#include \\\"runtime/runtime_filter_worker.h\\\"\\n#include \\\"runtime/runtime_state.h\\\"\\n#include \\\"storage/predicate_tree_params.h\\\"\\n#include \\\"util/hash_util.hpp\\\"\\n\\nnamespace starrocks {\\n\\nclass StreamLoadContext;\\n\\nnamespace pipeline {\\n\\nusing RuntimeFilterPort = starrocks::RuntimeFilterPort;\\nusing PerDriverScanRangesMap = std::map<int32_t, std::vector<TScanRangeParams>>;\\n\\nclass FragmentContext {\\n    friend FragmentContextManager;\\n\\npublic:\\n    FragmentContext();\\n    ~FragmentContext();\\n    const TUniqueId& query_id() const { return _query_id; }\\n    void set_query_id(const TUniqueId& query_id) { _query_id = query_id; }\\n    const TUniqueId& fragment_instance_id() const { return _fragment_instance_id; }\\n    void set_fragment_instance_id(const TUniqueId& fragment_instance_id) {\\n        _fragment_instance_id = fragment_instance_id;\\n    }\\n    void set_fe_addr(const TNetworkAddress& fe_addr) { _fe_addr = fe_addr; }\\n    const TNetworkAddress& fe_addr() { return _fe_addr; }\\n    FragmentFuture finish_future() { return _finish_promise.get_future(); }\\n    RuntimeState* runtime_state() const { return _runtime_state.get(); }\\n    std::shared_ptr<RuntimeState> runtime_state_ptr() { return _runtime_state; }\\n    void set_runtime_state(std::shared_ptr<RuntimeState>&& runtime_state) { _runtime_state = std::move(runtime_state); }\\n    ExecNode*& plan() { return _plan; }\\n\\n    void move_tplan(TPlan& tplan);\\n    const TPlan& tplan() const { return _tplan; }\\n    void set_data_sink(std::unique_ptr<DataSink> data_sink);\\n\\n    size_t total_dop() const;\\n\\n    bool all_execution_groups_finished() const { return _num_finished_execution_groups == _execution_groups.size(); }\\n    void count_down_execution_group(size_t val = 1);\\n\\n    bool need_report_exec_state();\\n    void report_exec_state_if_necessary();\\n\\n    void set_final_status(const Status& status);\\n\\n    Status final_status() const {\\n        auto* status = _final_status.load();\\n        return status == nullptr ? Status::OK() : *status;\\n    }\\n\\n    void cancel(const Status& status, bool cancelled_by_fe = false);\\n\\n    void finish() { cancel(Status::OK()); }\\n\\n    bool is_canceled() const { return _runtime_state->is_cancelled(); }\\n\\n    MorselQueueFactoryMap& morsel_queue_factories() { return _morsel_queue_factories; }\\n\\n    void set_pipelines(ExecutionGroups&& exec_groups, Pipelines&& pipelines);\\n\\n    Status prepare_all_pipelines();\\n\\n    template <class Func>\\n    void iterate_drivers(Func&& call) {\\n        iterate_pipeline([&](const Pipeline* pipeline) {\\n            for (auto& driver : pipeline->drivers()) {\\n                call(driver);\\n            }\\n        });\\n    }\\n\\n    void clear_all_drivers();\\n    void close_all_execution_groups();\\n\\n    RuntimeFilterHub* runtime_filter_hub() { return &_runtime_filter_hub; }\\n\\n    RuntimeFilterPort* runtime_filter_port() { return _runtime_state->runtime_filter_port(); }\\n\\n    void prepare_pass_through_chunk_buffer();\\n    void destroy_pass_through_chunk_buffer();\\n\\n    void set_driver_token(DriverLimiter::TokenPtr driver_token) { _driver_token = std::move(driver_token); }\\n    Status set_pipeline_timer(PipelineTimer* pipeline_timer);\\n    void clear_pipeline_timer();\\n\\n    query_cache::CacheParam& cache_param() { return _cache_param; }\\n\\n    void set_enable_cache(bool flag) { _enable_cache = flag; }\\n\\n    bool enable_cache() const { return _enable_cache; }\\n\\n    void set_stream_load_contexts(const std::vector<StreamLoadContext*>& contexts);\\n\\n    void set_enable_adaptive_dop(bool val) { _enable_adaptive_dop = val; }\\n    bool enable_adaptive_dop() const { return _enable_adaptive_dop; }\\n    AdaptiveDopParam& adaptive_dop_param() { return _adaptive_dop_param; }\\n\\n    const PredicateTreeParams& pred_tree_params() const { return _pred_tree_params; }\\n    void set_pred_tree_params(PredicateTreeParams&& params) { _pred_tree_params = std::move(params); }\\n\\n    size_t next_driver_id() { return _next_driver_id++; }\\n\\n    void set_workgroup(workgroup::WorkGroupPtr wg) { _workgroup = std::move(wg); }\\n    const workgroup::WorkGroupPtr& workgroup() const { return _workgroup; }\\n    bool enable_resource_group() const { return _workgroup != nullptr; }\\n    TQueryType::type query_type() const;\\n\\n    // STREAM MV\\n    Status reset_epoch();\\n    void set_is_stream_pipeline(bool is_stream_pipeline) { _is_stream_pipeline = is_stream_pipeline; }\\n    bool is_stream_pipeline() const { return _is_stream_pipeline; }\\n    void count_down_epoch_pipeline(RuntimeState* state, size_t val = 1);\\n\\n#ifdef BE_TEST\\n    // for ut\\n    void set_is_stream_test(bool is_stream_test) { _is_stream_test = is_stream_test; }\\n    bool is_stream_test() const { return _is_stream_test; }\\n#endif\\n\\n    size_t expired_log_count() { return _expired_log_count; }\\n\\n    void set_expired_log_count(size_t val) { _expired_log_count = val; }\\n\\n    void init_jit_profile();\\n\\n    void update_jit_profile(int64_t time_ns);\\n\\n    void iterate_pipeline(const std::function<void(Pipeline*)>& call);\\n    Status iterate_pipeline(const std::function<Status(Pipeline*)>& call);\\n\\n    Status prepare_active_drivers();\\n    Status submit_active_drivers(DriverExecutor* executor);\\n\\n    bool enable_group_execution() const { return _enable_group_execution; }\\n    void set_enable_group_execution(bool enable_group_execution) { _enable_group_execution = enable_group_execution; }\\n\\n    void set_report_when_finish(bool report) { _report_when_finish = report; }\\n\\n    // acquire runtime filter from cache\\n    void acquire_runtime_filters();\\n\\n    bool enable_event_scheduler() const { return event_scheduler() != nullptr; }\\n    EventScheduler* event_scheduler() const { return _event_scheduler.get(); }\\n    void init_event_scheduler();\\n\\n    PipelineTimer* pipeline_timer() { return _pipeline_timer; }\\n    void add_timer_observer(PipelineObserver* observer, uint64_t timeout);\\n    Status submit_all_timer();\\n\\nprivate:\\n    void _close_stream_load_contexts();\\n\\n    bool _enable_group_execution = false;\\n    // Id of this query\\n    TUniqueId _query_id;\\n    // Id of this instance\\n    TUniqueId _fragment_instance_id;\\n    TNetworkAddress _fe_addr;\\n\\n    // Hold tplan data datasink from delivery request to create driver lazily\\n    // after delivery request has been finished.\\n    TPlan _tplan;\\n    std::unique_ptr<DataSink> _data_sink;\\n\\n    // promise used to determine whether fragment finished its execution\\n    FragmentPromise _finish_promise;\\n\\n    // never adjust the order of _runtime_state, _plan, _pipelines and _drivers, since\\n    // _plan depends on _runtime_state and _drivers depends on _runtime_state.\\n    std::shared_ptr<RuntimeState> _runtime_state = nullptr;\\n    ExecNode* _plan = nullptr; // lives in _runtime_state->obj_pool()\\n    size_t _next_driver_id = 0;\\n    Pipelines _pipelines;\\n    ExecutionGroups _execution_groups;\\n    std::atomic<size_t> _num_finished_execution_groups = 0;\\n\\n    std::unique_ptr<EventScheduler> _event_scheduler;\\n    PipelineTimer* _pipeline_timer = nullptr;\\n    PipelineTimerTask* _timeout_task = nullptr;\\n    PipelineTimerTask* _report_state_task = nullptr;\\n    std::unordered_map<uint64_t, PipelineTimerTask*> _rf_timeout_tasks;\\n\\n    RuntimeFilterHub _runtime_filter_hub;\\n\\n    MorselQueueFactoryMap _morsel_queue_factories;\\n    workgroup::WorkGroupPtr _workgroup = nullptr;\\n\\n    std::atomic<Status*> _final_status = nullptr;\\n    Status _s_status;\\n\\n    DriverLimiter::TokenPtr _driver_token = nullptr;\\n\\n    query_cache::CacheParam _cache_param;\\n    bool _enable_cache = false;\\n    std::vector<StreamLoadContext*> _stream_load_contexts;\\n\\n    // STREAM MV\\n    std::atomic<size_t> _num_finished_epoch_pipelines = 0;\\n    bool _is_stream_pipeline = false;\\n#ifdef BE_TEST\\n    bool _is_stream_test = false;\\n#endif\\n\\n    bool _enable_adaptive_dop = false;\\n    AdaptiveDopParam _adaptive_dop_param;\\n\\n    PredicateTreeParams _pred_tree_params;\\n\\n    size_t _expired_log_count = 0;\\n\\n    std::atomic<int64_t> _last_report_exec_state_ns = MonotonicNanos();\\n\\n    RuntimeProfile::Counter* _jit_counter = nullptr;\\n    RuntimeProfile::Counter* _jit_timer = nullptr;\\n\\n    bool _report_when_finish{};\\n};\\n\\nclass FragmentContextManager {\\npublic:\\n    FragmentContextManager() = default;\\n    ~FragmentContextManager() = default;\\n\\n    FragmentContextManager(const FragmentContextManager&) = delete;\\n    FragmentContextManager(FragmentContextManager&&) = delete;\\n    FragmentContextManager& operator=(const FragmentContextManager&) = delete;\\n    FragmentContextManager& operator=(FragmentContextManager&&) = delete;\\n\\n    FragmentContext* get_or_register(const TUniqueId& fragment_id);\\n    FragmentContextPtr get(const TUniqueId& fragment_id);\\n\\n    Status register_ctx(const TUniqueId& fragment_id, FragmentContextPtr fragment_ctx);\\n    void unregister(const TUniqueId& fragment_id);\\n\\n    void cancel(const Status& status);\\n\\n    template <class Caller>\\n    void for_each_fragment(Caller&& caller) {\\n        std::lock_guard guard(_lock);\\n        for (auto& [_, fragment] : _fragment_contexts) {\\n            caller(fragment);\\n        }\\n    }\\n\\nprivate:\\n    std::mutex _lock;\\n    std::unordered_map<TUniqueId, FragmentContextPtr> _fragment_contexts;\\n};\\n} // namespace pipeline\\n} // namespace starrocks\\n\",\n  \"message\": \"Successfully read 293 lines from file: /Users/kks/git/starrocks/be/src/exec/pipeline/fragment_context.h\"\n}"
            }
          }
        ],
        "providerOptions": null
      },
      {
        "role": "assistant",
        "content": [
          {
            "type": "tool-call",
            "toolCallId": "call_MHxNc1I3RnVYbWJZclBmVHRyeEM",
            "toolName": "readFile",
            "input": {
              "file_path": "/Users/kks/git/starrocks/be/src/exec/pipeline/pipeline.h"
            }
          }
        ],
        "providerOptions": null
      },
      {
        "role": "tool",
        "content": [
          {
            "type": "tool-result",
            "toolCallId": "call_MHxNc1I3RnVYbWJZclBmVHRyeEM",
            "toolName": "readFile",
            "output": {
              "type": "text",
              "value": "{\n  \"success\": true,\n  \"file_path\": \"/Users/kks/git/starrocks/be/src/exec/pipeline/pipeline.h\",\n  \"content\": \"// Copyright 2021-present StarRocks, Inc. All rights reserved.\\n//\\n// Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\\n// you may not use this file except in compliance with the License.\\n// You may obtain a copy of the License at\\n//\\n//     https://www.apache.org/licenses/LICENSE-2.0\\n//\\n// Unless required by applicable law or agreed to in writing, software\\n// distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n// See the License for the specific language governing permissions and\\n// limitations under the License.\\n\\n#pragma once\\n\\n#include <ctime>\\n#include <utility>\\n\\n#include \\\"exec/pipeline/adaptive/adaptive_fwd.h\\\"\\n#include \\\"exec/pipeline/group_execution/execution_group_fwd.h\\\"\\n#include \\\"exec/pipeline/operator.h\\\"\\n#include \\\"exec/pipeline/pipeline_fwd.h\\\"\\n#include \\\"exec/pipeline/source_operator.h\\\"\\n#include \\\"gutil/strings/substitute.h\\\"\\n\\nnamespace starrocks {\\n\\nclass RuntimeState;\\n\\nnamespace pipeline {\\n\\nclass Pipeline {\\npublic:\\n    Pipeline() = delete;\\n    Pipeline(uint32_t id, OpFactories op_factories, ExecutionGroupRawPtr execution_group);\\n\\n    uint32_t get_id() const { return _id; }\\n\\n    Operators create_operators(int32_t degree_of_parallelism, int32_t i) {\\n        Operators operators;\\n        for (const auto& factory : _op_factories) {\\n            operators.emplace_back(factory->create(degree_of_parallelism, i));\\n        }\\n        return operators;\\n    }\\n    void instantiate_drivers(RuntimeState* state);\\n    Drivers& drivers();\\n    const Drivers& drivers() const;\\n    void count_down_driver(RuntimeState* state);\\n    void clear_drivers();\\n\\n    SourceOperatorFactory* source_operator_factory() {\\n        DCHECK(!_op_factories.empty());\\n        return down_cast<SourceOperatorFactory*>(_op_factories[0].get());\\n    }\\n    const SourceOperatorFactory* source_operator_factory() const {\\n        DCHECK(!_op_factories.empty());\\n        return down_cast<SourceOperatorFactory*>(_op_factories[0].get());\\n    }\\n    OperatorFactory* sink_operator_factory() {\\n        DCHECK(!_op_factories.empty());\\n        return _op_factories[_op_factories.size() - 1].get();\\n    }\\n    size_t degree_of_parallelism() const;\\n\\n    RuntimeProfile* runtime_profile() { return _runtime_profile.get(); }\\n    void setup_pipeline_profile(RuntimeState* runtime_state);\\n    void setup_drivers_profile(const DriverPtr& driver);\\n\\n    Status prepare(RuntimeState* state) {\\n        for (auto& op : _op_factories) {\\n            RETURN_IF_ERROR(op->prepare(state));\\n        }\\n        return Status::OK();\\n    }\\n    void close(RuntimeState* state) {\\n        for (auto& op : _op_factories) {\\n            op->close(state);\\n        }\\n    }\\n\\n    void acquire_runtime_filter(RuntimeState* state) {\\n        for (auto& op : _op_factories) {\\n            op->acquire_runtime_filter(state);\\n        }\\n    }\\n\\n    std::string to_readable_string() const {\\n        std::stringstream ss;\\n        ss << \\\"operator-chain: [\\\";\\n        for (size_t i = 0; i < _op_factories.size(); ++i) {\\n            if (i == 0) {\\n                ss << _op_factories[i]->get_name();\\n            } else {\\n                ss << \\\" -> \\\" << _op_factories[i]->get_name();\\n            }\\n        }\\n        ss << \\\"]\\\";\\n        return ss.str();\\n    }\\n\\n    // STREAM MV\\n    Status reset_epoch(RuntimeState* state);\\n    void count_down_epoch_finished_driver(RuntimeState* state);\\n\\n    size_t output_amplification_factor() const;\\n    Event* pipeline_event() const { return _pipeline_event.get(); }\\n\\nprivate:\\n    uint32_t _id = 0;\\n    std::shared_ptr<RuntimeProfile> _runtime_profile = nullptr;\\n    OpFactories _op_factories;\\n    Drivers _drivers;\\n    std::atomic<size_t> _num_finished_drivers = 0;\\n\\n    EventPtr _pipeline_event;\\n    ExecutionGroupRawPtr _execution_group = nullptr;\\n    // STREAM MV\\n    std::atomic<size_t> _num_epoch_finished_drivers = 0;\\n};\\n\\n} // namespace pipeline\\n} // namespace starrocks\\n\",\n  \"message\": \"Successfully read 125 lines from file: /Users/kks/git/starrocks/be/src/exec/pipeline/pipeline.h\"\n}"
            }
          }
        ],
        "providerOptions": null
      },
      {
        "role": "assistant",
        "content": [
          {
            "type": "tool-call",
            "toolCallId": "call_MHxXb1ZUSk5KcmRuVDBHc3hQWXk",
            "toolName": "readFile",
            "input": {
              "file_path": "/Users/kks/git/starrocks/be/src/exec/pipeline/pipeline_builder.cpp"
            }
          }
        ],
        "providerOptions": null
      },
      {
        "role": "tool",
        "content": [
          {
            "type": "tool-result",
            "toolCallId": "call_MHxXb1ZUSk5KcmRuVDBHc3hQWXk",
            "toolName": "readFile",
            "output": {
              "type": "text",
              "value": "{\n  \"success\": true,\n  \"file_path\": \"/Users/kks/git/starrocks/be/src/exec/pipeline/pipeline_builder.cpp\",\n  \"content\": \"// Copyright 2021-present StarRocks, Inc. All rights reserved.\\n//\\n// Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\\n// you may not use this file except in compliance with the License.\\n// You may obtain a copy of the License at\\n//\\n//     https://www.apache.org/licenses/LICENSE-2.0\\n//\\n// Unless required by applicable law or agreed to in writing, software\\n// distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n// See the License for the specific language governing permissions and\\n// limitations under the License.\\n\\n#include \\\"exec/pipeline/pipeline_builder.h\\\"\\n\\n#include <memory>\\n\\n#include \\\"adaptive/event.h\\\"\\n#include \\\"common/config.h\\\"\\n#include \\\"exec/exec_node.h\\\"\\n#include \\\"exec/pipeline/adaptive/collect_stats_context.h\\\"\\n#include \\\"exec/pipeline/adaptive/collect_stats_sink_operator.h\\\"\\n#include \\\"exec/pipeline/adaptive/collect_stats_source_operator.h\\\"\\n#include \\\"exec/pipeline/chunk_accumulate_operator.h\\\"\\n#include \\\"exec/pipeline/exchange/exchange_source_operator.h\\\"\\n#include \\\"exec/pipeline/exchange/local_exchange_source_operator.h\\\"\\n#include \\\"exec/pipeline/group_execution/execution_group.h\\\"\\n#include \\\"exec/pipeline/group_execution/execution_group_fwd.h\\\"\\n#include \\\"exec/pipeline/group_execution/group_operator.h\\\"\\n#include \\\"exec/pipeline/limit_operator.h\\\"\\n#include \\\"exec/pipeline/noop_sink_operator.h\\\"\\n#include \\\"exec/pipeline/pipeline_fwd.h\\\"\\n#include \\\"exec/pipeline/scan/scan_operator.h\\\"\\n#include \\\"exec/pipeline/spill_process_operator.h\\\"\\n#include \\\"exec/pipeline/wait_operator.h\\\"\\n#include \\\"exec/query_cache/cache_manager.h\\\"\\n#include \\\"exec/query_cache/cache_operator.h\\\"\\n#include \\\"exec/query_cache/conjugate_operator.h\\\"\\n#include \\\"exec/query_cache/lane_arbiter.h\\\"\\n#include \\\"exec/query_cache/multilane_operator.h\\\"\\n\\nnamespace starrocks::pipeline {\\n\\nvoid PipelineBuilderContext::init_colocate_groups(std::unordered_map<int32_t, ExecutionGroupPtr>&& colocate_groups) {\\n    _group_id_to_colocate_groups = std::move(colocate_groups);\\n    for (auto& [group_id, group] : _group_id_to_colocate_groups) {\\n        _execution_groups.emplace_back(group);\\n    }\\n}\\n\\nExecutionGroupRawPtr PipelineBuilderContext::find_exec_group_by_plan_node_id(int32_t plan_node_id) {\\n    for (auto& [group_id, group] : _group_id_to_colocate_groups) {\\n        if (group->contains(plan_node_id)) {\\n            return group.get();\\n        }\\n    }\\n    return _normal_exec_group;\\n}\\n\\n/// PipelineBuilderContext.\\nOpFactories PipelineBuilderContext::maybe_interpolate_local_broadcast_exchange(RuntimeState* state,\\n                                                                               int32_t plan_node_id,\\n                                                                               OpFactories& pred_operators,\\n                                                                               int num_receivers) {\\n    if (num_receivers == 1) {\\n        return maybe_interpolate_local_passthrough_exchange(state, plan_node_id, pred_operators);\\n    }\\n\\n    auto mem_mgr = std::make_shared<ChunkBufferMemoryManager>(num_receivers,\\n                                                              config::local_exchange_buffer_mem_limit_per_driver);\\n    auto local_exchange_source =\\n            std::make_shared<LocalExchangeSourceOperatorFactory>(next_operator_id(), plan_node_id, mem_mgr);\\n    local_exchange_source->set_runtime_state(state);\\n    inherit_upstream_source_properties(local_exchange_source.get(), source_operator(pred_operators));\\n    local_exchange_source->set_could_local_shuffle(true);\\n    local_exchange_source->set_degree_of_parallelism(num_receivers);\\n\\n    auto local_exchange = std::make_shared<BroadcastExchanger>(mem_mgr, local_exchange_source.get());\\n    auto local_exchange_sink =\\n            std::make_shared<LocalExchangeSinkOperatorFactory>(next_operator_id(), plan_node_id, local_exchange);\\n    pred_operators.emplace_back(std::move(local_exchange_sink));\\n    add_pipeline(pred_operators);\\n\\n    return {std::move(local_exchange_source)};\\n}\\n\\nOpFactories PipelineBuilderContext::maybe_interpolate_local_passthrough_exchange(RuntimeState* state,\\n                                                                                 int32_t plan_node_id,\\n                                                                                 OpFactories& pred_operators) {\\n    return maybe_interpolate_local_passthrough_exchange(state, plan_node_id, pred_operators, 1);\\n}\\n\\nOpFactories PipelineBuilderContext::maybe_interpolate_local_passthrough_exchange(RuntimeState* state,\\n                                                                                 int32_t plan_node_id,\\n                                                                                 OpFactories& pred_operators,\\n                                                                                 int num_receivers, bool force) {\\n    return _maybe_interpolate_local_passthrough_exchange(state, plan_node_id, pred_operators, num_receivers, force,\\n                                                         LocalExchanger::PassThroughType::CHUNK);\\n}\\n\\nOpFactories PipelineBuilderContext::maybe_interpolate_local_passthrough_exchange(\\n        RuntimeState* state, int32_t plan_node_id, OpFactories& pred_operators, int num_receivers,\\n        LocalExchanger::PassThroughType pass_through_type) {\\n    return _maybe_interpolate_local_passthrough_exchange(state, plan_node_id, pred_operators, num_receivers, false,\\n                                                         pass_through_type);\\n}\\n\\nOpFactories PipelineBuilderContext::maybe_interpolate_local_random_passthrough_exchange(RuntimeState* state,\\n                                                                                        int32_t plan_node_id,\\n                                                                                        OpFactories& pred_operators,\\n                                                                                        int num_receivers, bool force) {\\n    return _maybe_interpolate_local_passthrough_exchange(state, plan_node_id, pred_operators, num_receivers, force,\\n                                                         LocalExchanger::PassThroughType::RANDOM);\\n}\\n\\nOpFactories PipelineBuilderContext::maybe_interpolate_local_adpative_passthrough_exchange(\\n        RuntimeState* state, int32_t plan_node_id, OpFactories& pred_operators, int num_receivers, bool force) {\\n    return _maybe_interpolate_local_passthrough_exchange(state, plan_node_id, pred_operators, num_receivers, force,\\n                                                         LocalExchanger::PassThroughType::ADPATIVE);\\n}\\n\\nOpFactories PipelineBuilderContext::_maybe_interpolate_local_passthrough_exchange(\\n        RuntimeState* state, int32_t plan_node_id, OpFactories& pred_operators, int num_receivers, bool force,\\n        LocalExchanger::PassThroughType pass_through_type) {\\n    // predecessor pipeline has multiple drivers that will produce multiple output streams, but sort operator is\\n    // not parallelized now and can not accept multiple streams as input, so add a LocalExchange to gather multiple\\n    // streams and produce one output stream piping into the sort operator.\\n    DCHECK(!pred_operators.empty() && pred_operators[0]->is_source());\\n    auto* source_op = source_operator(pred_operators);\\n    if (!force && source_op->degree_of_parallelism() == num_receivers && !source_op->is_skewed()) {\\n        return pred_operators;\\n    }\\n\\n    pred_operators = maybe_interpolate_grouped_exchange(plan_node_id, pred_operators);\\n\\n    int max_input_dop = std::max(num_receivers, static_cast<int>(source_op->degree_of_parallelism()));\\n    auto mem_mgr = std::make_shared<ChunkBufferMemoryManager>(max_input_dop,\\n                                                              config::local_exchange_buffer_mem_limit_per_driver);\\n    auto local_exchange_source =\\n            std::make_shared<LocalExchangeSourceOperatorFactory>(next_operator_id(), plan_node_id, mem_mgr);\\n    local_exchange_source->set_runtime_state(state);\\n    inherit_upstream_source_properties(local_exchange_source.get(), source_op);\\n    local_exchange_source->set_could_local_shuffle(true);\\n    local_exchange_source->set_degree_of_parallelism(num_receivers);\\n\\n    std::shared_ptr<LocalExchanger> local_exchange;\\n    if (pass_through_type == LocalExchanger::PassThroughType::ADPATIVE) {\\n        local_exchange = std::make_shared<AdaptivePassthroughExchanger>(mem_mgr, local_exchange_source.get());\\n    } else if (pass_through_type == LocalExchanger::PassThroughType::RANDOM) {\\n        local_exchange = std::make_shared<RandomPassthroughExchanger>(mem_mgr, local_exchange_source.get());\\n    } else if (state->query_options().__isset.enable_connector_sink_writer_scaling &&\\n               state->query_options().enable_connector_sink_writer_scaling &&\\n               pass_through_type == LocalExchanger::PassThroughType::SCALE) {\\n        local_exchange = std::make_shared<ConnectorSinkPassthroughExchanger>(mem_mgr, local_exchange_source.get());\\n    } else {\\n        local_exchange = std::make_shared<PassthroughExchanger>(mem_mgr, local_exchange_source.get());\\n    }\\n    auto local_exchange_sink =\\n            std::make_shared<LocalExchangeSinkOperatorFactory>(next_operator_id(), plan_node_id, local_exchange);\\n    pred_operators.emplace_back(std::move(local_exchange_sink));\\n    add_pipeline(pred_operators);\\n\\n    return {std::move(local_exchange_source)};\\n}\\n\\nOpFactories PipelineBuilderContext::interpolate_local_key_partition_exchange(\\n        RuntimeState* state, int32_t plan_node_id, OpFactories& pred_operators,\\n        const std::vector<ExprContext*>& partition_expr_ctxs, int num_receivers,\\n        const std::vector<std::string>& transform_exprs) {\\n    auto* pred_source_op = source_operator(pred_operators);\\n    size_t source_dop = pred_source_op->degree_of_parallelism();\\n    auto mem_mgr =\\n            std::make_shared<ChunkBufferMemoryManager>(source_dop, config::local_exchange_buffer_mem_limit_per_driver);\\n    auto local_shuffle_source =\\n            std::make_shared<LocalExchangeSourceOperatorFactory>(next_operator_id(), plan_node_id, mem_mgr);\\n    auto local_exchanger = std::make_shared<KeyPartitionExchanger>(mem_mgr, local_shuffle_source.get(),\\n                                                                   partition_expr_ctxs, source_dop, transform_exprs);\\n    auto local_shuffle_sink =\\n            std::make_shared<LocalExchangeSinkOperatorFactory>(next_operator_id(), plan_node_id, local_exchanger);\\n    pred_operators.emplace_back(std::move(local_shuffle_sink));\\n    add_pipeline(pred_operators);\\n\\n    local_shuffle_source->set_runtime_state(state);\\n    inherit_upstream_source_properties(local_shuffle_source.get(), pred_source_op);\\n    local_shuffle_source->set_degree_of_parallelism(num_receivers);\\n\\n    return {std::move(local_shuffle_source)};\\n}\\n\\nOpFactories PipelineBuilderContext::maybe_interpolate_local_shuffle_exchange(\\n        RuntimeState* state, int32_t plan_node_id, OpFactories& pred_operators,\\n        const std::vector<ExprContext*>& self_partition_exprs) {\\n    return maybe_interpolate_local_shuffle_exchange(state, plan_node_id, pred_operators,\\n                                                    [&self_partition_exprs]() { return self_partition_exprs; });\\n}\\n\\nOpFactories PipelineBuilderContext::maybe_interpolate_local_shuffle_exchange(\\n        RuntimeState* state, int32_t plan_node_id, OpFactories& pred_operators,\\n        const PartitionExprsGenerator& self_partition_exprs_generator) {\\n    auto* source_op = source_operator(pred_operators);\\n    if (!source_op->could_local_shuffle()) {\\n        return pred_operators;\\n    }\\n\\n    if (!source_op->partition_exprs().empty()) {\\n        return _do_maybe_interpolate_local_shuffle_exchange(state, plan_node_id, pred_operators,\\n                                                            source_op->partition_exprs(), source_op->partition_type(),\\n                                                            source_op->get_bucket_properties());\\n    }\\n\\n    return _do_maybe_interpolate_local_shuffle_exchange(state, plan_node_id, pred_operators,\\n                                                        self_partition_exprs_generator(), source_op->partition_type(),\\n                                                        source_op->get_bucket_properties());\\n}\\n\\nOpFactories PipelineBuilderContext::maybe_interpolate_local_bucket_shuffle_exchange(\\n        RuntimeState* state, int32_t plan_node_id, OpFactories& pred_operators,\\n        const std::vector<ExprContext*>& partition_expr_ctxs) {\\n    auto* source_op = source_operator(pred_operators);\\n    if (!source_op->could_local_shuffle()) {\\n        return pred_operators;\\n    }\\n    return _do_maybe_interpolate_local_shuffle_exchange(state, plan_node_id, pred_operators, partition_expr_ctxs,\\n                                                        TPartitionType::BUCKET_SHUFFLE_HASH_PARTITIONED,\\n                                                        source_op->get_bucket_properties());\\n}\\n\\nOpFactories PipelineBuilderContext::_do_maybe_interpolate_local_shuffle_exchange(\\n        RuntimeState* state, int32_t plan_node_id, OpFactories& pred_operators,\\n        const std::vector<ExprContext*>& partition_expr_ctxs, const TPartitionType::type part_type,\\n        const std::vector<TBucketProperty>& bucket_properties) {\\n    DCHECK(!pred_operators.empty() && pred_operators[0]->is_source());\\n\\n    // interpolate grouped exchange if needed\\n    // TODO: If the local exchange supports spills, we don't need to prevent group execution\\n    pred_operators = maybe_interpolate_grouped_exchange(plan_node_id, pred_operators);\\n\\n    // If DOP is one, we needn't partition input chunks.\\n    size_t shuffle_partitions_num = degree_of_parallelism();\\n    if (shuffle_partitions_num <= 1) {\\n        return pred_operators;\\n    }\\n\\n    auto* pred_source_op = source_operator(pred_operators);\\n    int64_t limit_size = _prev_limit_size(pred_operators);\\n\\n    // To make sure at least one partition source operator is ready to output chunk before sink operators are full.\\n    auto mem_mgr = std::make_shared<ChunkBufferMemoryManager>(shuffle_partitions_num,\\n                                                              config::local_exchange_buffer_mem_limit_per_driver);\\n    auto local_shuffle_source =\\n            std::make_shared<LocalExchangeSourceOperatorFactory>(next_operator_id(), plan_node_id, mem_mgr);\\n    local_shuffle_source->set_runtime_state(state);\\n    inherit_upstream_source_properties(local_shuffle_source.get(), pred_source_op);\\n    local_shuffle_source->set_could_local_shuffle(pred_source_op->partition_exprs().empty() &&\\n                                                  bucket_properties.empty());\\n    local_shuffle_source->set_degree_of_parallelism(shuffle_partitions_num);\\n\\n    auto local_shuffle = std::make_shared<PartitionExchanger>(mem_mgr, local_shuffle_source.get(), part_type,\\n                                                              partition_expr_ctxs, bucket_properties);\\n    auto local_shuffle_sink =\\n            std::make_shared<LocalExchangeSinkOperatorFactory>(next_operator_id(), plan_node_id, local_shuffle);\\n    pred_operators.emplace_back(std::move(local_shuffle_sink));\\n    add_pipeline(pred_operators);\\n\\n    OpFactories source_operators = {std::move(local_shuffle_source)};\\n    _try_interpolate_limit_operator(plan_node_id, source_operators, limit_size);\\n    return source_operators;\\n}\\n\\nOpFactories PipelineBuilderContext::maybe_interpolate_local_ordered_partition_exchange(\\n        RuntimeState* state, int32_t plan_node_id, OpFactories& pred_operators,\\n        const std::vector<ExprContext*>& partition_expr_ctxs) {\\n    DCHECK(!pred_operators.empty() && pred_operators[0]->is_source());\\n\\n    // interpolate grouped exchange if needed\\n    // TODO: If the local exchange supports spills, we don't need to prevent group execution\\n    pred_operators = maybe_interpolate_grouped_exchange(plan_node_id, pred_operators);\\n\\n    // If DOP is one, we needn't partition input chunks.\\n    size_t shuffle_partitions_num = degree_of_parallelism();\\n    if (shuffle_partitions_num <= 1) {\\n        return pred_operators;\\n    }\\n\\n    auto* pred_source_op = source_operator(pred_operators);\\n    int64_t limit_size = _prev_limit_size(pred_operators);\\n\\n    auto mem_mgr = std::make_shared<ChunkBufferMemoryManager>(shuffle_partitions_num,\\n                                                              config::local_exchange_buffer_mem_limit_per_driver);\\n    auto local_shuffle_source =\\n            std::make_shared<LocalExchangeSourceOperatorFactory>(next_operator_id(), plan_node_id, mem_mgr);\\n    local_shuffle_source->set_runtime_state(state);\\n    inherit_upstream_source_properties(local_shuffle_source.get(), pred_source_op);\\n    local_shuffle_source->set_could_local_shuffle(pred_source_op->partition_exprs().empty());\\n    local_shuffle_source->set_degree_of_parallelism(shuffle_partitions_num);\\n\\n    auto local_shuffle =\\n            std::make_shared<OrderedPartitionExchanger>(mem_mgr, local_shuffle_source.get(), partition_expr_ctxs);\\n    auto local_shuffle_sink =\\n            std::make_shared<LocalExchangeSinkOperatorFactory>(next_operator_id(), plan_node_id, local_shuffle);\\n\\n    pred_operators.emplace_back(std::move(local_shuffle_sink));\\n    add_pipeline(pred_operators);\\n\\n    OpFactories source_operators = {std::move(local_shuffle_source)};\\n    _try_interpolate_limit_operator(plan_node_id, source_operators, limit_size);\\n    return source_operators;\\n}\\n\\nvoid PipelineBuilderContext::interpolate_spill_process(size_t plan_node_id,\\n                                                       const SpillProcessChannelFactoryPtr& spill_channel_factory,\\n                                                       size_t dop) {\\n    OpFactories spill_process_operators;\\n    auto spill_process_factory = std::make_shared<SpillProcessOperatorFactory>(next_operator_id(), \\\"spill_process\\\",\\n                                                                               plan_node_id, spill_channel_factory);\\n    spill_process_factory->set_degree_of_parallelism(dop);\\n    spill_process_operators.emplace_back(std::move(spill_process_factory));\\n    auto noop_sink_factory = std::make_shared<NoopSinkOperatorFactory>(next_operator_id(), plan_node_id);\\n    spill_process_operators.emplace_back(std::move(noop_sink_factory));\\n    add_pipeline(std::move(spill_process_operators));\\n}\\n\\nOpFactories PipelineBuilderContext::interpolate_grouped_exchange(int32_t plan_node_id, OpFactories& pred_operators) {\\n    size_t physical_dop = degree_of_parallelism();\\n    auto* source_op = source_operator(pred_operators);\\n    int logical_dop = source_op->degree_of_parallelism();\\n\\n    // check should interpolate limit operator\\n    int64_t limit_size = _prev_limit_size(pred_operators);\\n\\n    auto mem_mgr =\\n            std::make_shared<ChunkBufferMemoryManager>(logical_dop, config::local_exchange_buffer_mem_limit_per_driver);\\n\\n    auto local_shuffle_source =\\n            std::make_shared<LocalExchangeSourceOperatorFactory>(next_operator_id(), plan_node_id, mem_mgr);\\n    auto local_exchanger = std::make_shared<PassthroughExchanger>(mem_mgr, local_shuffle_source.get());\\n    auto group_exchange_sink = std::make_shared<GroupedExecutionSinkFactory>(next_operator_id(), plan_node_id,\\n                                                                             local_exchanger, _current_execution_group);\\n    pred_operators.emplace_back(std::move(group_exchange_sink));\\n\\n    auto prev_source_operator = source_operator(pred_operators);\\n    inherit_upstream_source_properties(local_shuffle_source.get(), prev_source_operator);\\n    local_shuffle_source->set_could_local_shuffle(true);\\n    local_shuffle_source->set_degree_of_parallelism(physical_dop);\\n    add_pipeline(pred_operators);\\n    // switch to new normal group\\n    _current_execution_group = _normal_exec_group;\\n\\n    OpFactories source_operators = {std::move(local_shuffle_source)};\\n    _try_interpolate_limit_operator(plan_node_id, source_operators, limit_size);\\n    return source_operators;\\n}\\n\\nOpFactories PipelineBuilderContext::maybe_interpolate_grouped_exchange(int32_t plan_node_id,\\n                                                                       OpFactories& pred_operators) {\\n    if (dynamic_cast<ColocateExecutionGroup*>(_current_execution_group) != nullptr) {\\n        return interpolate_grouped_exchange(plan_node_id, pred_operators);\\n    }\\n    return pred_operators;\\n}\\n\\nOpFactories PipelineBuilderContext::maybe_gather_pipelines_to_one(RuntimeState* state, int32_t plan_node_id,\\n                                                                  std::vector<OpFactories>& pred_operators_list,\\n                                                                  LocalExchanger::PassThroughType pass_through_type) {\\n    // If there is only one pred pipeline, we needn't local passthrough anymore.\\n    if (pred_operators_list.size() == 1) {\\n        return pred_operators_list[0];\\n    }\\n\\n    // Approximately, each pred driver can output state->chunk_size() rows at the same time.\\n    size_t max_input_dop = 0;\\n    for (const auto& pred_ops : pred_operators_list) {\\n        auto* source_op = source_operator(pred_ops);\\n        max_input_dop += source_op->degree_of_parallelism();\\n    }\\n\\n    auto mem_mgr = std::make_shared<ChunkBufferMemoryManager>(max_input_dop,\\n                                                              config::local_exchange_buffer_mem_limit_per_driver);\\n    auto local_exchange_source =\\n            std::make_shared<LocalExchangeSourceOperatorFactory>(next_operator_id(), plan_node_id, mem_mgr);\\n    local_exchange_source->set_runtime_state(state);\\n    auto* first_upstream_source = source_operator(pred_operators_list[0]);\\n    inherit_upstream_source_properties(local_exchange_source.get(), first_upstream_source);\\n    local_exchange_source->set_could_local_shuffle(true);\\n    local_exchange_source->set_degree_of_parallelism(degree_of_parallelism());\\n\\n    std::vector<EventPtr> group_blocking_events;\\n    for (const auto& pred_ops : pred_operators_list) {\\n        auto* source = source_operator(pred_ops);\\n        if (auto event = source->group_leader()->adaptive_blocking_event(); event != nullptr) {\\n            group_blocking_events.emplace_back(std::move(event));\\n        }\\n    }\\n\\n    for (int i = 1; i < pred_operators_list.size(); i++) {\\n        auto* upstream_source = source_operator(pred_operators_list[i]);\\n        local_exchange_source->add_upstream_source(upstream_source);\\n        first_upstream_source->union_group(upstream_source);\\n    }\\n\\n    if (!group_blocking_events.empty()) {\\n        EventPtr merged_blocking_events = Event::depends_all(group_blocking_events);\\n        local_exchange_source->group_leader()->set_adaptive_blocking_event(std::move(merged_blocking_events));\\n    }\\n\\n    std::shared_ptr<LocalExchanger> exchanger;\\n    if (pass_through_type == LocalExchanger::PassThroughType::RANDOM) {\\n        exchanger = std::make_shared<PassthroughExchanger>(mem_mgr, local_exchange_source.get());\\n    } else {\\n        exchanger = std::make_shared<DirectThroughExchanger>(mem_mgr, local_exchange_source.get());\\n    }\\n\\n    for (auto& pred_operators : pred_operators_list) {\\n        auto local_exchange_sink =\\n                std::make_shared<LocalExchangeSinkOperatorFactory>(next_operator_id(), plan_node_id, exchanger);\\n        pred_operators.emplace_back(std::move(local_exchange_sink));\\n        add_pipeline(pred_operators);\\n    }\\n\\n    return {std::move(local_exchange_source)};\\n}\\n\\nOpFactories PipelineBuilderContext::maybe_interpolate_collect_stats(RuntimeState* state, int32_t plan_node_id,\\n                                                                    OpFactories& pred_operators) {\\n    if (!_fragment_context->enable_adaptive_dop()) {\\n        return pred_operators;\\n    }\\n\\n    if (pred_operators.empty()) {\\n        return pred_operators;\\n    }\\n\\n    auto* pred_source_op = source_operator(pred_operators);\\n    size_t dop = pred_source_op->degree_of_parallelism();\\n    CollectStatsContextPtr collect_stats_ctx =\\n            std::make_shared<CollectStatsContext>(state, dop, _fragment_context->adaptive_dop_param());\\n\\n    auto last_plan_node_id = pred_operators[pred_operators.size() - 1]->plan_node_id();\\n    pred_operators.emplace_back(std::make_shared<CollectStatsSinkOperatorFactory>(next_operator_id(), last_plan_node_id,\\n                                                                                  collect_stats_ctx));\\n    add_pipeline(std::move(pred_operators));\\n\\n    auto downstream_source_op = std::make_shared<CollectStatsSourceOperatorFactory>(\\n            next_operator_id(), last_plan_node_id, std::move(collect_stats_ctx));\\n    inherit_upstream_source_properties(downstream_source_op.get(), pred_source_op);\\n    downstream_source_op->set_partition_exprs(pred_source_op->partition_exprs());\\n\\n    for (const auto& pipeline : _dependent_pipelines) {\\n        downstream_source_op->add_group_dependent_pipeline(pipeline);\\n    }\\n\\n    return {std::move(downstream_source_op)};\\n}\\n\\nOpFactories PipelineBuilderContext::maybe_interpolate_debug_ops(RuntimeState* state, int32_t plan_node_id,\\n                                                                OpFactories& pred_operators) {\\n    auto action_opt = runtime_state()->debug_action_mgr().get_debug_action(plan_node_id);\\n    if (action_opt.has_value() && action_opt.value().is_wait_action()) {\\n        auto* pred_source_op = source_operator(pred_operators);\\n        auto wait_context_factory = std::make_shared<WaitContextFactory>(action_opt->value);\\n        auto wait_sink =\\n                std::make_shared<WaitOperatorSinkFactory>(next_operator_id(), plan_node_id, wait_context_factory);\\n\\n        pred_operators.push_back(std::move(wait_sink));\\n        add_pipeline(pred_operators);\\n\\n        auto wait_src =\\n                std::make_shared<WaitOperatorSourceFactory>(next_operator_id(), plan_node_id, wait_context_factory);\\n        this->inherit_upstream_source_properties(wait_src.get(), pred_source_op);\\n        return {std::move(wait_src)};\\n    }\\n    return pred_operators;\\n}\\n\\nsize_t PipelineBuilderContext::dop_of_source_operator(int source_node_id) {\\n    auto* morsel_queue_factory = morsel_queue_factory_of_source_operator(source_node_id);\\n    return morsel_queue_factory->size();\\n}\\n\\nMorselQueueFactory* PipelineBuilderContext::morsel_queue_factory_of_source_operator(int source_node_id) {\\n    auto& morsel_queues = _fragment_context->morsel_queue_factories();\\n    DCHECK(morsel_queues.count(source_node_id));\\n    return morsel_queues[source_node_id].get();\\n}\\n\\nMorselQueueFactory* PipelineBuilderContext::morsel_queue_factory_of_source_operator(\\n        const SourceOperatorFactory* source_op) {\\n    if (!source_op->with_morsels()) {\\n        return nullptr;\\n    }\\n\\n    return morsel_queue_factory_of_source_operator(source_op->plan_node_id());\\n}\\n\\nSourceOperatorFactory* PipelineBuilderContext::source_operator(const OpFactories& ops) {\\n    return down_cast<SourceOperatorFactory*>(ops[0].get());\\n}\\n\\nbool PipelineBuilderContext::could_local_shuffle(const OpFactories& ops) const {\\n    return down_cast<SourceOperatorFactory*>(ops[0].get())->could_local_shuffle();\\n}\\n\\nbool PipelineBuilderContext::should_interpolate_cache_operator(int32_t plan_node_id, OpFactoryPtr& source_op) {\\n    if (!_fragment_context->enable_cache()) {\\n        return false;\\n    }\\n    const auto& cache_param = _fragment_context->cache_param();\\n    if (cache_param.plan_node_id != plan_node_id) {\\n        return false;\\n    }\\n    return dynamic_cast<pipeline::ScanOperatorFactory*>(source_op.get()) != nullptr;\\n}\\n\\nOpFactories PipelineBuilderContext::interpolate_cache_operator(\\n        int32_t plan_node_id, OpFactories& upstream_pipeline, OpFactories& downstream_pipeline,\\n        const std::function<std::tuple<OpFactoryPtr, SourceOperatorFactoryPtr>(bool)>& merge_operators_generator) {\\n    DCHECK(should_interpolate_cache_operator(downstream_pipeline[0]->plan_node_id(), upstream_pipeline[0]));\\n\\n    const auto& cache_param = _fragment_context->cache_param();\\n\\n    auto sink_op = upstream_pipeline.back();\\n    upstream_pipeline.pop_back();\\n    auto source_op = downstream_pipeline.front();\\n    downstream_pipeline.clear();\\n\\n    auto dop = down_cast<SourceOperatorFactory*>(source_op.get())->degree_of_parallelism();\\n    auto conjugate_op = std::make_shared<query_cache::ConjugateOperatorFactory>(sink_op, source_op);\\n    upstream_pipeline.push_back(std::move(conjugate_op));\\n\\n    auto last_ml_op_idx = upstream_pipeline.size() - 1;\\n    for (auto i = 1; i < upstream_pipeline.size(); ++i) {\\n        auto& op = upstream_pipeline[i];\\n        auto ml_op =\\n                std::make_shared<query_cache::MultilaneOperatorFactory>(next_operator_id(), op, cache_param.num_lanes);\\n        // only last multilane operator in the pipeline driver can work in passthrough mode\\n        auto can_passthrough = i == last_ml_op_idx;\\n        ml_op->set_can_passthrough(can_passthrough);\\n        upstream_pipeline[i] = std::move(ml_op);\\n    }\\n\\n    auto cache_mgr = ExecEnv::GetInstance()->cache_mgr();\\n    auto cache_op = std::make_shared<query_cache::CacheOperatorFactory>(next_operator_id(), plan_node_id, cache_mgr,\\n                                                                        cache_param);\\n    upstream_pipeline.push_back(cache_op);\\n\\n    auto merge_operators = merge_operators_generator(true);\\n    upstream_pipeline.push_back(std::move(std::get<0>(merge_operators)));\\n    downstream_pipeline.push_back(std::move(std::get<1>(merge_operators)));\\n    down_cast<SourceOperatorFactory*>(downstream_pipeline.front().get())->set_degree_of_parallelism(dop);\\n    return downstream_pipeline;\\n}\\n\\nvoid PipelineBuilderContext::inherit_upstream_source_properties(SourceOperatorFactory* downstream_source,\\n                                                                SourceOperatorFactory* upstream_source) {\\n    downstream_source->set_degree_of_parallelism(upstream_source->degree_of_parallelism());\\n    downstream_source->set_could_local_shuffle(upstream_source->could_local_shuffle());\\n    downstream_source->set_partition_type(upstream_source->partition_type());\\n    if (!upstream_source->partition_exprs().empty() || !downstream_source->partition_exprs().empty()) {\\n        downstream_source->set_partition_exprs(upstream_source->partition_exprs());\\n    }\\n\\n    if (downstream_source->adaptive_initial_state() == SourceOperatorFactory::AdaptiveState::NONE) {\\n        downstream_source->add_upstream_source(upstream_source);\\n    }\\n}\\n\\nvoid PipelineBuilderContext::push_dependent_pipeline(const Pipeline* pipeline) {\\n    _dependent_pipelines.emplace_back(pipeline);\\n}\\nvoid PipelineBuilderContext::pop_dependent_pipeline() {\\n    _dependent_pipelines.pop_back();\\n}\\n\\nint64_t PipelineBuilderContext::_prev_limit_size(const OpFactories& pred_operators) {\\n    for (auto it = pred_operators.rbegin(); it != pred_operators.rend(); ++it) {\\n        if (auto limit = dynamic_cast<LimitOperatorFactory*>(it->get())) {\\n            return limit->limit();\\n        } else if (dynamic_cast<ChunkAccumulateOperatorFactory*>(it->get()) == nullptr) {\\n            return -1;\\n        }\\n    }\\n    return -1;\\n}\\n\\nvoid PipelineBuilderContext::_try_interpolate_limit_operator(int32_t plan_node_id, OpFactories& pred_operators,\\n                                                             int64_t limit_size) {\\n    if (limit_size >= 0 && limit_size < config::pipline_limit_max_delivery) {\\n        pred_operators.emplace_back(\\n                std::make_shared<LimitOperatorFactory>(next_operator_id(), plan_node_id, limit_size));\\n    }\\n}\\n\\nvoid PipelineBuilderContext::_subscribe_pipeline_event(Pipeline* pipeline) {\\n    bool enable_wait_event = _fragment_context->runtime_state()->enable_wait_dependent_event();\\n    enable_wait_event &= !_current_execution_group->is_colocate_exec_group();\\n    if (enable_wait_event && !_dependent_pipelines.empty()) {\\n        pipeline->pipeline_event()->set_need_wait_dependencies_finished(true);\\n        pipeline->pipeline_event()->add_dependency(_dependent_pipelines.back()->pipeline_event());\\n    }\\n}\\n\\nOpFactories PipelineBuilder::decompose_exec_node_to_pipeline(const FragmentContext& fragment, ExecNode* exec_node) {\\n    pipeline::OpFactories operators = exec_node->decompose_to_pipeline(&_context);\\n    operators = _context.maybe_interpolate_grouped_exchange(exec_node->id(), operators);\\n    return operators;\\n}\\n\\nstd::pair<ExecutionGroups, Pipelines> PipelineBuilder::build() {\\n    return {_context.execution_groups(), _context.pipelines()};\\n}\\n\\n} // namespace starrocks::pipeline\\n\",\n  \"message\": \"Successfully read 614 lines from file: /Users/kks/git/starrocks/be/src/exec/pipeline/pipeline_builder.cpp\"\n}"
            }
          }
        ],
        "providerOptions": null
      },
      {
        "role": "assistant",
        "content": [
          {
            "type": "tool-call",
            "toolCallId": "call_MHxMQTluSU1DcndGS29ZTm51cW4",
            "toolName": "codeSearch",
            "input": {
              "path": "fe/fe-core/src/main/java/com/starrocks/sql/plan/",
              "pattern": "Fragment"
            }
          }
        ],
        "providerOptions": null
      },
      {
        "role": "tool",
        "content": [
          {
            "type": "tool-result",
            "toolCallId": "call_MHxMQTluSU1DcndGS29ZTm51cW4",
            "toolName": "codeSearch",
            "output": {
              "type": "text",
              "value": "{\n  \"success\": true,\n  \"result\": \"Found 20 matches:\\nFile: /Users/kks/git/starrocks/fe/fe-core/src/main/java/com/starrocks/sql/plan/ExecPlan.java\\n  25: import com.starrocks.planner.PlanFragment;\\n  26: import com.starrocks.planner.PlanFragmentId;\\n  54: private final ArrayList<PlanFragment> fragments = new ArrayList<>();\\n  55: private final Map<Integer, PlanFragment> cteProduceFragments = Maps.newHashMap();\\n  56: // splitProduceFragments and joinNodeMap is used for skew join\\n  57: private final Map<Integer, PlanFragment> splitProduceFragments = Maps.newHashMap();\\n  67: private final IdGenerator<PlanFragmentId> fragmentIdGenerator = PlanFragmentId.createGenerator();\\n  101: public ExecPlan(ConnectContext connectContext, List<PlanFragment> fragments) {\\n  106: this.fragments.addAll(fragments);\\n  122: public ArrayList<PlanFragment> getFragments() {\\n\\nFile: /Users/kks/git/starrocks/fe/fe-core/src/main/java/com/starrocks/sql/plan/PlanFragmentBuilder.java\\n  76: import com.starrocks.planner.FragmentNormalizer;\\n  89: import com.starrocks.planner.MultiCastPlanFragment;\\n  98: import com.starrocks.planner.PlanFragment;\\n  112: import com.starrocks.planner.SplitCastPlanFragment;\\n  260: * PlanFragmentBuilder used to transform physical operator to exec plan fragment\\n  262: public class PlanFragmentBuilder {\\n  263: private static final Logger LOG = LogManager.getLogger(PlanFragmentBuilder.class);\\n  269: boolean hasOutputFragment, boolean isShortCircuit) {\\n  272: createOutputFragment(new PhysicalPlanTranslator(columnRefFactory).translate(plan, execPlan), execPlan,\\n  273: outputColumns, hasOutputFragment);\"\n}"
            }
          }
        ],
        "providerOptions": null
      },
      {
        "role": "assistant",
        "content": [
          {
            "type": "tool-call",
            "toolCallId": "call_MHw2T1V6a08yTzhIZnpVV1RvNWg",
            "toolName": "listFiles",
            "input": {
              "directory_path": "/Users/kks/git/starrocks/fe/fe-core/src/main/java/com/starrocks/planner/"
            }
          }
        ],
        "providerOptions": null
      },
      {
        "role": "tool",
        "content": [
          {
            "type": "tool-result",
            "toolCallId": "call_MHw2T1V6a08yTzhIZnpVV1RvNWg",
            "toolName": "listFiles",
            "output": {
              "type": "text",
              "value": "dirs: expression; stream; AbstractOlapTableScanNode.java; AggregateInfo.java; AggregateInfoBase.java; AggregationNode.java; AnalyticEvalNode.java; AssertNumRowsNode.java; BenchmarkScanNode.java; BinlogScanNode.java; BlackHoleTableSink.java; CaptureVersionNode.java; DataPartition.java; DataSink.java; DataStreamSink.java; DecodeNode.java; DeltaLakeScanNode.java; DescriptorTable.java; DictionaryCacheSink.java; DistributionPruner.java; EmptySetNode.java; EsScanNode.java; ExceptNode.java; ExchangeNode.java; ExecGroup.java; ExecGroupId.java; ExecGroupSets.java; ExportSink.java; FetchNode.java; FileScanNode.java; FileTableScanNode.java; FragmentNormalizer.java; HashDistributionKey.java; HashDistributionPruner.java; HashJoinNode.java; HdfsScanNode.java; HiveTableSink.java; HudiScanNode.java; IcebergDeleteSink.java; IcebergMetadataScanNode.java; IcebergScanNode.java; IcebergTableSink.java; IntersectNode.java; JDBCScanNode.java; JoinNode.java; KuduScanNode.java; LoadScanNode.java; LookUpNode.java; MergeJoinNode.java; MetaScanNode.java; MultiCastDataSink.java; MultiCastPlanFragment.java; MysqlScanNode.java; MysqlTableSink.java; NestLoopJoinNode.java; NoopSink.java; OdpsScanNode.java; OlapScanNode.java; OlapTableSink.java; PaimonScanNode.java; PartitionColumnFilter.java; PartitionIdGenerator.java; PartitionPruner.java; PlanFragment.java; PlanFragmentId.java; PlanNode.java; PlanNodeId.java; ProjectNode.java; RangeDistributionPruner.java; RangePartitionPruner.java; RawValuesNode.java; RepeatNode.java; ResultSink.java; RuntimeFilterBuildNode.java; RuntimeFilterDescription.java; RuntimeFilterId.java; RuntimeFilterLayout.java; RuntimeFilterPushDownContext.java; ScanNode.java; SchemaScanNode.java; SchemaTableSink.java; SelectNode.java; SetOperationNode.java; SlotDescriptor.java; SlotId.java; SortInfo.java; SortNode.java; SplitCastDataSink.java; SplitCastPlanFragment.java; StreamLoadPlanner.java; StreamLoadScanNode.java; TableFunctionNode.java; TableFunctionTableSink.java; TupleDescriptor.java; TupleId.java; UnionNode.java\n\nexpression dirs: ExprOpcodeRegistry.java; ExprToNormalFormVisitor.java; ExprToThrift.java\n\nstream dirs: StreamAggNode.java; StreamJoinNode.java"
            }
          }
        ],
        "providerOptions": null
      },
      {
        "role": "assistant",
        "content": [
          {
            "type": "tool-call",
            "toolCallId": "call_MHxzTDdiR09GUkxhNEdSMDNha2s",
            "toolName": "readFile",
            "input": {
              "file_path": "/Users/kks/git/starrocks/fe/fe-core/src/main/java/com/starrocks/planner/PlanFragment.java"
            }
          }
        ],
        "providerOptions": null
      },
      {
        "role": "tool",
        "content": [
          {
            "type": "tool-result",
            "toolCallId": "call_MHxzTDdiR09GUkxhNEdSMDNha2s",
            "toolName": "readFile",
            "output": {
              "type": "text",
              "value": "{\n  \"success\": true,\n  \"file_path\": \"/Users/kks/git/starrocks/fe/fe-core/src/main/java/com/starrocks/planner/PlanFragment.java\",\n  \"content\": \"// Copyright 2021-present StarRocks, Inc. All rights reserved.\\n//\\n// Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\\n// you may not use this file except in compliance with the License.\\n// You may obtain a copy of the License at\\n//\\n//     https://www.apache.org/licenses/LICENSE-2.0\\n//\\n// Unless required by applicable law or agreed to in writing, software\\n// distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n// See the License for the specific language governing permissions and\\n// limitations under the License.\\n\\n// This file is based on code available under the Apache license here:\\n//   https://github.com/apache/incubator-doris/blob/master/fe/fe-core/src/main/java/org/apache/doris/planner/PlanFragment.java\\n\\n// Licensed to the Apache Software Foundation (ASF) under one\\n// or more contributor license agreements.  See the NOTICE file\\n// distributed with this work for additional information\\n// regarding copyright ownership.  The ASF licenses this file\\n// to you under the Apache License, Version 2.0 (the\\n// \\\"License\\\"); you may not use this file except in compliance\\n// with the License.  You may obtain a copy of the License at\\n//\\n//   http://www.apache.org/licenses/LICENSE-2.0\\n//\\n// Unless required by applicable law or agreed to in writing,\\n// software distributed under the License is distributed on an\\n// \\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\n// KIND, either express or implied.  See the License for the\\n// specific language governing permissions and limitations\\n// under the License.\\n\\npackage com.starrocks.planner;\\n\\nimport com.google.common.base.Preconditions;\\nimport com.google.common.collect.Lists;\\nimport com.google.common.collect.Maps;\\nimport com.google.common.collect.Sets;\\nimport com.starrocks.common.Config;\\nimport com.starrocks.common.FeConstants;\\nimport com.starrocks.common.Pair;\\nimport com.starrocks.connector.BucketProperty;\\nimport com.starrocks.planner.expression.ExprToThrift;\\nimport com.starrocks.qe.ConnectContext;\\nimport com.starrocks.qe.SessionVariable;\\nimport com.starrocks.sql.ast.TreeNode;\\nimport com.starrocks.sql.ast.expression.Expr;\\nimport com.starrocks.sql.ast.expression.ExprToSql;\\nimport com.starrocks.sql.ast.expression.ExprUtils;\\nimport com.starrocks.sql.optimizer.Utils;\\nimport com.starrocks.sql.optimizer.statistics.ColumnDict;\\nimport com.starrocks.sql.plan.ExecPlan;\\nimport com.starrocks.thrift.TCacheParam;\\nimport com.starrocks.thrift.TDataSink;\\nimport com.starrocks.thrift.TExplainLevel;\\nimport com.starrocks.thrift.TExpr;\\nimport com.starrocks.thrift.TGlobalDict;\\nimport com.starrocks.thrift.TGroupExecutionParam;\\nimport com.starrocks.thrift.TNetworkAddress;\\nimport com.starrocks.thrift.TPartitionType;\\nimport com.starrocks.thrift.TPlanFragment;\\nimport com.starrocks.thrift.TResultSinkType;\\nimport org.apache.commons.collections.CollectionUtils;\\nimport org.apache.commons.collections4.MapUtils;\\nimport org.roaringbitmap.RoaringBitmap;\\n\\nimport java.nio.ByteBuffer;\\nimport java.util.ArrayList;\\nimport java.util.Comparator;\\nimport java.util.Deque;\\nimport java.util.LinkedList;\\nimport java.util.List;\\nimport java.util.Map;\\nimport java.util.Optional;\\nimport java.util.Queue;\\nimport java.util.Set;\\nimport java.util.function.Consumer;\\nimport java.util.stream.Collectors;\\nimport java.util.stream.Stream;\\n\\n/**\\n * PlanFragments form a tree structure via their ExchangeNodes. A tree of fragments\\n * connected in that way forms a plan. The output of a plan is produced by the root\\n * fragment and is either the result of the query or an intermediate result\\n * needed by a different plan (such as a hash table).\\n * <p>\\n * Plans are grouped into cohorts based on the consumer of their output: all\\n * plans that materialize intermediate results for a particular consumer plan\\n * are grouped into a single cohort.\\n * <p>\\n * A PlanFragment encapsulates the specific tree of execution nodes that\\n * are used to produce the output of the plan fragment, as well as output exprs,\\n * destination node, etc. If there are no output exprs, the full row that is\\n * is produced by the plan root is marked as materialized.\\n * <p>\\n * A plan fragment can have one or many instances, each of which in turn is executed by\\n * an individual node and the output sent to a specific instance of the destination\\n * fragment (or, in the case of the root fragment, is materialized in some form).\\n * <p>\\n * A hash-partitioned plan fragment is the result of one or more hash-partitioning data\\n * streams being received by plan nodes in this fragment. In the future, a fragment's\\n * data partition could also be hash partitioned based on a scan node that is reading\\n * from a physically hash-partitioned table.\\n * <p>\\n * The sequence of calls is:\\n * - c'tor\\n * - assemble with getters, etc.\\n * - finalize()\\n * - toThrift()\\n * <p>\\n * TODO: the tree of PlanNodes is connected across fragment boundaries, which makes\\n * it impossible search for things within a fragment (using TreeNode functions);\\n * fix that\\n */\\npublic class PlanFragment extends TreeNode<PlanFragment> {\\n    // id for this plan fragment\\n    protected final PlanFragmentId fragmentId;\\n\\n    // root of plan tree executed by this fragment\\n    protected PlanNode planRoot;\\n\\n    // exchange node to which this fragment sends its output\\n    private ExchangeNode destNode;\\n\\n    // if null, outputs the entire row produced by planRoot\\n    protected ArrayList<Expr> outputExprs;\\n\\n    // created in finalize() or set in setSink()\\n    protected DataSink sink;\\n\\n    // specification of the partition of the input of this fragment;\\n    // an UNPARTITIONED fragment is executed on only a single node\\n    // TODO: improve this comment, \\\"input\\\" is a bit misleading\\n    protected final DataPartition dataPartition;\\n\\n    // specification of how the output of this fragment is partitioned (i.e., how\\n    // it's sent to its destination);\\n    // if the output is UNPARTITIONED, it is being broadcast\\n    protected DataPartition outputPartition;\\n\\n    // Whether query statistics is sent with every batch. In order to get the query\\n    // statistics correctly when query contains limit, it is necessary to send query \\n    // statistics with every batch, or only in close.\\n    protected boolean transferQueryStatisticsWithEveryBatch;\\n\\n    // TODO: SubstitutionMap outputSmap;\\n    // substitution map to remap exprs onto the output of this fragment, to be applied\\n    // at destination fragment\\n\\n    // specification of the number of parallel when fragment is executed\\n    // default value is 1\\n    protected int parallelExecNum = 1;\\n    protected int pipelineDop = 1;\\n    protected boolean dopEstimated = false;\\n\\n    // Whether to assign scan ranges to each driver sequence of pipeline,\\n    // for the normal backend assignment (not colocate, bucket, and replicated join).\\n    protected boolean assignScanRangesPerDriverSeq = false;\\n    protected boolean withLocalShuffle = false;\\n\\n    protected double fragmentCost;\\n\\n    protected Map<Integer, RuntimeFilterDescription> buildRuntimeFilters = Maps.newHashMap();\\n    protected Map<Integer, RuntimeFilterDescription> probeRuntimeFilters = Maps.newHashMap();\\n\\n    protected List<Pair<Integer, ColumnDict>> queryGlobalDicts = Lists.newArrayList();\\n    protected Map<Integer, Expr> queryGlobalDictExprs;\\n    protected List<Pair<Integer, ColumnDict>> loadGlobalDicts = Lists.newArrayList();\\n\\n    private final Set<Integer> runtimeFilterBuildNodeIds = Sets.newHashSet();\\n\\n    private TCacheParam cacheParam = null;\\n    private boolean hasOlapTableSink = false;\\n    private boolean hasIcebergTableSink = false;\\n    private boolean hasHiveTableSink = false;\\n    private boolean hasTableFunctionTableSink = false;\\n\\n    private boolean forceSetTableSinkDop = false;\\n    private boolean forceAssignScanRangesPerDriverSeq = false;\\n\\n    private boolean useRuntimeAdaptiveDop = false;\\n\\n    private boolean isShortCircuit = false;\\n\\n    // Controls whether group execution is used for plan fragment execution.\\n    private List<ExecGroup> colocateExecGroups = Lists.newArrayList();\\n\\n    private List<Integer> collectExecStatsIds;\\n\\n    // If all scan nodes in the entire plan are OLAP scan nodes and at most one tablet is used,\\n    // then all fragments will have at most one instance (running on a single BE).\\n    // In this case, there is no need to put the ResultSink in a dedicated gather fragment.\\n    //\\n    // CN mode(prefer_compute_node is true or share-data mode) needs special handling here, because its strategy for selecting\\n    // BE/CN nodes for remote fragments (whose left-deep node is not a scan node) is different from non-CN mode:\\n    // - CN mode: selects all CN nodes as instances, which will be more than 1 here,\\n    //   causing the fragment that contains the ResultSink to also have more than 1 instance.\\n    // - None-CN mode: selects the BE node(s) where the child fragment is running.\\n    //\\n    // Therefore, if isSingleTabletGatherOutputFragment is true, CN mode should use the same selection strategy as none-CN mode.\\n    private boolean isSingleTabletGatherOutputFragment = false;\\n\\n    /**\\n     * C'tor for fragment with specific partition; the output is by default broadcast.\\n     */\\n    public PlanFragment(PlanFragmentId id, PlanNode root, DataPartition partition) {\\n        this.fragmentId = id;\\n        this.dataPartition = partition;\\n        this.outputPartition = DataPartition.UNPARTITIONED;\\n        this.transferQueryStatisticsWithEveryBatch = false;\\n        // when dop adaptation is enabled, parallelExecNum and pipelineDop set to degreeOfParallelism and 1 respectively\\n        // in default. these values just a hint to help determine numInstances and pipelineDop of a PlanFragment.\\n        setPlanRoot(root);\\n        setParallelExecNumIfExists();\\n        setFragmentInPlanTree(planRoot);\\n    }\\n\\n    /**\\n     * Assigns 'this' as fragment of all PlanNodes in the plan tree rooted at node.\\n     * Does not traverse the children of ExchangeNodes because those must belong to a\\n     * different fragment.\\n     */\\n    private void setFragmentInPlanTree(PlanNode node) {\\n        if (node == null) {\\n            return;\\n        }\\n        node.setFragment(this);\\n        if (node instanceof ExchangeNode) {\\n            return;\\n        }\\n        for (PlanNode child : node.getChildren()) {\\n            setFragmentInPlanTree(child);\\n        }\\n    }\\n\\n    public double getFragmentCost() {\\n        return fragmentCost;\\n    }\\n\\n    public void setFragmentCost(double fragmentCost) {\\n        this.fragmentCost = fragmentCost;\\n    }\\n\\n    public boolean canUsePipeline() {\\n        return getPlanRoot().canUsePipeLine() && getSink().canUsePipeLine();\\n    }\\n\\n    public boolean canUseRuntimeAdaptiveDop() {\\n        return getPlanRoot().canUseRuntimeAdaptiveDop() && getSink().canUseRuntimeAdaptiveDop();\\n    }\\n\\n    public void enableAdaptiveDop() {\\n        useRuntimeAdaptiveDop = true;\\n        // Constrict DOP as the power of two to make the strategy of decrement DOP easy.\\n        // After decreasing DOP from old_dop to new_dop, chunks from the i-th input driver is passed\\n        // to the j-th output driver, where j=i%new_dop.\\n        pipelineDop = Utils.computeMaxLEPower2(pipelineDop);\\n    }\\n\\n    public void disableRuntimeAdaptiveDop() {\\n        useRuntimeAdaptiveDop = false;\\n    }\\n\\n    public boolean isUseRuntimeAdaptiveDop() {\\n        return useRuntimeAdaptiveDop;\\n    }\\n\\n    /**\\n     * Assign ParallelExecNum by PARALLEL_FRAGMENT_EXEC_INSTANCE_NUM in SessionVariable for synchronous request\\n     * Assign ParallelExecNum by default value for Asynchronous request\\n     */\\n    private void setParallelExecNumIfExists() {\\n        if (ConnectContext.get() != null) {\\n            ConnectContext connectContext = ConnectContext.get();\\n            SessionVariable sv = connectContext.getSessionVariable();\\n            if (sv.isEnablePipelineEngine()) {\\n                this.parallelExecNum = 1;\\n                this.pipelineDop = sv.getDegreeOfParallelism(connectContext.getCurrentWarehouseId());\\n            } else {\\n                this.parallelExecNum = sv.getParallelExecInstanceNum();\\n                this.pipelineDop = 1;\\n            }\\n        }\\n    }\\n\\n    public void limitMaxPipelineDop(int maxPipelineDop) {\\n        if (pipelineDop > maxPipelineDop) {\\n            pipelineDop = maxPipelineDop;\\n            if (useRuntimeAdaptiveDop) {\\n                pipelineDop = Utils.computeMaxLEPower2(pipelineDop);\\n            }\\n        }\\n    }\\n\\n    public ExchangeNode getDestNode() {\\n        return destNode;\\n    }\\n\\n    public DataPartition getOutputPartition() {\\n        return outputPartition;\\n    }\\n\\n    /**\\n     * Parallel load fragment instance num in single host\\n     */\\n    public void setParallelExecNum(int parallelExecNum) {\\n        this.parallelExecNum = parallelExecNum;\\n    }\\n\\n    public int getPipelineDop() {\\n        return pipelineDop;\\n    }\\n\\n    public void setPipelineDop(int dop) {\\n        this.pipelineDop = dop;\\n    }\\n\\n    public boolean hasTableSink() {\\n        return hasIcebergTableSink() || hasOlapTableSink() || hasHiveTableSink() || hasTableFunctionTableSink();\\n    }\\n\\n    public boolean hasOlapTableSink() {\\n        return this.hasOlapTableSink;\\n    }\\n\\n    public void setHasOlapTableSink() {\\n        this.hasOlapTableSink = true;\\n    }\\n\\n    public boolean hasIcebergTableSink() {\\n        return this.hasIcebergTableSink;\\n    }\\n\\n    public void setHasIcebergTableSink() {\\n        this.hasIcebergTableSink = true;\\n    }\\n\\n    public boolean hasHiveTableSink() {\\n        return this.hasHiveTableSink;\\n    }\\n\\n    public void setHasHiveTableSink() {\\n        this.hasHiveTableSink = true;\\n    }\\n\\n    public boolean hasTableFunctionTableSink() {\\n        return this.hasTableFunctionTableSink;\\n    }\\n\\n    public void setHasTableFunctionTableSink() {\\n        this.hasTableFunctionTableSink = true;\\n    }\\n\\n    public boolean forceSetTableSinkDop() {\\n        return this.forceSetTableSinkDop;\\n    }\\n\\n    public void setForceSetTableSinkDop() {\\n        this.forceSetTableSinkDop = true;\\n    }\\n\\n    public boolean isWithLocalShuffle() {\\n        return withLocalShuffle;\\n    }\\n\\n    public void setWithLocalShuffleIfTrue(boolean withLocalShuffle) {\\n        this.withLocalShuffle |= withLocalShuffle;\\n    }\\n\\n    public boolean isUseGroupExecution() {\\n        return !colocateExecGroups.isEmpty();\\n    }\\n\\n    public boolean isAssignScanRangesPerDriverSeq() {\\n        return assignScanRangesPerDriverSeq;\\n    }\\n\\n    public void setAssignScanRangesPerDriverSeq(boolean assignScanRangesPerDriverSeq) {\\n        this.assignScanRangesPerDriverSeq |= assignScanRangesPerDriverSeq;\\n    }\\n\\n    public boolean isForceAssignScanRangesPerDriverSeq() {\\n        return forceAssignScanRangesPerDriverSeq;\\n    }\\n\\n    public void setForceAssignScanRangesPerDriverSeq() {\\n        this.forceAssignScanRangesPerDriverSeq = true;\\n    }\\n\\n    public void computeLocalRfWaitingSet(PlanNode root, boolean clearGlobalRuntimeFilter) {\\n        root.fillLocalRfWaitingSet(runtimeFilterBuildNodeIds);\\n        // TopN Filter should't wait\\n        if (root instanceof RuntimeFilterBuildNode && !(root instanceof SortNode) && !(root instanceof AggregationNode)) {\\n            runtimeFilterBuildNodeIds.add(root.getId().asInt());\\n        }\\n        if (clearGlobalRuntimeFilter) {\\n            root.clearProbeRuntimeFilters();\\n            if (root instanceof RuntimeFilterBuildNode) {\\n                ((RuntimeFilterBuildNode) root).clearBuildRuntimeFilters();\\n            }\\n        }\\n        for (PlanNode child : root.getChildren()) {\\n            if (child.getFragment() == this) {\\n                computeLocalRfWaitingSet(child, clearGlobalRuntimeFilter);\\n            }\\n        }\\n    }\\n\\n    public void assignColocateExecGroups(PlanNode root, List<ExecGroup> groups) {\\n        for (ExecGroup group : groups) {\\n            if (group.contains(root)) {\\n                colocateExecGroups.add(group);\\n                groups.remove(group);\\n                break;\\n            }\\n        }\\n        if (!groups.isEmpty()) {\\n            for (PlanNode child : root.getChildren()) {\\n                if (child.getFragment() == this) {\\n                    assignColocateExecGroups(child, groups);\\n                }\\n            }\\n        }\\n    }\\n\\n    public boolean isColocateGroupFragment() {\\n        return colocateExecGroups.size() > 0;\\n    }\\n\\n    public boolean isDopEstimated() {\\n        return dopEstimated;\\n    }\\n\\n    public void setOutputExprs(List<Expr> outputExprs) {\\n        this.outputExprs = ExprUtils.cloneList(outputExprs, null);\\n    }\\n\\n    /**\\n     * Finalize plan tree and create stream sink, if needed.\\n     */\\n    public void createDataSink(TResultSinkType resultSinkType, ExecPlan execPlan) {\\n        if (sink != null) {\\n            return;\\n        }\\n        if (destNode != null) {\\n            // we're streaming to an exchange node\\n            DataStreamSink streamSink = new DataStreamSink(destNode.getId());\\n            streamSink.setPartition(outputPartition);\\n            streamSink.setMerge(destNode.isMerge());\\n            streamSink.setFragment(this);\\n            sink = streamSink;\\n        } else {\\n            if (planRoot == null) {\\n                // only output expr, no FROM clause\\n                // \\\"select 1 + 2\\\"\\n                return;\\n            }\\n            // add ResultSink\\n            // we're streaming to an result sink\\n\\n            if (resultSinkType == TResultSinkType.ARROW_FLIGHT_PROTOCAL) {\\n                sink = new ResultSink(planRoot.getId(), resultSinkType, execPlan.getColNames());\\n            } else {\\n                sink = new ResultSink(planRoot.getId(), resultSinkType);\\n            }\\n        }\\n    }\\n\\n    public void createDataSink(TResultSinkType resultSinkType) {\\n        createDataSink(resultSinkType, null);\\n    }\\n\\n    public int getParallelExecNum() {\\n        return parallelExecNum;\\n    }\\n\\n    public List<Integer> getCollectExecStatsIds() {\\n        return collectExecStatsIds;\\n    }\\n\\n    public void setCollectExecStatsIds(List<Integer> collectExecStatsIds) {\\n        this.collectExecStatsIds = collectExecStatsIds;\\n    }\\n\\n    public TPlanFragment toThrift() {\\n        TPlanFragment result = new TPlanFragment();\\n        if (planRoot != null) {\\n            result.setPlan(planRoot.treeToThrift());\\n        }\\n        if (outputExprs != null) {\\n            result.setOutput_exprs(ExprToThrift.treesToThrift(outputExprs));\\n        }\\n        if (sink != null) {\\n            result.setOutput_sink(sink.toThrift());\\n        }\\n        result.setPartition(dataPartition.toThrift());\\n\\n        if (!queryGlobalDicts.isEmpty()) {\\n            result.setQuery_global_dicts(dictToThrift(queryGlobalDicts));\\n        }\\n        if (MapUtils.isNotEmpty(queryGlobalDictExprs)) {\\n            Preconditions.checkState(!queryGlobalDicts.isEmpty(), \\\"Global dict expression error!\\\");\\n            Map<Integer, TExpr> exprs = Maps.newHashMap();\\n            queryGlobalDictExprs.forEach((k, v) -> exprs.put(k, ExprToThrift.treeToThrift(v)));\\n            result.setQuery_global_dict_exprs(exprs);\\n        }\\n        if (!loadGlobalDicts.isEmpty()) {\\n            result.setLoad_global_dicts(dictToThrift(loadGlobalDicts));\\n        }\\n        if (cacheParam != null) {\\n            if (ConnectContext.get() != null) {\\n                SessionVariable sessionVariable = ConnectContext.get().getSessionVariable();\\n                cacheParam.setForce_populate(sessionVariable.isQueryCacheForcePopulate());\\n                cacheParam.setEntry_max_bytes(sessionVariable.getQueryCacheEntryMaxBytes());\\n                cacheParam.setEntry_max_rows(sessionVariable.getQueryCacheEntryMaxRows());\\n            }\\n            result.setCache_param(cacheParam);\\n        }\\n\\n        if (!colocateExecGroups.isEmpty()) {\\n            TGroupExecutionParam tGroupExecutionParam = new TGroupExecutionParam();\\n            tGroupExecutionParam.setEnable_group_execution(true);\\n            for (ExecGroup colocateExecGroup : colocateExecGroups) {\\n                tGroupExecutionParam.addToExec_groups(colocateExecGroup.toThrift());\\n            }\\n            result.setGroup_execution_param(tGroupExecutionParam);\\n        }\\n        return result;\\n    }\\n\\n    /**\\n     * Create thrift fragment with the unique fields, including\\n     * - output_sink (only for MultiCastDataStreamSink and ExportSink).\\n     *\\n     * @return The thrift fragment with the unique fields.\\n     */\\n    public TPlanFragment toThriftForUniqueFields() {\\n        TPlanFragment result = new TPlanFragment();\\n        // Fill the required field.\\n        result.setPartition(dataPartition.toThrift());\\n\\n        if (sink != null && (this instanceof MultiCastPlanFragment || sink instanceof ExportSink)) {\\n            result.setOutput_sink(sink.toThrift());\\n        }\\n\\n        return result;\\n    }\\n\\n    public List<TGlobalDict> dictToThrift(List<Pair<Integer, ColumnDict>> dicts) {\\n        List<TGlobalDict> result = Lists.newArrayList();\\n        for (Pair<Integer, ColumnDict> dictPair : dicts) {\\n            TGlobalDict globalDict = new TGlobalDict();\\n            globalDict.setColumnId(dictPair.first);\\n            List<ByteBuffer> strings = Lists.newArrayList();\\n            List<Integer> integers = Lists.newArrayList();\\n            for (Map.Entry<ByteBuffer, Integer> kv : dictPair.second.getDict().entrySet()) {\\n                strings.add(kv.getKey());\\n                integers.add(kv.getValue());\\n            }\\n            globalDict.setVersion(dictPair.second.getCollectedVersion());\\n            globalDict.setStrings(strings);\\n            globalDict.setIds(integers);\\n            result.add(globalDict);\\n        }\\n        return result;\\n    }\\n\\n    // normalize dicts of the fragment, it is different from dictToThrift in three points:\\n    // 1. SlotIds must be replaced by remapped SlotIds;\\n    // 2. dict should be sorted according to its corresponding remapped SlotIds;\\n    public List<TGlobalDict> normalizeDicts(List<Pair<Integer, ColumnDict>> dicts, FragmentNormalizer normalizer) {\\n        List<TGlobalDict> result = Lists.newArrayList();\\n        // replace slot id with the remapped slot id, sort dicts according to remapped slot ids.\\n        List<Pair<Integer, ColumnDict>> sortedDicts =\\n                dicts.stream().map(p -> Pair.create(normalizer.remapSlotId(p.first), p.second))\\n                        .sorted(Comparator.comparingInt(p -> p.first)).collect(Collectors.toList());\\n\\n        for (Pair<Integer, ColumnDict> dictPair : sortedDicts) {\\n            TGlobalDict globalDict = new TGlobalDict();\\n            globalDict.setColumnId(dictPair.first);\\n            globalDict.setVersion(dictPair.second.getCollectedVersion());\\n            result.add(globalDict);\\n        }\\n        return result;\\n    }\\n\\n    public String getExplainString(TExplainLevel explainLevel) {\\n        StringBuilder str = new StringBuilder();\\n        Preconditions.checkState(dataPartition != null);\\n        str.append(\\\" OUTPUT EXPRS:\\\");\\n\\n        StringBuilder outputBuilder = new StringBuilder();\\n        if (CollectionUtils.isNotEmpty(outputExprs)) {\\n            outputBuilder.append(outputExprs.stream().map(ExprToSql::toSql)\\n                    .collect(Collectors.joining(\\\" | \\\")));\\n\\n        }\\n        str.append(outputBuilder);\\n        str.append(\\\"\\\\n\\\");\\n        if (!colocateExecGroups.isEmpty() && Config.show_execution_groups) {\\n            str.append(\\\"  colocate exec groups: \\\");\\n            for (ExecGroup group : colocateExecGroups) {\\n                str.append(group);\\n            }\\n            str.append(\\\"\\\\n\\\");\\n        }\\n        str.append(\\\"  PARTITION: \\\").append(dataPartition.getExplainString(explainLevel)).append(\\\"\\\\n\\\");\\n        if (sink != null) {\\n            str.append(sink.getExplainString(\\\"  \\\", explainLevel)).append(\\\"\\\\n\\\");\\n        }\\n        if (planRoot != null) {\\n            str.append(planRoot.getExplainString(\\\"  \\\", \\\"  \\\", explainLevel));\\n        }\\n        return str.toString();\\n    }\\n\\n    public String getVerboseExplain() {\\n        StringBuilder str = new StringBuilder();\\n        Preconditions.checkState(dataPartition != null);\\n        if (FeConstants.showFragmentCost) {\\n            str.append(\\\"  Fragment Cost: \\\").append(fragmentCost).append(\\\"\\\\n\\\");\\n        }\\n        if (!colocateExecGroups.isEmpty() && Config.show_execution_groups) {\\n            str.append(\\\"  colocate exec groups: \\\");\\n            for (ExecGroup group : colocateExecGroups) {\\n                str.append(group);\\n            }\\n            str.append(\\\"\\\\n\\\");\\n        }\\n        if (CollectionUtils.isNotEmpty(outputExprs)) {\\n            str.append(\\\"  Output Exprs:\\\");\\n            str.append(outputExprs.stream().map(ExprToSql::toSql)\\n                    .collect(Collectors.joining(\\\" | \\\")));\\n        }\\n        str.append(\\\"\\\\n\\\");\\n        str.append(\\\"  Input Partition: \\\").append(dataPartition.getExplainString(TExplainLevel.NORMAL));\\n        if (sink != null) {\\n            str.append(sink.getVerboseExplain(\\\"  \\\")).append(\\\"\\\\n\\\");\\n        }\\n        if (MapUtils.isNotEmpty(queryGlobalDictExprs)) {\\n            str.append(\\\"  Global Dict Exprs:\\\\n\\\");\\n            queryGlobalDictExprs.entrySet().stream()\\n                    .map(p -> \\\"    \\\" + p.getKey() + \\\": \\\" + ExprToSql.toMySql(p.getValue()) + \\\"\\\\n\\\").forEach(str::append);\\n            str.append(\\\"\\\\n\\\");\\n        }\\n        if (planRoot != null) {\\n            str.append(planRoot.getVerboseExplain(\\\"  \\\", \\\"  \\\"));\\n        }\\n        return str.toString();\\n    }\\n\\n    public String getCostExplain() {\\n        StringBuilder str = new StringBuilder();\\n        Preconditions.checkState(dataPartition != null);\\n        StringBuilder outputBuilder = new StringBuilder();\\n        if (CollectionUtils.isNotEmpty(outputExprs)) {\\n            str.append(\\\"  Output Exprs:\\\");\\n            outputBuilder.append(outputExprs.stream().map(ExprToSql::toSql)\\n                    .collect(Collectors.joining(\\\" | \\\")));\\n        }\\n        str.append(outputBuilder.toString());\\n        str.append(\\\"\\\\n\\\");\\n        str.append(\\\"  Input Partition: \\\").append(dataPartition.getExplainString(TExplainLevel.NORMAL));\\n        if (sink != null) {\\n            str.append(sink.getVerboseExplain(\\\"  \\\")).append(\\\"\\\\n\\\");\\n        }\\n        if (planRoot != null) {\\n            str.append(planRoot.getCostExplain(\\\"  \\\", \\\"  \\\"));\\n        }\\n        return str.toString();\\n    }\\n\\n    /**\\n     * Returns true if this fragment is partitioned.\\n     */\\n    public boolean isPartitioned() {\\n        return (dataPartition.getType() != TPartitionType.UNPARTITIONED);\\n    }\\n\\n    public PlanFragment getDestFragment() {\\n        if (destNode == null) {\\n            return null;\\n        }\\n        return destNode.getFragment();\\n    }\\n\\n    public void setDestination(ExchangeNode destNode) {\\n        this.destNode = destNode;\\n        PlanFragment dest = getDestFragment();\\n        Preconditions.checkNotNull(dest);\\n        dest.addChild(this);\\n    }\\n\\n    public void clearDestination() {\\n        this.destNode = null;\\n    }\\n\\n    public DataPartition getDataPartition() {\\n        return dataPartition;\\n    }\\n\\n    public boolean isGatherFragment() {\\n        return getDataPartition() == DataPartition.UNPARTITIONED;\\n    }\\n\\n    public void setOutputPartition(DataPartition outputPartition) {\\n        this.outputPartition = outputPartition;\\n    }\\n\\n    public void clearOutputPartition() {\\n        this.outputPartition = DataPartition.UNPARTITIONED;\\n    }\\n\\n    public PlanNode getPlanRoot() {\\n        return planRoot;\\n    }\\n\\n    public void setPlanRoot(PlanNode root) {\\n        planRoot = root;\\n        setFragmentInPlanTree(planRoot);\\n    }\\n\\n    public DataSink getSink() {\\n        return sink;\\n    }\\n\\n    public void setSink(DataSink sink) {\\n        Preconditions.checkNotNull(sink);\\n        sink.setFragment(this);\\n        this.sink = sink;\\n    }\\n\\n    public TDataSink sinkToThrift() {\\n        return sink != null ? sink.toThrift() : null;\\n    }\\n\\n    public PlanFragmentId getFragmentId() {\\n        return fragmentId;\\n    }\\n\\n    public boolean isTransferQueryStatisticsWithEveryBatch() {\\n        return transferQueryStatisticsWithEveryBatch;\\n    }\\n\\n    public void collectBuildRuntimeFilters() {\\n        Map<Integer, RuntimeFilterDescription> filters = Maps.newHashMap();\\n        collectNodes().stream()\\n                .filter(node -> node instanceof RuntimeFilterBuildNode)\\n                .flatMap(node -> ((RuntimeFilterBuildNode) node).getBuildRuntimeFilters().stream())\\n                .forEach(desc -> filters.put(desc.getFilterId(), desc));\\n        buildRuntimeFilters = filters;\\n    }\\n\\n    public void collectProbeRuntimeFilters() {\\n        Map<Integer, RuntimeFilterDescription> filters = Maps.newHashMap();\\n        collectNodes().stream()\\n                .flatMap(node -> node.getProbeRuntimeFilters().stream())\\n                .forEach(desc -> filters.put(desc.getFilterId(), desc));\\n        probeRuntimeFilters = filters;\\n    }\\n\\n    public List<PlanNode> collectNodes() {\\n        List<PlanNode> nodes = Lists.newArrayList();\\n        collectNodesImpl(getPlanRoot(), nodes);\\n        return nodes;\\n    }\\n\\n    private void collectNodesImpl(PlanNode root, List<PlanNode> nodes) {\\n        nodes.add(root);\\n        if (!(root instanceof ExchangeNode)) {\\n            root.getChildren().forEach(child -> collectNodesImpl(child, nodes));\\n        }\\n\\n        if (root instanceof HashJoinNode) {\\n            HashJoinNode hashJoinNode = (HashJoinNode) root;\\n            if (hashJoinNode.isSkewBroadJoin()) {\\n                HashJoinNode shuffleJoinNode = hashJoinNode.getSkewJoinFriend();\\n                // TODO(fixme): ensure broadcast join 's rf size is equal to shuffle join's rf size, if not clear the specific\\n                //  broadcast's rf.\\n                if (shuffleJoinNode == null || shuffleJoinNode.getBuildRuntimeFilters() == null) {\\n                   return;\\n                }\\n                if (shuffleJoinNode.getBuildRuntimeFilters().size() != hashJoinNode.getBuildRuntimeFilters().size()) {\\n                    shuffleJoinNode.clearBuildRuntimeFilters();\\n                    hashJoinNode.clearBuildRuntimeFilters();\\n                    return;\\n                }\\n                for (RuntimeFilterDescription description : hashJoinNode.getBuildRuntimeFilters()) {\\n                    int filterId = shuffleJoinNode.getRfIdByEqJoinConjunctsIndex(description.getExprOrder());\\n                    // skew join's broadcast join rf need to remember the filter id of corresponding shuffle join\\n                    description.setSkew_shuffle_filter_id(filterId);\\n                }\\n            }\\n        }\\n    }\\n\\n    public void setRuntimeFilterMergeNodeAddresses(PlanNode root, TNetworkAddress host) {\\n        if (root instanceof ExchangeNode) {\\n            return;\\n        }\\n\\n        if (root instanceof JoinNode) {\\n            JoinNode joinNode = (JoinNode) root;\\n            for (RuntimeFilterDescription description : joinNode.getBuildRuntimeFilters()) {\\n                description.addMergeNode(host);\\n            }\\n        }\\n        if (root instanceof AggregationNode aggNode) {\\n            for (RuntimeFilterDescription description : aggNode.getBuildRuntimeFilters()) {\\n                if (description.isHasRemoteTargets()) {\\n                    description.addMergeNode(host);\\n                }\\n            }\\n        }\\n\\n        for (PlanNode node : root.getChildren()) {\\n            setRuntimeFilterMergeNodeAddresses(node, host);\\n        }\\n    }\\n\\n    public Map<Integer, RuntimeFilterDescription> getBuildRuntimeFilters() {\\n        return buildRuntimeFilters;\\n    }\\n\\n    public Map<Integer, RuntimeFilterDescription> getProbeRuntimeFilters() {\\n        return probeRuntimeFilters;\\n    }\\n\\n    public List<Pair<Integer, ColumnDict>> getQueryGlobalDicts() {\\n        return this.queryGlobalDicts;\\n    }\\n\\n    public Map<Integer, Expr> getQueryGlobalDictExprs() {\\n        return queryGlobalDictExprs;\\n    }\\n\\n    public List<Pair<Integer, ColumnDict>> getLoadGlobalDicts() {\\n        return this.loadGlobalDicts;\\n    }\\n\\n    public void setQueryGlobalDicts(List<Pair<Integer, ColumnDict>> dicts) {\\n        this.queryGlobalDicts = dicts;\\n    }\\n\\n    // For plan fragment has join\\n    public void mergeQueryGlobalDicts(List<Pair<Integer, ColumnDict>> dicts) {\\n        if (this.queryGlobalDicts != dicts) {\\n            this.queryGlobalDicts = Stream.concat(this.queryGlobalDicts.stream(), dicts.stream()).distinct()\\n                    .collect(Collectors.toList());\\n        }\\n    }\\n\\n    public void mergeQueryDictExprs(Map<Integer, Expr> queryGlobalDictExprs) {\\n        if (this.queryGlobalDictExprs != queryGlobalDictExprs) {\\n            Map<Integer, Expr> n = Maps.newHashMap(MapUtils.emptyIfNull(this.queryGlobalDictExprs));\\n            n.putAll(MapUtils.emptyIfNull(queryGlobalDictExprs));\\n            this.queryGlobalDictExprs = n;\\n        }\\n    }\\n\\n    public void setQueryGlobalDictExprs(Map<Integer, Expr> queryGlobalDictExprs) {\\n        this.queryGlobalDictExprs = queryGlobalDictExprs;\\n    }\\n\\n    public void setLoadGlobalDicts(\\n            List<Pair<Integer, ColumnDict>> loadGlobalDicts) {\\n        this.loadGlobalDicts = loadGlobalDicts;\\n    }\\n\\n    public boolean hashLocalBucketShuffleRightOrFullJoin(PlanNode planRoot) {\\n        if (planRoot instanceof JoinNode) {\\n            JoinNode joinNode = (JoinNode) planRoot;\\n            if (joinNode.isLocalHashBucket() &&\\n                    (joinNode.getJoinOp().isFullOuterJoin() || joinNode.getJoinOp().isRightJoin())) {\\n                return true;\\n            }\\n        }\\n        for (PlanNode childNode : planRoot.getChildren()) {\\n            if (hashLocalBucketShuffleRightOrFullJoin(childNode)) {\\n                return true;\\n            }\\n        }\\n        return false;\\n    }\\n\\n    public TCacheParam getCacheParam() {\\n        return cacheParam;\\n    }\\n\\n    public void setCacheParam(TCacheParam cacheParam) {\\n        this.cacheParam = cacheParam;\\n    }\\n\\n    public Map<PlanNodeId, ScanNode> collectScanNodes() {\\n        Map<PlanNodeId, ScanNode> scanNodes = Maps.newHashMap();\\n        Queue<PlanNode> queue = Lists.newLinkedList();\\n        queue.add(planRoot);\\n        while (!queue.isEmpty()) {\\n            PlanNode node = queue.poll();\\n\\n            if (node instanceof ExchangeNode) {\\n                continue;\\n            }\\n            if (node instanceof ScanNode) {\\n                scanNodes.put(node.getId(), (ScanNode) node);\\n            }\\n\\n            queue.addAll(node.getChildren());\\n        }\\n\\n        return scanNodes;\\n    }\\n\\n    public Optional<List<BucketProperty>> extractBucketProperties() {\\n        List<List<BucketProperty>> properties = collectScanNodes().values().stream()\\n                .filter(scanNode -> scanNode instanceof IcebergScanNode)\\n                .map(scanNode -> ((IcebergScanNode) scanNode).getBucketProperties())\\n                .filter(Optional::isPresent).map(Optional::get).toList();\\n        return BucketProperty.checkAndGetBucketProperties(properties);\\n\\n    }\\n\\n    public boolean isUnionFragment() {\\n        Deque<PlanNode> dq = new LinkedList<>();\\n        dq.offer(planRoot);\\n\\n        while (!dq.isEmpty()) {\\n            PlanNode nd = dq.poll();\\n\\n            if (nd instanceof UnionNode) {\\n                return true;\\n            }\\n            if (!(nd instanceof ExchangeNode)) {\\n                dq.addAll(nd.getChildren());\\n            }\\n        }\\n        return false;\\n    }\\n\\n    public ArrayList<Expr> getOutputExprs() {\\n        return outputExprs;\\n    }\\n\\n    public boolean isShortCircuit() {\\n        return isShortCircuit;\\n    }\\n\\n    public void setShortCircuit(boolean shortCircuit) {\\n        isShortCircuit = shortCircuit;\\n    }\\n\\n    /**\\n     * Returns the leftmost node of the fragment.\\n     */\\n    public PlanNode getLeftMostNode() {\\n        PlanNode node = planRoot;\\n        while (!node.getChildren().isEmpty() && !(node instanceof ExchangeNode)) {\\n            node = node.getChild(0);\\n        }\\n        return node;\\n    }\\n\\n    public void reset() {\\n        // Do nothing.\\n    }\\n\\n    public void disablePhysicalPropertyOptimize() {\\n        colocateExecGroups.clear();\\n        forEachNode(planRoot, PlanNode::disablePhysicalPropertyOptimize);\\n    }\\n\\n    private void forEachNode(PlanNode root, Consumer<PlanNode> consumer) {\\n        consumer.accept(root);\\n        for (PlanNode child : root.getChildren()) {\\n            forEachNode(child, consumer);\\n        }\\n    }\\n\\n    private RoaringBitmap collectNonBroadcastRfIds(PlanNode root) {\\n        RoaringBitmap filterIds = root.getChildren().stream()\\n                .filter(child -> child.getFragmentId().equals(root.getFragmentId()))\\n                .map(this::collectNonBroadcastRfIds)\\n                .reduce(RoaringBitmap.bitmapOf(), (a, b) -> RoaringBitmap.or(a, b));\\n        if (root instanceof HashJoinNode) {\\n            HashJoinNode joinNode = (HashJoinNode) root;\\n            if (!joinNode.isBroadcast()) {\\n                joinNode.getBuildRuntimeFilters().forEach(rf -> filterIds.add(rf.getFilterId()));\\n            }\\n        }\\n        return filterIds;\\n    }\\n\\n    /**\\n     * In the same fragment, collect all nodes of the right subtree of BroadcastJoinNode that will build global RFs\\n     * For example, the following plan will add 3 and 4 to {@code localRightOffsprings}.\\n     * <pre>{@code\\n     *       Broadcast Join#1\\n     *        /        \\\\\",\n  \"message\": \"Successfully read file: /Users/kks/git/starrocks/fe/fe-core/src/main/java/com/starrocks/planner/PlanFragment.java (TRUNCATED: showing first 1000 of 1078 total lines)\"\n}"
            }
          }
        ],
        "providerOptions": null
      },
      {
        "role": "assistant",
        "content": [
          {
            "type": "tool-call",
            "toolCallId": "call_MHxNN1JyRklyZVRvRFFXU2VNVVM",
            "toolName": "readFile",
            "input": {
              "file_path": "/Users/kks/git/starrocks/be/src/exec/pipeline/README.md"
            }
          }
        ],
        "providerOptions": null
      },
      {
        "role": "tool",
        "content": [
          {
            "type": "tool-result",
            "toolCallId": "call_MHxNN1JyRklyZVRvRFFXU2VNVVM",
            "toolName": "readFile",
            "output": {
              "type": "text",
              "value": "{\n  \"success\": true,\n  \"file_path\": \"/Users/kks/git/starrocks/be/src/exec/pipeline/README.md\",\n  \"content\": \"Pipeline Engine\\n\\n## Pipeline\\n\\n## Operator\\n\\n## OperatorFactory\\n1. Create operators from OperatorFactory, One OperatorFactory could have many operators.\\n2. Could store some read-only operator status in OperatorFactory, and Operator\\nonly store const reference\",\n  \"message\": \"Successfully read 10 lines from file: /Users/kks/git/starrocks/be/src/exec/pipeline/README.md\"\n}"
            }
          }
        ],
        "providerOptions": null
      }
    ],
    "tools": [
      {
        "type": "function",
        "name": "bash",
        "description": "Execute shell commands safely on the system.\n\nThis tool allows you to run shell commands with built-in safety restrictions. Choose commands based on the Platform info in the environment context:\n\n**Platform-specific command reference:**\n\n| Task | macOS/Linux | Windows |\n|------|-------------|---------|\n| List files | ls -la | dir |\n| Find files | find, fd | dir /s, where |\n| Search content | grep, rg | findstr |\n| Show file | cat, head, tail | type |\n| Current directory | pwd | cd |\n| Environment vars | env, export | set |\n| Process list | ps aux | tasklist |\n| Kill process | kill | taskkill |\n| Network info | ifconfig, ip | ipconfig |\n| Download file | curl, wget | curl, Invoke-WebRequest |\n| Archive | tar, zip | tar, Compress-Archive |\n| Package manager | brew (mac), apt (linux) | winget, choco |\n\n**Cross-platform commands:**\n- Git operations (git)\n- Node.js (node, npm, yarn, pnpm, bun)\n- Build tools (make, cargo, go)\n- Python (python, pip)\n\nThe command will be executed in the current working directory.\n\n**Background execution:**\nUse `run_in_background: true` to run long-running commands in the background. The command will continue running even if it produces no output for an extended period. Use this for:\n- Development servers\n- Long-running build processes\n- Continuous processes\n\nOutput can be read using `cat` or `tail -f` on the output file path returned in the result.",
        "parameters": {
          "additionalProperties": false,
          "properties": {
            "command": {
              "type": "string"
            },
            "runInBackground": {
              "default": false,
              "type": "boolean"
            }
          },
          "required": [
            "command"
          ],
          "type": "object"
        },
        "strict": true
      },
      {
        "type": "function",
        "name": "callAgent",
        "description": "Call a registered sub-agent for a focused task. Subagents start with empty context.\n\n**Parameters**\n- `agentId`: The agent to call (see Available Subagents in system prompt)\n- `task`: Clear outcome description (2-5 sentences)\n- `context`: All necessary artifacts (file contents, schemas, requirements)\n- `targets`: Files/modules this agent will touch (for conflict avoidance)\n\n**Key Rules**\n- Subagents have zero history - pass all needed context\n- Use targets to enable safe parallel execution\n- For parallel calls, issue multiple callAgent in same response",
        "parameters": {
          "additionalProperties": false,
          "properties": {
            "agentId": {
              "type": "string"
            },
            "context": {
              "type": "string"
            },
            "targets": {
              "items": {
                "type": "string"
              },
              "type": "array"
            },
            "task": {
              "type": "string"
            }
          },
          "required": [
            "agentId",
            "task",
            "context"
          ],
          "type": "object"
        },
        "strict": true
      },
      {
        "type": "function",
        "name": "readFile",
        "description": "Use this tool to read the contents of an existing file.\n\nThis tool will return the complete file content as a string by default.\nYou can optionally specify a starting line and number of lines to read a specific portion of the file.\n\nThe file path should be an absolute path.",
        "parameters": {
          "additionalProperties": false,
          "properties": {
            "file_path": {
              "type": "string"
            },
            "line_count": {
              "type": "number"
            },
            "start_line": {
              "type": "number"
            }
          },
          "required": [
            "file_path"
          ],
          "type": "object"
        },
        "strict": true
      },
      {
        "type": "function",
        "name": "codeSearch",
        "description": "Use this tool when you need to find files containing specific patterns.\n\nUse this to find code patterns, function definitions, variable usage, or any text in the codebase.",
        "parameters": {
          "additionalProperties": false,
          "properties": {
            "file_types": {
              "items": {
                "type": "string"
              },
              "type": "array"
            },
            "path": {
              "type": "string"
            },
            "pattern": {
              "type": "string"
            }
          },
          "required": [
            "pattern",
            "path"
          ],
          "type": "object"
        },
        "strict": true
      },
      {
        "type": "function",
        "name": "glob",
        "description": "- Fast file pattern matching tool that works with any codebase size\n- Supports glob patterns like \"**/*.js\" or \"src/**/*.ts\"\n- Returns matching file paths sorted by modification time\n- Use this tool when you need to find files by name patterns",
        "parameters": {
          "additionalProperties": false,
          "properties": {
            "path": {
              "type": "string"
            },
            "pattern": {
              "type": "string"
            }
          },
          "required": [
            "pattern"
          ],
          "type": "object"
        },
        "strict": true
      },
      {
        "type": "function",
        "name": "lsp",
        "description": "Perform Language Server Protocol (LSP) operations like go-to-definition, references, hover, and symbols.\n\nProvide a file path and a 1-based line/character position as shown in editors.",
        "parameters": {
          "additionalProperties": false,
          "properties": {
            "character": {
              "type": "number"
            },
            "filePath": {
              "type": "string"
            },
            "line": {
              "type": "number"
            },
            "operation": {
              "enum": [
                "goToDefinition",
                "findReferences",
                "hover",
                "documentSymbol",
                "workspaceSymbol",
                "goToImplementation",
                "prepareCallHierarchy",
                "incomingCalls",
                "outgoingCalls"
              ],
              "type": "string"
            },
            "query": {
              "type": "string"
            }
          },
          "required": [
            "operation",
            "filePath"
          ],
          "type": "object"
        },
        "strict": true
      },
      {
        "type": "function",
        "name": "listFiles",
        "description": "Use this tool to list files and directories in a specified directory.\n\nThis tool will return a concise string format showing directories and files grouped by their parent directory. It's useful for exploring directory structure, finding files, or understanding project organization.\n\nThe directory path must be absolute.",
        "parameters": {
          "additionalProperties": false,
          "properties": {
            "directory_path": {
              "type": "string"
            },
            "max_depth": {
              "type": "number"
            }
          },
          "required": [
            "directory_path"
          ],
          "type": "object"
        },
        "strict": true
      },
      {
        "type": "function",
        "name": "todoWrite",
        "description": "Creates and manages todo items for task tracking and progress management in the current session.\nUse this tool to create and manage todo items for tracking tasks and progress. This tool provides comprehensive todo management:\n\n## When to Use This Tool\n\nUse this tool proactively in these scenarios:\n\n1. **Complex multi-step tasks** - When a task requires 3 or more distinct steps or actions\n2. **Non-trivial and complex tasks** - Tasks that require careful planning or multiple operations\n3. **User explicitly requests todo list** - When the user directly asks you to use the todo list\n4. **User provides multiple tasks** - When users provide a list of things to be done (numbered or comma-separated)\n5. **After receiving new instructions** - Immediately capture user requirements as todos\n6. **When you start working on a task** - Mark it as in_progress BEFORE beginning work. Ideally you should only have one todo as in_progress at a time\n7. **After completing a task** - Mark it as completed and add any new follow-up tasks discovered during implementation\n\n## When NOT to Use This Tool\n\nSkip using this tool when:\n1. There is only a single, straightforward task\n2. The task is trivial and tracking it provides no organizational benefit\n3. The task can be completed in less than 3 trivial steps\n4. The task is purely conversational or informational\n\n## Task States and Management\n\n1. **Task States**: Use these states to track progress:\n   - pending: Task not yet started\n   - in_progress: Currently working on (limit to ONE task at a time)\n   - completed: Task finished successfully\n\n2. **Task Management**:\n   - Update task status in real-time as you work\n   - Mark tasks complete IMMEDIATELY after finishing (don't batch completions)\n   - Only have ONE task in_progress at any time\n   - Complete current tasks before starting new ones\n   - Remove tasks that are no longer relevant from the list entirely\n\n3. **Task Completion Requirements**:\n   - ONLY mark a task as completed when you have FULLY accomplished it\n   - If you encounter errors, blockers, or cannot finish, keep the task as in_progress\n   - When blocked, create a new task describing what needs to be resolved\n   - Never mark a task as completed if:\n     - Tests are failing\n     - Implementation is partial\n     - You encountered unresolved errors\n     - You couldn't find necessary files or dependencies\n\n4. **Task Breakdown**:\n   - Create specific, actionable items\n   - Break complex tasks into smaller, manageable steps\n   - Use clear, descriptive task names\n\n## Tool Capabilities\n\n- **Create new todos**: Add tasks with content, priority, and status\n- **Update existing todos**: Modify any aspect of a todo (status, priority, content)\n- **Delete todos**: Remove completed or irrelevant tasks\n- **Batch operations**: Update multiple todos in a single operation\n- **Clear all todos**: Reset the entire todo list\n\nWhen in doubt, use this tool. Being proactive with task management demonstrates attentiveness and ensures you complete all requirements successfully.",
        "parameters": {
          "additionalProperties": false,
          "properties": {
            "todos": {
              "items": {
                "additionalProperties": false,
                "properties": {
                  "content": {
                    "type": "string"
                  },
                  "id": {
                    "type": "string"
                  },
                  "status": {
                    "enum": [
                      "pending",
                      "in_progress",
                      "completed"
                    ],
                    "type": "string"
                  }
                },
                "required": [
                  "content",
                  "status",
                  "id"
                ],
                "type": "object"
              },
              "type": "array"
            }
          },
          "required": [
            "todos"
          ],
          "type": "object"
        },
        "strict": true
      },
      {
        "type": "function",
        "name": "writeFile",
        "description": "Use this tool to write content to a file.\n\nThis tool will create or overwrite a file with the provided content. It will automatically create directories if they don't exist. It's useful for creating new files or updating existing files in the project.\n\nThe file path should be an absolute path.",
        "parameters": {
          "additionalProperties": false,
          "properties": {
            "content": {
              "type": "string"
            },
            "file_path": {
              "type": "string"
            }
          },
          "required": [
            "file_path",
            "content"
          ],
          "type": "object"
        },
        "strict": true
      },
      {
        "type": "function",
        "name": "editFile",
        "description": "Edit an existing file with one or more text replacements.\n\nCRITICAL RULES:\n1. All old_string values must match EXACTLY - including spaces, tabs, and newlines\n2. Use readFile tool FIRST to see the exact content\n3. Include 3-5 lines of context before and after each change\n4. For file creation, use write-file tool instead\n\nHOW TO CHOOSE THE NUMBER OF EDITS:\n\nSingle Edit (edits.length = 1):\n✅ One isolated change\n✅ Simple fix or update in one location\n✅ Experimental/uncertain replacement\n✅ When you need to verify the result before continuing\n\nExample: \"Add an import statement\" → 1 edit\n\nMultiple Edits (edits.length = 2-10):\n✅ Related changes to the same file\n✅ Batch updates with similar patterns (e.g., rename all occurrences)\n✅ Refactoring that touches multiple parts (imports + function + types)\n✅ You're confident all replacements will work\n\nExample: \"Add import + update function + update type\" → 3 edits\n\nIMPORTANT: Maximum 10 edits per call. For larger refactorings, make multiple calls.\n\nBest practice workflow:\n1. Use readFile to see current content\n2. Identify all changes you want to make to this file\n3. For each change, copy EXACT text with context\n4. Create edit block(s) with old_string and new_string\n5. Call edit-file with your edit(s)",
        "parameters": {
          "additionalProperties": false,
          "properties": {
            "edits": {
              "items": {
                "additionalProperties": false,
                "properties": {
                  "new_string": {
                    "type": "string"
                  },
                  "old_string": {
                    "type": "string"
                  }
                },
                "required": [
                  "old_string",
                  "new_string"
                ],
                "type": "object"
              },
              "type": "array"
            },
            "file_path": {
              "type": "string"
            }
          },
          "required": [
            "file_path",
            "edits"
          ],
          "type": "object"
        },
        "strict": true
      },
      {
        "type": "function",
        "name": "webSearch",
        "description": "Search the web for comprehensive and up-to-date information.\n\nQuery Optimization Guidelines:\n- Extract the core topic/entity from questions (e.g., \"who is Elon Musk\" → \"Elon Musk\", \"what is React\" → \"React\")\n- For comparisons (e.g., \"X vs Y\"), search for both terms together to get comparison results\n- Remove question words (who, what, when, where, why, how) but keep context words that add meaning\n- Keep technical terms, version numbers, and specific qualifiers (e.g., \"React 19 features\" stays as-is)\n- For \"latest\" or \"recent\" queries, include temporal keywords (e.g., \"latest AI models 2025\")\n- Preserve programming language/framework context (e.g., \"error handling in Rust\" → \"Rust error handling\")\n- For debugging queries, keep error messages and stack traces intact\n- Use multiple searches only when topics are completely unrelated, not for comparisons",
        "parameters": {
          "additionalProperties": false,
          "properties": {
            "query": {
              "type": "string"
            }
          },
          "required": [
            "query"
          ],
          "type": "object"
        },
        "strict": true
      },
      {
        "type": "function",
        "name": "webFetch",
        "description": "Fetch and extract content from a web url",
        "parameters": {
          "additionalProperties": false,
          "properties": {
            "url": {
              "type": "string"
            }
          },
          "required": [
            "url"
          ],
          "type": "object"
        },
        "strict": true
      }
    ],
    "temperature": 1.0,
    "max_tokens": 15000,
    "top_p": 0.95,
    "top_k": 64,
    "provider_options": {
      "anthropic": {
        "thinking": {
          "budgetTokens": 12000,
          "type": "enabled"
        }
      },
      "google": {
        "thinkingConfig": {
          "includeThoughts": true,
          "thinkingBudget": 8192
        }
      },
      "moonshot": {
        "temperature": 1,
        "thinking": {
          "type": "enabled"
        }
      },
      "openai": {
        "reasoningEffort": "low"
      },
      "openrouter": {
        "effort": "low"
      }
    },
    "extra_body": null
  },
  "expected_events": [
    {
      "type": "text-start"
    },
    {
      "type": "text-delta",
      "text": "In StarRocks, the relationship between **Fragments** and **Pipelines** is one of **hierarchical decomposition**: a Fragment is a distributed unit of execution that is further broken down into one or more Pipelines for local execution on"
    },
    {
      "type": "text-delta",
      "text": " a Backend (BE).\n\n### 1. PlanFragment (The Distributed Unit)\nA **PlanFragment** is the unit of distribution and scheduling in StarRocks.\n*   **Logical Scope**: It encapsulates a portion of the SQL execution plan (a tree of `PlanNodes`).\n*   **Connectivity**: Fragments are connected"
    },
    {
      "type": "text-delta",
      "text": " to each other via **Exchange Nodes**. Data is shuffled or broadcasted between fragments across the network.\n*   **Instantiation**: When a query runs, a `PlanFragment` is instantiated into multiple **Fragment Instances**, each running on a specific Backend.\n\n### 2. Pipeline (The Local Execution Unit)\n"
    },
    {
      "type": "text-delta",
      "text": "A **Pipeline** is the unit of local execution within a Fragment Instance.\n*   **Decomposition**: When a Fragment Instance starts on a BE, the `PipelineBuilder` decomposes the fragment's `ExecNode` tree into a set of `Pipelines`.\n*   **Structure**: Each Pipeline consists of a"
    },
    {
      "type": "text-delta",
      "text": " chain of **Operators** (Source -> Transformation -> Sink).\n*   **Pipelining**: Within a pipeline, data (in Chunks) flows through operators in a push/pull fashion without blocking.\n*   **Boundaries**: Pipelines are separated by **Pipeline Breakers** (blocking operators). For example, a"
    },
    {
      "type": "text-delta",
      "text": " Hash Join splits a fragment into:\n    1.  A **Build Pipeline**: Reads data and builds a hash table (Sink).\n    2.  A **Probe Pipeline**: Reads data from the other side (Source) and probes the hash table.\n\n### 3. Key Relationships\n\n| Feature | Plan"
    },
    {
      "type": "text-delta",
      "text": "Fragment | Pipeline |\n| :--- | :--- | :--- |\n| **Defined in** | Frontend (Java) | Backend (C++) |\n| **Granularity** | Coarse (Distributed sub-tree) | Fine (Local data flow segment) |\n| **Parallelism** | Parallelized"
    },
    {
      "type": "text-delta",
      "text": " via multiple instances on different nodes. | Parallelized via multiple **Drivers** (instantiations of the pipeline) on different CPU cores. |\n| **Communication** | Network RPC (via ExchangeNodes). | Memory buffers/Local Exchange (via Sink/Source operators). |\n| **Lifecycle** | Managed by `FragmentContext"
    },
    {
      "type": "text-delta",
      "text": "`. | Managed by `PipelineDriver` and scheduled by the Pipeline Engine. |\n\n### Summary of the Hierarchy\n1.  **Query**: The total SQL execution.\n2.  **Fragment**: A segment of the query that can run on multiple nodes.\n3.  **Fragment Instance**: The execution of a fragment on"
    },
    {
      "type": "text-delta",
      "text": " a single node.\n4.  **Pipeline**: A chain of operators within an instance, separated by blocking points.\n5.  **Pipeline Driver**: A thread-executable task representing one parallel unit of a Pipeline.\n\nIn short, **Fragments manage \"Where\" data moves across the cluster, while Pipelines manage \""
    },
    {
      "type": "usage",
      "input_tokens": 40966,
      "output_tokens": 647,
      "total_tokens": 42155,
      "cached_input_tokens": null,
      "cache_creation_input_tokens": null
    },
    {
      "type": "text-delta",
      "text": "How\" data flows through CPU cores within a single node.**"
    },
    {
      "type": "done",
      "finish_reason": "stop"
    }
  ]
}